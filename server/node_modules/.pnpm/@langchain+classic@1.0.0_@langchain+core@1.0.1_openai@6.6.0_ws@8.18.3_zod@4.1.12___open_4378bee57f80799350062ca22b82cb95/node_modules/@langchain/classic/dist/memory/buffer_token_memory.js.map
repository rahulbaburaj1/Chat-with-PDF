{"version":3,"file":"buffer_token_memory.js","names":["fields: ConversationTokenBufferMemoryInput","_values: InputValues","result","inputValues: InputValues","outputValues: OutputValues"],"sources":["../../src/memory/buffer_token_memory.ts"],"sourcesContent":["import type { BaseLanguageModelInterface } from \"@langchain/core/language_models/base\";\nimport {\n  InputValues,\n  MemoryVariables,\n  OutputValues,\n} from \"@langchain/core/memory\";\nimport { getBufferString } from \"@langchain/core/messages\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n\n/**\n * Interface for the input parameters of the `BufferTokenMemory` class.\n */\n\nexport interface ConversationTokenBufferMemoryInput\n  extends BaseChatMemoryInput {\n  /* Prefix for human messages in the buffer. */\n  humanPrefix?: string;\n\n  /* Prefix for AI messages in the buffer. */\n  aiPrefix?: string;\n\n  /* The LLM for this instance. */\n  llm: BaseLanguageModelInterface;\n\n  /* Memory key for buffer instance. */\n  memoryKey?: string;\n\n  /* Maximmum number of tokens allowed in the buffer. */\n  maxTokenLimit?: number;\n}\n\n/**\n * Class that represents a conversation chat memory with a token buffer.\n * It extends the `BaseChatMemory` class and implements the\n * `ConversationTokenBufferMemoryInput` interface.\n * @example\n * ```typescript\n * const memory = new ConversationTokenBufferMemory({\n *   llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n *   maxTokenLimit: 10,\n * });\n *\n * // Save conversation context\n * await memory.saveContext({ input: \"hi\" }, { output: \"whats up\" });\n * await memory.saveContext({ input: \"not much you\" }, { output: \"not much\" });\n *\n * // Load memory variables\n * const result = await memory.loadMemoryVariables({});\n * console.log(result);\n * ```\n */\n\nexport class ConversationTokenBufferMemory\n  extends BaseChatMemory\n  implements ConversationTokenBufferMemoryInput\n{\n  humanPrefix = \"Human\";\n\n  aiPrefix = \"AI\";\n\n  memoryKey = \"history\";\n\n  maxTokenLimit = 2000; // Default max token limit of 2000 which can be overridden\n\n  llm: BaseLanguageModelInterface;\n\n  constructor(fields: ConversationTokenBufferMemoryInput) {\n    super(fields);\n    this.llm = fields.llm;\n    this.humanPrefix = fields?.humanPrefix ?? this.humanPrefix;\n    this.aiPrefix = fields?.aiPrefix ?? this.aiPrefix;\n    this.memoryKey = fields?.memoryKey ?? this.memoryKey;\n    this.maxTokenLimit = fields?.maxTokenLimit ?? this.maxTokenLimit;\n  }\n\n  get memoryKeys() {\n    return [this.memoryKey];\n  }\n\n  /**\n   * Loads the memory variables. It takes an `InputValues` object as a\n   * parameter and returns a `Promise` that resolves with a\n   * `MemoryVariables` object.\n   * @param _values `InputValues` object.\n   * @returns A `Promise` that resolves with a `MemoryVariables` object.\n   */\n  async loadMemoryVariables(_values: InputValues): Promise<MemoryVariables> {\n    const messages = await this.chatHistory.getMessages();\n    if (this.returnMessages) {\n      const result = {\n        [this.memoryKey]: messages,\n      };\n      return result;\n    }\n    const result = {\n      [this.memoryKey]: getBufferString(\n        messages,\n        this.humanPrefix,\n        this.aiPrefix\n      ),\n    };\n    return result;\n  }\n\n  /**\n   * Saves the context from this conversation to buffer. If the amount\n   * of tokens required to save the buffer exceeds MAX_TOKEN_LIMIT,\n   * prune it.\n   */\n  async saveContext(inputValues: InputValues, outputValues: OutputValues) {\n    await super.saveContext(inputValues, outputValues);\n\n    // Prune buffer if it exceeds the max token limit set for this instance.\n    const buffer = await this.chatHistory.getMessages();\n    let currBufferLength = await this.llm.getNumTokens(\n      getBufferString(buffer, this.humanPrefix, this.aiPrefix)\n    );\n\n    if (currBufferLength > this.maxTokenLimit) {\n      const prunedMemory = [];\n      while (currBufferLength > this.maxTokenLimit) {\n        prunedMemory.push(buffer.shift());\n        currBufferLength = await this.llm.getNumTokens(\n          getBufferString(buffer, this.humanPrefix, this.aiPrefix)\n        );\n      }\n    }\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAoDA,IAAa,gCAAb,cACU,eAEV;CACE,cAAc;CAEd,WAAW;CAEX,YAAY;CAEZ,gBAAgB;CAEhB;CAEA,YAAYA,QAA4C;EACtD,MAAM,OAAO;EACb,KAAK,MAAM,OAAO;EAClB,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,WAAW,QAAQ,YAAY,KAAK;EACzC,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,gBAAgB,QAAQ,iBAAiB,KAAK;CACpD;CAED,IAAI,aAAa;AACf,SAAO,CAAC,KAAK,SAAU;CACxB;;;;;;;;CASD,MAAM,oBAAoBC,SAAgD;EACxE,MAAM,WAAW,MAAM,KAAK,YAAY,aAAa;AACrD,MAAI,KAAK,gBAAgB;GACvB,MAAMC,WAAS,GACZ,KAAK,YAAY,SACnB;AACD,UAAOA;EACR;EACD,MAAM,SAAS,GACZ,KAAK,YAAY,gBAChB,UACA,KAAK,aACL,KAAK,SACN,CACF;AACD,SAAO;CACR;;;;;;CAOD,MAAM,YAAYC,aAA0BC,cAA4B;EACtE,MAAM,MAAM,YAAY,aAAa,aAAa;EAGlD,MAAM,SAAS,MAAM,KAAK,YAAY,aAAa;EACnD,IAAI,mBAAmB,MAAM,KAAK,IAAI,aACpC,gBAAgB,QAAQ,KAAK,aAAa,KAAK,SAAS,CACzD;AAED,MAAI,mBAAmB,KAAK,eAAe;GACzC,MAAM,eAAe,CAAE;AACvB,UAAO,mBAAmB,KAAK,eAAe;IAC5C,aAAa,KAAK,OAAO,OAAO,CAAC;IACjC,mBAAmB,MAAM,KAAK,IAAI,aAChC,gBAAgB,QAAQ,KAAK,aAAa,KAAK,SAAS,CACzD;GACF;EACF;CACF;AACF"}
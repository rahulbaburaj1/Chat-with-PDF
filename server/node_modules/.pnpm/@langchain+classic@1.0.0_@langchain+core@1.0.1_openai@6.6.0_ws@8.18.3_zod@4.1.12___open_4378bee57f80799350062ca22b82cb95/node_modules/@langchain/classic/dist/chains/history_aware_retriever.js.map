{"version":3,"file":"history_aware_retriever.js","names":[],"sources":["../../src/chains/history_aware_retriever.ts"],"sourcesContent":["import type { LanguageModelLike } from \"@langchain/core/language_models/base\";\nimport {\n  type Runnable,\n  type RunnableInterface,\n  RunnableSequence,\n  RunnableBranch,\n} from \"@langchain/core/runnables\";\nimport { type BasePromptTemplate } from \"@langchain/core/prompts\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport type { DocumentInterface } from \"@langchain/core/documents\";\nimport type { BaseMessage } from \"@langchain/core/messages\";\n\n/**\n * Params for the createHistoryAwareRetriever method.\n */\nexport type CreateHistoryAwareRetrieverParams = {\n  /**\n   * Language model to use for generating a search term given chat history.\n   */\n  llm: LanguageModelLike;\n  /**\n   * RetrieverLike object that takes a string as input and outputs a list of Documents.\n   */\n  retriever: RunnableInterface<string, DocumentInterface[]>;\n  /**\n   * The prompt used to generate the search query for the retriever.\n   */\n  rephrasePrompt: BasePromptTemplate;\n};\n\n/**\n * Create a chain that takes conversation history and returns documents.\n * If there is no `chat_history`, then the `input` is just passed directly to the\n * retriever. If there is `chat_history`, then the prompt and LLM will be used\n * to generate a search query. That search query is then passed to the retriever.\n * @param {CreateHistoryAwareRetriever} params\n * @returns An LCEL Runnable. The runnable input must take in `input`, and if there\n * is chat history should take it in the form of `chat_history`.\n * The Runnable output is a list of Documents\n * @example\n * ```typescript\n * // pnpm add langchain @langchain/openai\n *\n * import { ChatOpenAI } from \"@langchain/openai\";\n * import { pull } from \"langchain/hub\";\n * import { createHistoryAwareRetriever } from \"@langchain/classic/chains/history_aware_retriever\";\n *\n * const rephrasePrompt = await pull(\"langchain-ai/chat-langchain-rephrase\");\n * const llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n * const retriever = ...\n * const chain = await createHistoryAwareRetriever({\n *   llm,\n *   retriever,\n *   rephrasePrompt,\n * });\n * const result = await chain.invoke({\"input\": \"...\", \"chat_history\": [] })\n * ```\n */\nexport async function createHistoryAwareRetriever({\n  llm,\n  retriever,\n  rephrasePrompt,\n}: CreateHistoryAwareRetrieverParams): Promise<\n  Runnable<\n    { input: string; chat_history: string | BaseMessage[] },\n    DocumentInterface[]\n  >\n> {\n  if (!rephrasePrompt.inputVariables.includes(\"input\")) {\n    throw new Error(\n      `Expected \"input\" to be a prompt variable, but got ${JSON.stringify(\n        rephrasePrompt.inputVariables\n      )}`\n    );\n  }\n  const retrieveDocuments = RunnableBranch.from([\n    [\n      (input) => !input.chat_history || input.chat_history.length === 0,\n      RunnableSequence.from([(input) => input.input, retriever]),\n    ],\n    RunnableSequence.from([\n      rephrasePrompt,\n      llm,\n      new StringOutputParser(),\n      retriever,\n    ]),\n  ]).withConfig({\n    runName: \"history_aware_retriever\",\n  });\n  return retrieveDocuments;\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA0DA,eAAsB,4BAA4B,EAChD,KACA,WACA,gBACkC,EAKlC;AACA,KAAI,CAAC,eAAe,eAAe,SAAS,QAAQ,CAClD,OAAM,IAAI,MACR,CAAC,kDAAkD,EAAE,KAAK,UACxD,eAAe,eAChB,EAAE;CAGP,MAAM,oBAAoB,eAAe,KAAK,CAC5C,CACE,CAAC,UAAU,CAAC,MAAM,gBAAgB,MAAM,aAAa,WAAW,GAChE,iBAAiB,KAAK,CAAC,CAAC,UAAU,MAAM,OAAO,SAAU,EAAC,AAC3D,GACD,iBAAiB,KAAK;EACpB;EACA;EACA,IAAI;EACJ;CACD,EAAC,AACH,EAAC,CAAC,WAAW,EACZ,SAAS,0BACV,EAAC;AACF,QAAO;AACR"}
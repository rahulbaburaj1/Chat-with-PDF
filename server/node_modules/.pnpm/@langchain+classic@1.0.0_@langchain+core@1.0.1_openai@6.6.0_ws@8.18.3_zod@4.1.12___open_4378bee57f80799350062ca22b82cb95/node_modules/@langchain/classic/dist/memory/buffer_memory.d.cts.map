{"version":3,"file":"buffer_memory.d.cts","names":["InputValues","MemoryVariables","BaseChatMemory","BaseChatMemoryInput","BufferMemoryInput","BufferMemory","Promise"],"sources":["../../src/memory/buffer_memory.d.ts"],"sourcesContent":["import { InputValues, MemoryVariables } from \"@langchain/core/memory\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n/**\n * Interface for the input parameters of the `BufferMemory` class.\n */\nexport interface BufferMemoryInput extends BaseChatMemoryInput {\n    humanPrefix?: string;\n    aiPrefix?: string;\n    memoryKey?: string;\n}\n/**\n * The `BufferMemory` class is a type of memory component used for storing\n * and managing previous chat messages. It is a wrapper around\n * `ChatMessageHistory` that extracts the messages into an input variable.\n * This class is particularly useful in applications like chatbots where\n * it is essential to remember previous interactions. Note: The memory\n * instance represents the history of a single conversation. Therefore, it\n * is not recommended to share the same history or memory instance between\n * two different chains. If you deploy your LangChain app on a serverless\n * environment, do not store memory instances in a variable, as your\n * hosting provider may reset it by the next time the function is called.\n * @example\n * ```typescript\n * // Initialize the memory to store chat history and set up the language model with a specific temperature.\n * const memory = new BufferMemory({ memoryKey: \"chat_history\" });\n * const model = new ChatOpenAI({ model: \"gpt-4o-mini\", temperature: 0.9 });\n *\n * // Create a prompt template for a friendly conversation between a human and an AI.\n * const prompt =\n *   PromptTemplate.fromTemplate(`The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n *\n * Current conversation:\n * {chat_history}\n * Human: {input}\n * AI:`);\n *\n * // Set up the chain with the language model, prompt, and memory.\n * const chain = new LLMChain({ llm: model, prompt, memory });\n *\n * // Example usage of the chain to continue the conversation.\n * // The `call` method sends the input to the model and returns the AI's response.\n * const res = await chain.call({ input: \"Hi! I'm Jim.\" });\n * console.log({ res });\n *\n * ```\n */\nexport declare class BufferMemory extends BaseChatMemory implements BufferMemoryInput {\n    humanPrefix: string;\n    aiPrefix: string;\n    memoryKey: string;\n    constructor(fields?: BufferMemoryInput);\n    get memoryKeys(): string[];\n    /**\n     * Loads the memory variables. It takes an `InputValues` object as a\n     * parameter and returns a `Promise` that resolves with a\n     * `MemoryVariables` object.\n     * @param _values `InputValues` object.\n     * @returns A `Promise` that resolves with a `MemoryVariables` object.\n     */\n    loadMemoryVariables(_values: InputValues): Promise<MemoryVariables>;\n}\n"],"mappings":";;;;;;;AAKA;AAyCqBK,UAzCJD,iBAAAA,SAA0BD,mBAyCV,CAAA;EAAA,WAAA,CAAA,EAAA,MAAA;EAAA,QAIRC,CAAAA,EAAAA,MAAAA;EAAiB,SASTJ,CAAAA,EAAAA,MAAAA;;;;;AAboD;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAAhEK,YAAAA,SAAqBH,cAAAA,YAA0BE;;;;uBAI3CA;;;;;;;;;+BASQJ,cAAcM,QAAQL"}
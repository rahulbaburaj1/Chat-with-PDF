{"version":3,"file":"buffer_token_memory.d.ts","names":["BaseLanguageModelInterface","InputValues","MemoryVariables","OutputValues","BaseChatMemory","BaseChatMemoryInput","ConversationTokenBufferMemoryInput","ConversationTokenBufferMemory","Promise"],"sources":["../../src/memory/buffer_token_memory.d.ts"],"sourcesContent":["import type { BaseLanguageModelInterface } from \"@langchain/core/language_models/base\";\nimport { InputValues, MemoryVariables, OutputValues } from \"@langchain/core/memory\";\nimport { BaseChatMemory, BaseChatMemoryInput } from \"./chat_memory.js\";\n/**\n * Interface for the input parameters of the `BufferTokenMemory` class.\n */\nexport interface ConversationTokenBufferMemoryInput extends BaseChatMemoryInput {\n    /* Prefix for human messages in the buffer. */\n    humanPrefix?: string;\n    /* Prefix for AI messages in the buffer. */\n    aiPrefix?: string;\n    /* The LLM for this instance. */\n    llm: BaseLanguageModelInterface;\n    /* Memory key for buffer instance. */\n    memoryKey?: string;\n    /* Maximmum number of tokens allowed in the buffer. */\n    maxTokenLimit?: number;\n}\n/**\n * Class that represents a conversation chat memory with a token buffer.\n * It extends the `BaseChatMemory` class and implements the\n * `ConversationTokenBufferMemoryInput` interface.\n * @example\n * ```typescript\n * const memory = new ConversationTokenBufferMemory({\n *   llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n *   maxTokenLimit: 10,\n * });\n *\n * // Save conversation context\n * await memory.saveContext({ input: \"hi\" }, { output: \"whats up\" });\n * await memory.saveContext({ input: \"not much you\" }, { output: \"not much\" });\n *\n * // Load memory variables\n * const result = await memory.loadMemoryVariables({});\n * console.log(result);\n * ```\n */\nexport declare class ConversationTokenBufferMemory extends BaseChatMemory implements ConversationTokenBufferMemoryInput {\n    humanPrefix: string;\n    aiPrefix: string;\n    memoryKey: string;\n    maxTokenLimit: number; // Default max token limit of 2000 which can be overridden\n    llm: BaseLanguageModelInterface;\n    constructor(fields: ConversationTokenBufferMemoryInput);\n    get memoryKeys(): string[];\n    /**\n     * Loads the memory variables. It takes an `InputValues` object as a\n     * parameter and returns a `Promise` that resolves with a\n     * `MemoryVariables` object.\n     * @param _values `InputValues` object.\n     * @returns A `Promise` that resolves with a `MemoryVariables` object.\n     */\n    loadMemoryVariables(_values: InputValues): Promise<MemoryVariables>;\n    /**\n     * Saves the context from this conversation to buffer. If the amount\n     * of tokens required to save the buffer exceeds MAX_TOKEN_LIMIT,\n     * prune it.\n     */\n    saveContext(inputValues: InputValues, outputValues: OutputValues): Promise<void>;\n}\n"],"mappings":";;;;;;;;AAMA;AAAmD,UAAlCM,kCAAAA,SAA2CD,mBAAT,CAAA;EAAA;EAMhB,WANyBA,CAAAA,EAAAA,MAAAA;EAAmB;EAgC1DE,QAAAA,CAAAA,EAAAA,MAAAA;EAA6B;EAAA,GAKzCP,EA/BAA,0BA+BAA;EAA0B;EACuB,SASzBC,CAAAA,EAAAA,MAAAA;EAAW;EAA0B,aAAvBO,CAAAA,EAAAA,MAAAA;;;;;;AAfwE;;;;;;;;;;;;;;;;cAAlGD,6BAAAA,SAAsCH,cAAAA,YAA0BE;;;;;OAK5EN;sBACeM;;;;;;;;;+BASSL,cAAcO,QAAQN;;;;;;2BAM1BD,2BAA2BE,eAAeK"}
{"version":3,"file":"config.d.cts","names":["BaseLanguageModel","RunnableConfig","Example","Run","EvaluationResult","RunEvaluator","Criteria","CriteriaType","EmbeddingDistanceEvalChainInput","LoadEvaluatorOptions","EvaluatorType","EvaluatorInputs","EvaluatorInputFormatter","rawInput","rawPrediction","rawReferenceOutput","run","DynamicRunEvaluatorParams","Record","Input","Prediction","Reference","RunEvaluatorLike","Promise","isOffTheShelfEvaluator","T","EvalConfig","U","isCustomEvaluator","RunEvalType","RunEvalConfig","CriteriaEvalChainConfig","LabeledCriteria","Partial","Pick","EmbeddingDistance"],"sources":["../../src/smith/config.d.ts"],"sourcesContent":["import { BaseLanguageModel } from \"@langchain/core/language_models/base\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport { Example, Run } from \"langsmith\";\nimport { EvaluationResult, RunEvaluator } from \"langsmith/evaluation\";\nimport { Criteria as CriteriaType, type EmbeddingDistanceEvalChainInput } from \"../evaluation/index.js\";\nimport { LoadEvaluatorOptions } from \"../evaluation/loader.js\";\nimport { EvaluatorType } from \"../evaluation/types.js\";\nexport type EvaluatorInputs = {\n    input?: string | unknown;\n    prediction: string | unknown;\n    reference?: string | unknown;\n};\nexport type EvaluatorInputFormatter = ({ rawInput, rawPrediction, rawReferenceOutput, run }: {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    rawInput: any;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    rawPrediction: any;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    rawReferenceOutput?: any;\n    run: Run;\n}) => EvaluatorInputs;\nexport type DynamicRunEvaluatorParams<\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nInput extends Record<string, any> = Record<string, unknown>, \n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nPrediction extends Record<string, any> = Record<string, unknown>, \n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nReference extends Record<string, any> = Record<string, unknown>> = {\n    input: Input;\n    prediction?: Prediction;\n    reference?: Reference;\n    run: Run;\n    example?: Example;\n};\n/**\n * Type of a function that can be coerced into a RunEvaluator function.\n * While we have the class-based RunEvaluator, it's often more convenient to directly\n * pass a function to the runner. This type allows us to do that.\n */\nexport type RunEvaluatorLike = ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => Promise<EvaluationResult>) | ((props: DynamicRunEvaluatorParams, options: RunnableConfig) => EvaluationResult);\nexport declare function isOffTheShelfEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is T | EvalConfig;\nexport declare function isCustomEvaluator<T extends keyof EvaluatorType, U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike>(evaluator: T | EvalConfig | U): evaluator is U;\nexport type RunEvalType<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = T | EvalConfig | U;\n/**\n * Configuration class for running evaluations on datasets.\n *\n * @remarks\n * RunEvalConfig in LangSmith is a configuration class for running evaluations on datasets. Its primary purpose is to define the parameters and evaluators that will be applied during the evaluation of a dataset. This configuration can include various evaluators, custom evaluators, and different keys for inputs, predictions, and references.\n *\n * @typeparam T - The type of evaluators.\n * @typeparam U - The type of custom evaluators.\n */\nexport type RunEvalConfig<T extends keyof EvaluatorType = \"criteria\" | \"labeled_criteria\" | \"embedding_distance\", U extends RunEvaluator | RunEvaluatorLike = RunEvaluator | RunEvaluatorLike> = {\n    /**\n     * Evaluators to apply to a dataset run.\n     * You can optionally specify these by name, or by\n     * configuring them with an EvalConfig object.\n     */\n    evaluators?: RunEvalType<T, U>[];\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs?: EvaluatorInputFormatter;\n    /**\n     * Custom evaluators to apply to a dataset run.\n     * Each evaluator is provided with a run trace containing the model\n     * outputs, as well as an \"example\" object representing a record\n     * in the dataset.\n     *\n     * @deprecated Use `evaluators` instead.\n     */\n    customEvaluators?: U[];\n};\nexport interface EvalConfig extends LoadEvaluatorOptions {\n    /**\n     * The name of the evaluator to use.\n     * Example: labeled_criteria, criteria, etc.\n     */\n    evaluatorType: keyof EvaluatorType;\n    /**\n     * The feedback (or metric) name to use for the logged\n     * evaluation results. If none provided, we default to\n     * the evaluationName.\n     */\n    feedbackKey?: string;\n    /**\n     * Convert the evaluation data into formats that can be used by the evaluator.\n     * This should most commonly be a string.\n     * Parameters are the raw input from the run, the raw output, raw reference output, and the raw run.\n     * @example\n     * ```ts\n     * // Chain input: { input: \"some string\" }\n     * // Chain output: { output: \"some output\" }\n     * // Reference example output format: { output: \"some reference output\" }\n     * const formatEvaluatorInputs = ({\n     *   rawInput,\n     *   rawPrediction,\n     *   rawReferenceOutput,\n     * }) => {\n     *   return {\n     *     input: rawInput.input,\n     *     prediction: rawPrediction.output,\n     *     reference: rawReferenceOutput.output,\n     *   };\n     * };\n     * ```\n     * @returns The prepared data.\n     */\n    formatEvaluatorInputs: EvaluatorInputFormatter;\n}\n/**\n * Configuration to load a \"CriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [Criteria(\"helpfulness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     Criteria({\n *       \"isCompliant\": \"Does the submission comply with the requirements of XYZ\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: \"helpfulness\"\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"criteria\",\n *     criteria: { \"isCompliant\": \"Does the submission comply with the requirements of XYZ\" },\n *     formatEvaluatorInputs: ...\n *   }]\n * };\n */\nexport type Criteria = EvalConfig & {\n    evaluatorType: \"criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/criteria-evaluator\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\n// for compatibility reasons\nexport type CriteriaEvalChainConfig = Criteria;\nexport declare function Criteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): EvalConfig;\n/**\n * Configuration to load a \"LabeledCriteriaEvalChain\" evaluator,\n * which prompts an LLM to determine whether the model's\n * prediction complies with the provided criteria and also\n * provides a \"ground truth\" label for the evaluator to incorporate\n * in its evaluation.\n * @param criteria - The criteria to use for the evaluator.\n * @param llm - The language model to use for the evaluator.\n * @returns The configuration for the evaluator.\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [LabeledCriteria(\"correctness\")],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [\n *     LabeledCriteria({\n *       \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\"\n *     })\n *   ],\n * };\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: \"correctness\",\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n * ```\n * @example\n * ```ts\n * const evalConfig = {\n *   evaluators: [{\n *     evaluatorType: \"labeled_criteria\",\n *     criteria: { \"mentionsAllFacts\": \"Does the include all facts provided in the reference?\" },\n *     formatEvaluatorInputs: ...\n *   }],\n * };\n */\nexport type LabeledCriteria = EvalConfig & {\n    evaluatorType: \"labeled_criteria\";\n    /**\n     * The \"criteria\" to insert into the prompt template\n     * used for evaluation. See the prompt at\n     * https://smith.langchain.com/hub/langchain-ai/labeled-criteria\n     * for more information.\n     */\n    criteria?: CriteriaType | Record<string, string>;\n    /**\n     * The language model to use as the evaluator, defaults to GPT-4\n     */\n    llm?: BaseLanguageModel;\n};\nexport declare function LabeledCriteria(criteria: CriteriaType | Record<string, string>, config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"llm\" | \"feedbackKey\">): LabeledCriteria;\n/**\n * Configuration to load a \"EmbeddingDistanceEvalChain\" evaluator,\n * which embeds distances to score semantic difference between\n * a prediction and reference.\n */\nexport type EmbeddingDistance = EvalConfig & EmbeddingDistanceEvalChainInput & {\n    evaluatorType: \"embedding_distance\";\n};\nexport declare function EmbeddingDistance(distanceMetric: EmbeddingDistanceEvalChainInput[\"distanceMetric\"], config?: Pick<Partial<LabeledCriteria>, \"formatEvaluatorInputs\" | \"embedding\" | \"feedbackKey\">): EmbeddingDistance;\n"],"mappings":";;;;;;;;;;KAOYW,eAAAA;;;EAAAA,SAAAA,CAAAA,EAAAA,MAAe,GAAA,OAAA;AAK3B,CAAA;AAAmC,KAAvBC,uBAAAA,GAAuB,CAAA;EAAA,QAAA;EAAA,aAAA;EAAA,kBAAA;EAAA;AASnC,CATmC,EAAA;EAAA;EAAc,QAAEE,EAAAA,GAAAA;EAAa;EAAoB,aAAEE,EAAAA,GAAAA;EAAG;EAO7E,kBACNL,CAAAA,EAAAA,GAAAA;EAAe,GAAA,EADZR,GACY;AACrB,CAAA,EAAA,GADMQ,eACMM;AAAyB,KAAzBA,yBAAyB;;cAEvBC,MAAsBA,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GAAAA,MAAAA,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA;;mBAEjBA,MAAsBA,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GAAAA,MAAAA,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA;;kBAEvBA,MAAsBA,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GAAAA,MAAAA,CAAAA,MAAAA,EAAAA,OAAAA,CAAAA,CAAAA,GAAAA;EAAM,KACnCC,EAAAA,KAAAA;EAAK,UACCC,CAAAA,EAAAA,UAAAA;EAAU,SACXC,CAAAA,EAAAA,SAAAA;EAAS,GAChBlB,EAAAA,GAAAA;EAAG,OACED,CAAAA,EAAAA,OAAAA;AAAO,CAAA;AAOrB;;;;;AAA+FqB,KAAnFD,gBAAAA,GAAmFC,CAAAA,CAAAA,KAAAA,EAAvDN,yBAAuDM,EAAAA,OAAAA,EAAnBtB,cAAmBsB,EAAAA,GAAAA,OAAAA,CAAQnB,gBAARmB,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,EAAsCN,yBAAtCM,EAAAA,OAAAA,EAA0EtB,cAA1EsB,EAAAA,GAA6FnB,gBAA7FmB,CAAAA;AAAsCN,iBAC7GO,sBAD6GP,CAAAA,UAAAA,MACtEP,aADsEO,EAAAA,UAC7CZ,YAD6CY,GAC9BK,gBAD8BL,GACXZ,YADWY,GACIK,gBADJL,CAAAA,CAAAA,SAAAA,EACiCQ,CADjCR,GACqCS,UADrCT,GACkDU,CADlDV,CAAAA,EAAAA,SAAAA,IACmEQ,CADnER,GACuES,UADvET;AAAoChB,iBAEjJ2B,iBAFiJ3B,CAAAA,UAAAA,MAE/GS,aAF+GT,EAAAA,UAEtFI,YAFsFJ,GAEvEqB,gBAFuErB,GAEpDI,YAFoDJ,GAErCqB,gBAFqCrB,CAAAA,CAAAA,SAAAA,EAERwB,CAFQxB,GAEJyB,UAFIzB,GAES0B,CAFT1B,CAAAA,EAAAA,SAAAA,IAE0B0B,CAF1B1B;AAAmBG,KAGhLyB,WAHgLzB,CAAAA,UAAAA,MAGpJM,aAHoJN,GAAAA,UAAAA,GAAAA,kBAAAA,GAAAA,oBAAAA,EAAAA,UAGlEC,YAHkED,GAGnDkB,gBAHmDlB,GAGhCC,YAHgCD,GAGjBkB,gBAHiBlB,CAAAA,GAGGqB,CAHHrB,GAGOsB,UAHPtB,GAGoBuB,CAHpBvB;AAAgB;AAC5M;;;;;;;;AAA0KsB,KAY9JI,aAZ8JJ,CAAAA,UAAAA,MAYhIhB,aAZgIgB,GAAAA,UAAAA,GAAAA,kBAAAA,GAAAA,oBAAAA,EAAAA,UAY9CrB,YAZ8CqB,GAY/BJ,gBAZ+BI,GAYZrB,YAZYqB,GAYGJ,gBAZHI,CAAAA,GAAAA;EAAU;;;AAAkC;AACtN;EAAyC,UAAA,CAAA,EAiBxBG,WAjBwB,CAiBZJ,CAjBY,EAiBTE,CAjBS,CAAA,EAAA;EAAA;;;;;;;;;AAA2J;AACpM;;;;;;;;;;AAAiN;AAUjN;;EAAyB,qBAAiBjB,CAAAA,EA8BdE,uBA9BcF;EAAa;;;;;;;;EA8BJ,gBAS5BiB,CAAAA,EAAAA,CAAAA,EAAAA;AAAC,CAAA;AAEPD,UAAAA,UAAAA,SAAmBjB,oBAAT,CAAA;EAAA;;;;EAA6B,aAAA,EAAA,MAK/BC,aAL+B;EA8E5CJ;;;;;EAQwB,WAI1BN,CAAAA,EAAAA,MAAAA;EAAiB;AAG3B;AACA;;;;;;;;AAAuL;AA4CvL;;;;;;AAY2B;AAE3B;;;;;EAA8H,qBAAvBiC,EArH5ErB,uBAqH4EqB;;;AAA4F;AAMnM;;;;AAA4E;AAG5E;;;;;;;AAA+N;;;;;;;;;;;;;;;;;;;;;;;;;;;KAnFnN3B,UAAAA,GAAWoB;;;;;;;;aAQRnB,WAAeW;;;;QAIpBlB;;;KAGE+B,uBAAAA,GAA0BzB;iBACdA,UAAAA,WAAmBC,WAAeW,iCAAiCgB,KAAKD,QAAQD,qEAAqEN;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;KA4CjKM,eAAAA,GAAkBN;;;;;;;;aAQfnB,WAAeW;;;;QAIpBlB;;iBAEcgC,eAAAA,WAA0BzB,WAAeW,iCAAiCgB,KAAKD,QAAQD,qEAAqEA;;;;;;KAMxKG,iBAAAA,GAAoBT,aAAalB;;;iBAGrB2B,iBAAAA,iBAAkC3B,4DAA4D0B,KAAKD,QAAQD,2EAA2EG"}
{"version":3,"file":"metadata.d.ts","names":["MessageOutputVersion","ResponseMetadata","mergeResponseMetadata","ModalitiesTokenDetails","InputTokenDetails","OutputTokenDetails","UsageMetadata","mergeUsageMetadata"],"sources":["../../../../../../langchain-core/dist/messages/metadata.d.ts"],"sourcesContent":["import { MessageOutputVersion } from \"./message.js\";\n\n//#region src/messages/metadata.d.ts\ntype ResponseMetadata = {\n  model_provider?: string;\n  model_name?: string;\n  output_version?: MessageOutputVersion;\n  [key: string]: unknown;\n};\ndeclare function mergeResponseMetadata(a?: ResponseMetadata, b?: ResponseMetadata): ResponseMetadata;\ntype ModalitiesTokenDetails = {\n  /**\n   * Text tokens.\n   * Does not need to be reported, but some models will do so.\n   */\n  text?: number;\n  /**\n   * Image (non-video) tokens.\n   */\n  image?: number;\n  /**\n   * Audio tokens.\n   */\n  audio?: number;\n  /**\n   * Video tokens.\n   */\n  video?: number;\n  /**\n   * Document tokens.\n   * e.g. PDF\n   */\n  document?: number;\n};\n/**\n * Breakdown of input token counts.\n *\n * Does not *need* to sum to full input token count. Does *not* need to have all keys.\n */\ntype InputTokenDetails = ModalitiesTokenDetails & {\n  /**\n   * Input tokens that were cached and there was a cache hit.\n   *\n   * Since there was a cache hit, the tokens were read from the cache.\n   * More precisely, the model state given these tokens was read from the cache.\n   */\n  cache_read?: number;\n  /**\n   * Input tokens that were cached and there was a cache miss.\n   *\n   * Since there was a cache miss, the cache was created from these tokens.\n   */\n  cache_creation?: number;\n};\n/**\n * Breakdown of output token counts.\n *\n * Does *not* need to sum to full output token count. Does *not* need to have all keys.\n */\ntype OutputTokenDetails = ModalitiesTokenDetails & {\n  /**\n   * Reasoning output tokens.\n   *\n   * Tokens generated by the model in a chain of thought process (i.e. by\n   * OpenAI's o1 models) that are not returned as part of model output.\n   */\n  reasoning?: number;\n};\n/**\n * Usage metadata for a message, such as token counts.\n */\ntype UsageMetadata = {\n  /**\n   * Count of input (or prompt) tokens. Sum of all input token types.\n   */\n  input_tokens: number;\n  /**\n   * Count of output (or completion) tokens. Sum of all output token types.\n   */\n  output_tokens: number;\n  /**\n   * Total token count. Sum of input_tokens + output_tokens.\n   */\n  total_tokens: number;\n  /**\n   * Breakdown of input token counts.\n   *\n   * Does *not* need to sum to full input token count. Does *not* need to have all keys.\n   */\n  input_token_details?: InputTokenDetails;\n  /**\n   * Breakdown of output token counts.\n   *\n   * Does *not* need to sum to full output token count. Does *not* need to have all keys.\n   */\n  output_token_details?: OutputTokenDetails;\n};\ndeclare function mergeUsageMetadata(a?: UsageMetadata, b?: UsageMetadata): UsageMetadata;\n//#endregion\nexport { InputTokenDetails, ModalitiesTokenDetails, OutputTokenDetails, ResponseMetadata, UsageMetadata, mergeResponseMetadata, mergeUsageMetadata };\n//# sourceMappingURL=metadata.d.ts.map"],"mappings":";;;;AAAoD,KAG/CC,gBAAAA,GAAgB;EAM+E,cAC/FE,CAAAA,EAAAA,MAAAA;EAAsB,UA6BtBC,CAAAA,EAAAA,MAAAA;EAA0C,cAoB1CC,CAAAA,EArDcL,oBAqDOG;EAAsB,CAAA,GAY3CG,EAAAA,MAAAA,CAAAA,EAAAA,OAAa;CAAA;KA7DbH,sBAAAA,GAqFoBE;EAAkB;;;;;;;;;;;;;;;;;;;;;;;;;;;;KAxDtCD,iBAAAA,GAAoBD;;;;;;;;;;;;;;;;;;;;KAoBpBE,kBAAAA,GAAqBF;;;;;;;;;;;;KAYrBG,aAAAA;;;;;;;;;;;;;;;;;;wBAkBmBF;;;;;;yBAMCC"}
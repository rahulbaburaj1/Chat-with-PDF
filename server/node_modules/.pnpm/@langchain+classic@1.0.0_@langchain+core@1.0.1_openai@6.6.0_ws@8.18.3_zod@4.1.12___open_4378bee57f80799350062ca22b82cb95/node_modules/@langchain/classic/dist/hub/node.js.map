{"version":3,"file":"node.js","names":["ownerRepoCommit: string","options?: {\n    apiKey?: string;\n    apiUrl?: string;\n    includeModel?: boolean;\n  }"],"sources":["../../src/hub/node.ts"],"sourcesContent":["import { Runnable } from \"@langchain/core/runnables\";\nimport {\n  basePull,\n  generateModelImportMap,\n  generateOptionalImportMap,\n  bindOutputSchema,\n} from \"./base.js\";\nimport { load } from \"../load/index.js\";\nimport { getChatModelByClassName } from \"../chat_models/universal.js\";\n\n// TODO: Make this the default, add web entrypoint in next breaking release\n\nexport { basePush as push } from \"./base.js\";\n\n/**\n * Pull a prompt from the hub.\n * @param ownerRepoCommit The name of the repo containing the prompt, as well as an optional commit hash separated by a slash.\n * @param options.apiKey LangSmith API key to use when pulling the prompt\n * @param options.apiUrl LangSmith API URL to use when pulling the prompt\n * @param options.includeModel Whether to also instantiate and attach a model instance to the prompt,\n *   if the prompt has associated model metadata. If set to true, invoking the resulting pulled prompt will\n *   also invoke the instantiated model. You must have the appropriate LangChain integration package installed.\n * @returns\n */\nexport async function pull<T extends Runnable>(\n  ownerRepoCommit: string,\n  options?: {\n    apiKey?: string;\n    apiUrl?: string;\n    includeModel?: boolean;\n  }\n) {\n  const promptObject = await basePull(ownerRepoCommit, options);\n  let modelClass;\n  if (options?.includeModel) {\n    if (Array.isArray(promptObject.manifest.kwargs?.last?.kwargs?.bound?.id)) {\n      const modelName =\n        promptObject.manifest.kwargs?.last?.kwargs?.bound?.id.at(-1);\n\n      if (modelName) {\n        modelClass = await getChatModelByClassName(modelName);\n        if (!modelClass) {\n          console.warn(\n            `Received unknown model name from prompt hub: \"${modelName}\"`\n          );\n        }\n      }\n    }\n  }\n  const loadedPrompt = await load<T>(\n    JSON.stringify(promptObject.manifest),\n    undefined,\n    generateOptionalImportMap(modelClass),\n    generateModelImportMap(modelClass)\n  );\n  return bindOutputSchema(loadedPrompt);\n}\n"],"mappings":";;;;;;;;;;;;;;;AAwBA,eAAsB,KACpBA,iBACAC,SAKA;CACA,MAAM,eAAe,MAAM,SAAS,iBAAiB,QAAQ;CAC7D,IAAI;AACJ,KAAI,SAAS,cACX;MAAI,MAAM,QAAQ,aAAa,SAAS,QAAQ,MAAM,QAAQ,OAAO,GAAG,EAAE;GACxE,MAAM,YACJ,aAAa,SAAS,QAAQ,MAAM,QAAQ,OAAO,GAAG,GAAG,GAAG;AAE9D,OAAI,WAAW;IACb,aAAa,MAAM,wBAAwB,UAAU;AACrD,QAAI,CAAC,YACH,QAAQ,KACN,CAAC,8CAA8C,EAAE,UAAU,CAAC,CAAC,CAC9D;GAEJ;EACF;;CAEH,MAAM,eAAe,MAAM,KACzB,KAAK,UAAU,aAAa,SAAS,EACrC,QACA,0BAA0B,WAAW,EACrC,uBAAuB,WAAW,CACnC;AACD,QAAO,iBAAiB,aAAa;AACtC"}
{"version":3,"file":"conversational_retrieval_chain.d.ts","names":["BaseLanguageModelInterface","BaseRetrieverInterface","BaseMessage","ChainValues","CallbackManagerForChainRun","SerializedChatVectorDBQAChain","BaseChain","ChainInputs","LLMChain","QAChainParams","LoadValues","Record","ConversationalRetrievalQAChainInput","ConversationalRetrievalQAChain","Promise","Omit"],"sources":["../../src/chains/conversational_retrieval_chain.d.ts"],"sourcesContent":["import type { BaseLanguageModelInterface } from \"@langchain/core/language_models/base\";\nimport type { BaseRetrieverInterface } from \"@langchain/core/retrievers\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChainValues } from \"@langchain/core/utils/types\";\nimport { CallbackManagerForChainRun } from \"@langchain/core/callbacks/manager\";\nimport { SerializedChatVectorDBQAChain } from \"./serde.js\";\nimport { BaseChain, ChainInputs } from \"./base.js\";\nimport { LLMChain } from \"./llm_chain.js\";\nimport { QAChainParams } from \"./question_answering/load.js\";\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\nexport type LoadValues = Record<string, any>;\n/**\n * Interface for the input parameters of the\n * ConversationalRetrievalQAChain class.\n */\nexport interface ConversationalRetrievalQAChainInput extends ChainInputs {\n    retriever: BaseRetrieverInterface;\n    combineDocumentsChain: BaseChain;\n    questionGeneratorChain: LLMChain;\n    returnSourceDocuments?: boolean;\n    returnGeneratedQuestion?: boolean;\n    inputKey?: string;\n}\n/**\n * Class for conducting conversational question-answering tasks with a\n * retrieval component. Extends the BaseChain class and implements the\n * ConversationalRetrievalQAChainInput interface.\n * @example\n * ```typescript\n * import { ChatAnthropic } from \"@langchain/anthropic\";\n * import {\n *   ChatPromptTemplate,\n *   MessagesPlaceholder,\n * } from \"@langchain/core/prompts\";\n * import { BaseMessage } from \"@langchain/core/messages\";\n * import { createStuffDocumentsChain } from \"@langchain/classic/chains/combine_documents\";\n * import { createHistoryAwareRetriever } from \"@langchain/classic/chains/history_aware_retriever\";\n * import { createRetrievalChain } from \"@langchain/classic/chains/retrieval\";\n *\n * const retriever = ...your retriever;\n * const llm = new ChatAnthropic();\n *\n * // Contextualize question\n * const contextualizeQSystemPrompt = `\n * Given a chat history and the latest user question\n * which might reference context in the chat history,\n * formulate a standalone question which can be understood\n * without the chat history. Do NOT answer the question, just\n * reformulate it if needed and otherwise return it as is.`;\n * const contextualizeQPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", contextualizeQSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n * const historyAwareRetriever = await createHistoryAwareRetriever({\n *   llm,\n *   retriever,\n *   rephrasePrompt: contextualizeQPrompt,\n * });\n *\n * // Answer question\n * const qaSystemPrompt = `\n * You are an assistant for question-answering tasks. Use\n * the following pieces of retrieved context to answer the\n * question. If you don't know the answer, just say that you\n * don't know. Use three sentences maximum and keep the answer\n * concise.\n * \\n\\n\n * {context}`;\n * const qaPrompt = ChatPromptTemplate.fromMessages([\n *   [\"system\", qaSystemPrompt],\n *   new MessagesPlaceholder(\"chat_history\"),\n *   [\"human\", \"{input}\"],\n * ]);\n *\n * // Below we use createStuffDocuments_chain to feed all retrieved context\n * // into the LLM. Note that we can also use StuffDocumentsChain and other\n * // instances of BaseCombineDocumentsChain.\n * const questionAnswerChain = await createStuffDocumentsChain({\n *   llm,\n *   prompt: qaPrompt,\n * });\n *\n * const ragChain = await createRetrievalChain({\n *   retriever: historyAwareRetriever,\n *   combineDocsChain: questionAnswerChain,\n * });\n *\n * // Usage:\n * const chat_history: BaseMessage[] = [];\n * const response = await ragChain.invoke({\n *   chat_history,\n *   input: \"...\",\n * });\n * ```\n */\nexport declare class ConversationalRetrievalQAChain extends BaseChain implements ConversationalRetrievalQAChainInput {\n    static lc_name(): string;\n    inputKey: string;\n    chatHistoryKey: string;\n    get inputKeys(): string[];\n    get outputKeys(): string[];\n    retriever: BaseRetrieverInterface;\n    combineDocumentsChain: BaseChain;\n    questionGeneratorChain: LLMChain;\n    returnSourceDocuments: boolean;\n    returnGeneratedQuestion: boolean;\n    constructor(fields: ConversationalRetrievalQAChainInput);\n    /**\n     * Static method to convert the chat history input into a formatted\n     * string.\n     * @param chatHistory Chat history input which can be a string, an array of BaseMessage instances, or an array of string arrays.\n     * @returns A formatted string representing the chat history.\n     */\n    static getChatHistoryString(chatHistory: string | BaseMessage[] | string[][]): string;\n    /** @ignore */\n    _call(values: ChainValues, runManager?: CallbackManagerForChainRun): Promise<ChainValues>;\n    _chainType(): string;\n    static deserialize(_data: SerializedChatVectorDBQAChain, _values: LoadValues): Promise<ConversationalRetrievalQAChain>;\n    serialize(): SerializedChatVectorDBQAChain;\n    /**\n     * Static method to create a new ConversationalRetrievalQAChain from a\n     * BaseLanguageModel and a BaseRetriever.\n     * @param llm {@link BaseLanguageModelInterface} instance used to generate a new question.\n     * @param retriever {@link BaseRetrieverInterface} instance used to retrieve relevant documents.\n     * @param options.returnSourceDocuments Whether to return source documents in the final output\n     * @param options.questionGeneratorChainOptions Options to initialize the standalone question generation chain used as the first internal step\n     * @param options.qaChainOptions {@link QAChainParams} used to initialize the QA chain used as the second internal step\n     * @returns A new instance of ConversationalRetrievalQAChain.\n     */\n    static fromLLM(llm: BaseLanguageModelInterface, retriever: BaseRetrieverInterface, options?: {\n        outputKey?: string; // not used\n        returnSourceDocuments?: boolean;\n        /** @deprecated Pass in questionGeneratorChainOptions.template instead */\n        questionGeneratorTemplate?: string;\n        /** @deprecated Pass in qaChainOptions.prompt instead */\n        qaTemplate?: string;\n        questionGeneratorChainOptions?: {\n            llm?: BaseLanguageModelInterface;\n            template?: string;\n        };\n        qaChainOptions?: QAChainParams;\n    } & Omit<ConversationalRetrievalQAChainInput, \"retriever\" | \"combineDocumentsChain\" | \"questionGeneratorChain\">): ConversationalRetrievalQAChain;\n}\n"],"mappings":";;;;;;;;;;;;KAUYU,UAAAA,GAAaC;AAAzB;AAKA;;;AAE2BL,UAFVM,mCAAAA,SAA4CL,WAElCD,CAAAA;EAAS,SACRE,EAFbP,sBAEaO;EAAQ,qBAHyBD,EAElCD,SAFkCC;EAAW,sBAAA,EAG5CC,QAH4C;EAiFnDK,qBAAAA,CAAAA,EAAAA,OAAAA;EAA8B,uBAAA,CAAA,EAAA,OAAA;EAAA,QAMpCZ,CAAAA,EAAAA,MAAAA;;;;;;;;;;;;;;;;;;;;;;;AANqG;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAA/FY,8BAAAA,SAAuCP,SAAAA,YAAqBM;;;;;;aAMlEX;yBACYK;0BACCE;;;sBAGJI;;;;;;;oDAO8BV;;gBAEpCC,0BAA0BC,6BAA6BU,QAAQX;;4BAEnDE,wCAAwCK,aAAaI,QAAQD;eAC1ER;;;;;;;;;;;sBAWOL,uCAAuCC;;;;;;;;YAQ7CD;;;qBAGOS;MACjBM,KAAKH,yGAAyGC"}
{"version":3,"file":"chat_models.cjs","names":["BaseChatModel","options: this[\"ParsedCallOptions\"]","fields?: BaseChatOpenAIFields","options?: this[\"ParsedCallOptions\"]","isReasoningModel","reasoning: OpenAIClient.Reasoning | undefined","resFormat?: CallOptions[\"response_format\"]","interopZodResponseFormat","additionalOptions?: this[\"ParsedCallOptions\"]","options: OpenAICoreRequestOptions | undefined","openAIEndpointConfig: OpenAIEndpointConfig","getEndpoint","OpenAIClient","tool: ChatOpenAIToolType","fields?: { strict?: boolean }","isCustomTool","convertResponsesCustomTool","_convertToOpenAITool","tools: ChatOpenAIToolType[]","kwargs?: Partial<CallOptions>","strict: boolean | undefined","isBuiltInTool","input: BaseLanguageModelInput","options?: CallOptions","messages: BaseMessage[]","messageToOpenAIRole","generations: ChatGeneration[]","functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[]","function_call?:\n      | \"none\"\n      | \"auto\"\n      | OpenAIClient.Chat.ChatCompletionFunctionCallOption","formatFunctionDefinitions","config: StructuredOutputMethodOptions<boolean>","outputSchema: InteropZodType<RunOutput> | Record<string, unknown>","config?: StructuredOutputMethodOptions<boolean>","llm: Runnable<BaseLanguageModelInput>","outputParser: Runnable<AIMessageChunk, RunOutput>","getStructuredOutputMethod","StructuredOutputParser","JsonOutputParser","RunnableLambda","aiMessage: AIMessageChunk","JsonOutputKeyToolsParser","openAIFunctionDefinition: FunctionDefinition","RunnablePassthrough","input: any","config","RunnableSequence","params: ChatResponsesInvocationParams","isBuiltInToolChoice","formatToOpenAIToolChoice","finalChunk: ChatGenerationChunk | undefined","runManager?: CallbackManagerForLLMRun","request: OpenAIClient.Responses.ResponseCreateParams","requestOptions?: OpenAIClient.RequestOptions","wrapOpenAIClientError","response: ResponsesCreateInvoke | ResponsesParseInvoke","messageId: string | undefined","content: MessageContent","tool_calls: ToolCall[]","invalid_tool_calls: InvalidToolCall[]","response_metadata: Record<string, unknown>","additional_kwargs: {\n      [key: string]: unknown;\n      refusal?: string;\n      reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n      tool_outputs?: unknown[];\n      parsed?: unknown;\n      [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n    }","e: unknown","errMessage: string | undefined","parseCustomToolCall","AIMessage","_convertOpenAIResponsesUsageToLangChainUsage","chunk: OpenAIClient.Responses.ResponseStreamEvent","content: Record<string, unknown>[]","generationInfo: Record<string, unknown>","usage_metadata: UsageMetadata | undefined","tool_call_chunks: ToolCallChunk[]","additional_kwargs: {\n      [key: string]: unknown;\n      reasoning?: Partial<ChatOpenAIReasoningSummary>;\n      tool_outputs?: unknown[];\n    }","id: string | undefined","summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined","ChatGenerationChunk","AIMessageChunk","_convertToResponsesMessageFromV1","input: ResponsesInputItem[]","iife","isCustomToolCall","fallthroughCallTypes: ResponsesInputItem[\"type\"][]","messages: ResponsesInputItem[]","messages","completionsApiContentBlockConverter","reasoning: ChatOpenAIReasoningSummary","fields: { stream?: boolean; strict?: boolean }","reducedTools: ResponsesTool[]","isOpenAICustomTool","convertCompletionsCustomTool","extra?: { streaming?: boolean }","params: Partial<ChatCompletionsInvocationParams>","messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[]","_convertMessagesToOpenAIParams","finalChunks: Record<number, ChatGenerationChunk>","generation: ChatGeneration","defaultRole: OpenAIClient.Chat.ChatCompletionRole | undefined","usage: OpenAIClient.Completions.CompletionUsage | undefined","generationInfo: Record<string, any>","request: OpenAIClient.Chat.ChatCompletionCreateParams","message: OpenAIClient.Chat.Completions.ChatCompletionMessage","rawResponse: OpenAIClient.Chat.Completions.ChatCompletion","rawToolCalls: OpenAIToolCall[] | undefined","e: any","additional_kwargs: Record<string, unknown>","response_metadata: Record<string, unknown> | undefined","handleMultiModalOutput","ChatMessage","delta: Record<string, any>","rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk","defaultRole?: OpenAIClient.Chat.ChatCompletionRole","HumanMessageChunk","toolCallChunks: ToolCallChunk[]","SystemMessageChunk","FunctionMessageChunk","ToolMessageChunk","ChatMessageChunk","fields?: ChatOpenAIFields","options: this[\"ParsedCallOptions\"] | undefined","config: Partial<CallOptions>"],"sources":["../src/chat_models.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  AIMessage,\n  AIMessageChunk,\n  type BaseMessage,\n  ChatMessage,\n  ChatMessageChunk,\n  FunctionMessageChunk,\n  HumanMessageChunk,\n  SystemMessageChunk,\n  ToolMessageChunk,\n  OpenAIToolCall,\n  isAIMessage,\n  type UsageMetadata,\n  type BaseMessageFields,\n  type MessageContent,\n  type InvalidToolCall,\n  MessageContentImageUrl,\n  isDataContentBlock,\n  convertToProviderContentBlock,\n} from \"@langchain/core/messages\";\nimport {\n  ChatGenerationChunk,\n  type ChatGeneration,\n  type ChatResult,\n} from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport {\n  BaseChatModel,\n  type LangSmithParams,\n  type BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  isOpenAITool as isOpenAIFunctionTool,\n  type BaseFunctionCallOptions,\n  type BaseLanguageModelInput,\n  type FunctionDefinition,\n  type StructuredOutputMethodOptions,\n} from \"@langchain/core/language_models/base\";\nimport { NewTokenIndices } from \"@langchain/core/callbacks/base\";\nimport {\n  Runnable,\n  RunnableLambda,\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\nimport {\n  JsonOutputParser,\n  StructuredOutputParser,\n} from \"@langchain/core/output_parsers\";\nimport {\n  JsonOutputKeyToolsParser,\n  makeInvalidToolCall,\n  parseToolCall,\n} from \"@langchain/core/output_parsers/openai_tools\";\nimport type {\n  ToolCall,\n  ToolCallChunk,\n  ToolMessage,\n} from \"@langchain/core/messages/tool\";\nimport {\n  getSchemaDescription,\n  InteropZodType,\n  isInteropZodSchema,\n} from \"@langchain/core/utils/types\";\nimport { toJsonSchema } from \"@langchain/core/utils/json_schema\";\nimport { ResponseInputMessageContentList } from \"openai/resources/responses/responses.js\";\nimport {\n  type OpenAICallOptions,\n  type OpenAIChatInput,\n  type OpenAICoreRequestOptions,\n  type ChatOpenAIResponseFormat,\n  ChatOpenAIReasoningSummary,\n  ResponseFormatConfiguration,\n  OpenAIVerbosityParam,\n} from \"./types.js\";\nimport { type OpenAIEndpointConfig, getEndpoint } from \"./utils/azure.js\";\nimport { wrapOpenAIClientError } from \"./utils/client.js\";\nimport {\n  type FunctionDef,\n  formatFunctionDefinitions,\n  OpenAIToolChoice,\n  formatToOpenAIToolChoice,\n  _convertToOpenAITool,\n  ChatOpenAIToolType,\n  convertCompletionsCustomTool,\n  convertResponsesCustomTool,\n  isBuiltInTool,\n  isBuiltInToolChoice,\n  isCustomTool,\n  isCustomToolCall,\n  isOpenAICustomTool,\n  parseCustomToolCall,\n  ResponsesTool,\n  ResponsesToolChoice,\n} from \"./utils/tools.js\";\nimport {\n  getStructuredOutputMethod,\n  interopZodResponseFormat,\n  handleMultiModalOutput,\n  _convertOpenAIResponsesUsageToLangChainUsage,\n} from \"./utils/output.js\";\nimport {\n  _convertMessagesToOpenAIParams,\n  completionsApiContentBlockConverter,\n  ResponsesInputItem,\n} from \"./utils/message_inputs.js\";\nimport { _convertToResponsesMessageFromV1 } from \"./utils/standard.js\";\nimport { iife, isReasoningModel, messageToOpenAIRole } from \"./utils/misc.js\";\n\nconst _FUNCTION_CALL_IDS_MAP_KEY = \"__openai_function_call_ids__\";\n\ninterface OpenAILLMOutput {\n  tokenUsage: {\n    completionTokens?: number;\n    promptTokens?: number;\n    totalTokens?: number;\n  };\n}\n\nexport type { OpenAICallOptions, OpenAIChatInput };\n\nexport interface BaseChatOpenAICallOptions\n  extends OpenAICallOptions,\n    BaseFunctionCallOptions {\n  /**\n   * A list of tools that the model may use to generate responses.\n   * Each tool can be a function, a built-in tool, or a custom tool definition.\n   * If not provided, the model will not use any tools.\n   */\n  tools?: ChatOpenAIToolType[];\n\n  /**\n   * Specifies which tool the model should use to respond.\n   * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n   * If not set, the model will decide which tool to use automatically.\n   */\n  // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n  tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n\n  /**\n   * Adds a prompt index to prompts passed to the model to track\n   * what prompt is being used for a given generation.\n   */\n  promptIndex?: number;\n\n  /**\n   * An object specifying the format that the model must output.\n   */\n  response_format?: ChatOpenAIResponseFormat;\n\n  /**\n   * When provided, the completions API will make a best effort to sample\n   * deterministically, such that repeated requests with the same `seed`\n   * and parameters should return the same result.\n   */\n  seed?: number;\n\n  /**\n   * Additional options to pass to streamed completions.\n   * If provided, this takes precedence over \"streamUsage\" set at\n   * initialization time.\n   */\n  stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n\n  /**\n   * The model may choose to call multiple functions in a single turn. You can\n   * set parallel_tool_calls to false which ensures only one tool is called at most.\n   * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n   */\n  parallel_tool_calls?: boolean;\n\n  /**\n   * If `true`, model output is guaranteed to exactly match the JSON Schema\n   * provided in the tool definition. If `true`, the input schema will also be\n   * validated according to\n   * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n   *\n   * If `false`, input schema will not be validated and model output will not\n   * be validated.\n   *\n   * If `undefined`, `strict` argument will not be passed to the model.\n   */\n  strict?: boolean;\n\n  /**\n   * Output types that you would like the model to generate for this request. Most\n   * models are capable of generating text, which is the default:\n   *\n   * `[\"text\"]`\n   *\n   * The `gpt-4o-audio-preview` model can also be used to\n   * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n   * this model generate both text and audio responses, you can use:\n   *\n   * `[\"text\", \"audio\"]`\n   */\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  /**\n   * Parameters for audio output. Required when audio output is requested with\n   * `modalities: [\"audio\"]`.\n   * [Learn more](https://platform.openai.com/docs/guides/audio).\n   */\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  /**\n   * Static predicted output content, such as the content of a text file that is being regenerated.\n   * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n   */\n  prediction?: OpenAIClient.ChatCompletionPredictionContent;\n\n  /**\n   * Options for reasoning models.\n   *\n   * Note that some options, like reasoning summaries, are only available when using the responses\n   * API. If these options are set, the responses API will be used to fulfill the request.\n   *\n   * These options will be ignored when not using a reasoning model.\n   */\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates. Replaces the `user` field.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey?: string;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\nexport interface BaseChatOpenAIFields\n  extends Partial<OpenAIChatInput>,\n    BaseChatModelParams {\n  /**\n   * Optional configuration options for the OpenAI client.\n   */\n  configuration?: ClientOptions;\n}\n\n/** @internal */\nexport abstract class BaseChatOpenAI<\n    CallOptions extends BaseChatOpenAICallOptions\n  >\n  extends BaseChatModel<CallOptions, AIMessageChunk>\n  implements Partial<OpenAIChatInput>\n{\n  temperature?: number;\n\n  topP?: number;\n\n  frequencyPenalty?: number;\n\n  presencePenalty?: number;\n\n  n?: number;\n\n  logitBias?: Record<string, number>;\n\n  model = \"gpt-3.5-turbo\";\n\n  modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n\n  stop?: string[];\n\n  stopSequences?: string[];\n\n  user?: string;\n\n  timeout?: number;\n\n  streaming = false;\n\n  streamUsage = true;\n\n  maxTokens?: number;\n\n  logprobs?: boolean;\n\n  topLogprobs?: number;\n\n  apiKey?: string;\n\n  organization?: string;\n\n  __includeRawResponse?: boolean;\n\n  /** @internal */\n  client: OpenAIClient;\n\n  /** @internal */\n  clientConfig: ClientOptions;\n\n  /**\n   * Whether the model supports the `strict` argument when passing in tools.\n   * If `undefined` the `strict` argument will not be passed to OpenAI.\n   */\n  supportsStrictToolCalling?: boolean;\n\n  audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n\n  modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n\n  reasoning?: OpenAIClient.Reasoning;\n\n  /**\n   * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n   * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n   * OpenAI organization or project. This must be configured directly with OpenAI.\n   *\n   * See:\n   * https://platform.openai.com/docs/guides/your-data\n   * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n   *\n   * @default false\n   */\n  zdrEnabled?: boolean | undefined;\n\n  /**\n   * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n   * Specifies the service tier for prioritization and latency optimization.\n   */\n  service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n\n  /**\n   * Used by OpenAI to cache responses for similar requests to optimize your cache\n   * hit rates.\n   * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n   */\n  promptCacheKey: string;\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n\n  protected defaultOptions: CallOptions;\n\n  _llmType() {\n    return \"openai\";\n  }\n\n  static lc_name() {\n    return \"ChatOpenAI\";\n  }\n\n  get callKeys() {\n    return [\n      ...super.callKeys,\n      \"options\",\n      \"function_call\",\n      \"functions\",\n      \"tools\",\n      \"tool_choice\",\n      \"promptIndex\",\n      \"response_format\",\n      \"seed\",\n      \"reasoning\",\n      \"service_tier\",\n    ];\n  }\n\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"OPENAI_API_KEY\",\n      organization: \"OPENAI_ORGANIZATION\",\n    };\n  }\n\n  get lc_aliases(): Record<string, string> {\n    return {\n      apiKey: \"openai_api_key\",\n      modelName: \"model\",\n    };\n  }\n\n  get lc_serializable_keys(): string[] {\n    return [\n      \"configuration\",\n      \"logprobs\",\n      \"topLogprobs\",\n      \"prefixMessages\",\n      \"supportsStrictToolCalling\",\n      \"modalities\",\n      \"audio\",\n      \"temperature\",\n      \"maxTokens\",\n      \"topP\",\n      \"frequencyPenalty\",\n      \"presencePenalty\",\n      \"n\",\n      \"logitBias\",\n      \"user\",\n      \"streaming\",\n      \"streamUsage\",\n      \"model\",\n      \"modelName\",\n      \"modelKwargs\",\n      \"stop\",\n      \"stopSequences\",\n      \"timeout\",\n      \"apiKey\",\n      \"cache\",\n      \"maxConcurrency\",\n      \"maxRetries\",\n      \"verbose\",\n      \"callbacks\",\n      \"tags\",\n      \"metadata\",\n      \"disableStreaming\",\n      \"zdrEnabled\",\n      \"reasoning\",\n      \"promptCacheKey\",\n      \"verbosity\",\n    ];\n  }\n\n  getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams {\n    const params = this.invocationParams(options);\n    return {\n      ls_provider: \"openai\",\n      ls_model_name: this.model,\n      ls_model_type: \"chat\",\n      ls_temperature: params.temperature ?? undefined,\n      ls_max_tokens: params.max_tokens ?? undefined,\n      ls_stop: options.stop,\n    };\n  }\n\n  /** @ignore */\n  _identifyingParams(): Omit<\n    OpenAIClient.Chat.ChatCompletionCreateParams,\n    \"messages\"\n  > & {\n    model_name: string;\n  } & ClientOptions {\n    return {\n      model_name: this.model,\n      ...this.invocationParams(),\n      ...this.clientConfig,\n    };\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams() {\n    return this._identifyingParams();\n  }\n\n  constructor(fields?: BaseChatOpenAIFields) {\n    super(fields ?? {});\n\n    this.apiKey =\n      fields?.apiKey ??\n      fields?.configuration?.apiKey ??\n      getEnvironmentVariable(\"OPENAI_API_KEY\");\n    this.organization =\n      fields?.configuration?.organization ??\n      getEnvironmentVariable(\"OPENAI_ORGANIZATION\");\n\n    this.model = fields?.model ?? fields?.modelName ?? this.model;\n    this.modelKwargs = fields?.modelKwargs ?? {};\n    this.timeout = fields?.timeout;\n\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.topP = fields?.topP ?? this.topP;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.presencePenalty = fields?.presencePenalty ?? this.presencePenalty;\n    this.logprobs = fields?.logprobs;\n    this.topLogprobs = fields?.topLogprobs;\n    this.n = fields?.n ?? this.n;\n    this.logitBias = fields?.logitBias;\n    this.stop = fields?.stopSequences ?? fields?.stop;\n    this.stopSequences = this.stop;\n    this.user = fields?.user;\n    this.__includeRawResponse = fields?.__includeRawResponse;\n    this.audio = fields?.audio;\n    this.modalities = fields?.modalities;\n    this.reasoning = fields?.reasoning;\n    this.maxTokens = fields?.maxCompletionTokens ?? fields?.maxTokens;\n    this.promptCacheKey = fields?.promptCacheKey ?? this.promptCacheKey;\n    this.verbosity = fields?.verbosity ?? this.verbosity;\n\n    this.disableStreaming = fields?.disableStreaming === true;\n    this.streaming = fields?.streaming === true;\n    if (this.disableStreaming) this.streaming = false;\n    // disable streaming in BaseChatModel if explicitly disabled\n    if (fields?.streaming === false) this.disableStreaming = true;\n\n    this.streamUsage = fields?.streamUsage ?? this.streamUsage;\n    if (this.disableStreaming) this.streamUsage = false;\n\n    this.clientConfig = {\n      apiKey: this.apiKey,\n      organization: this.organization,\n      dangerouslyAllowBrowser: true,\n      ...fields?.configuration,\n    };\n\n    // If `supportsStrictToolCalling` is explicitly set, use that value.\n    // Else leave undefined so it's not passed to OpenAI.\n    if (fields?.supportsStrictToolCalling !== undefined) {\n      this.supportsStrictToolCalling = fields.supportsStrictToolCalling;\n    }\n\n    if (fields?.service_tier !== undefined) {\n      this.service_tier = fields.service_tier;\n    }\n\n    this.zdrEnabled = fields?.zdrEnabled ?? false;\n  }\n\n  /**\n   * Returns backwards compatible reasoning parameters from constructor params and call options\n   * @internal\n   */\n  protected _getReasoningParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): OpenAIClient.Reasoning | undefined {\n    if (!isReasoningModel(this.model)) {\n      return;\n    }\n\n    // apply options in reverse order of importance -- newer options supersede older options\n    let reasoning: OpenAIClient.Reasoning | undefined;\n    if (this.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...this.reasoning,\n      };\n    }\n    if (options?.reasoning !== undefined) {\n      reasoning = {\n        ...reasoning,\n        ...options.reasoning,\n      };\n    }\n\n    return reasoning;\n  }\n\n  /**\n   * Returns an openai compatible response format from a set of options\n   * @internal\n   */\n  protected _getResponseFormat(\n    resFormat?: CallOptions[\"response_format\"]\n  ): ResponseFormatConfiguration | undefined {\n    if (\n      resFormat &&\n      resFormat.type === \"json_schema\" &&\n      resFormat.json_schema.schema &&\n      isInteropZodSchema(resFormat.json_schema.schema)\n    ) {\n      return interopZodResponseFormat(\n        resFormat.json_schema.schema,\n        resFormat.json_schema.name,\n        {\n          description: resFormat.json_schema.description,\n        }\n      );\n    }\n    return resFormat as ResponseFormatConfiguration | undefined;\n  }\n\n  protected _combineCallOptions(\n    additionalOptions?: this[\"ParsedCallOptions\"]\n  ): this[\"ParsedCallOptions\"] {\n    return {\n      ...this.defaultOptions,\n      ...(additionalOptions ?? {}),\n    };\n  }\n\n  /** @internal */\n  _getClientOptions(\n    options: OpenAICoreRequestOptions | undefined\n  ): OpenAICoreRequestOptions {\n    if (!this.client) {\n      const openAIEndpointConfig: OpenAIEndpointConfig = {\n        baseURL: this.clientConfig.baseURL,\n      };\n\n      const endpoint = getEndpoint(openAIEndpointConfig);\n      const params = {\n        ...this.clientConfig,\n        baseURL: endpoint,\n        timeout: this.timeout,\n        maxRetries: 0,\n      };\n      if (!params.baseURL) {\n        delete params.baseURL;\n      }\n\n      this.client = new OpenAIClient(params);\n    }\n    const requestOptions = {\n      ...this.clientConfig,\n      ...options,\n    } as OpenAICoreRequestOptions;\n    return requestOptions;\n  }\n\n  // TODO: move to completions class\n  protected _convertChatOpenAIToolToCompletionsTool(\n    tool: ChatOpenAIToolType,\n    fields?: { strict?: boolean }\n  ): OpenAIClient.ChatCompletionTool {\n    if (isCustomTool(tool)) {\n      return convertResponsesCustomTool(tool.metadata.customTool);\n    }\n    if (isOpenAIFunctionTool(tool)) {\n      if (fields?.strict !== undefined) {\n        return {\n          ...tool,\n          function: {\n            ...tool.function,\n            strict: fields.strict,\n          },\n        };\n      }\n\n      return tool;\n    }\n    return _convertToOpenAITool(tool, fields);\n  }\n\n  override bindTools(\n    tools: ChatOpenAIToolType[],\n    kwargs?: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    let strict: boolean | undefined;\n    if (kwargs?.strict !== undefined) {\n      strict = kwargs.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n    return this.withConfig({\n      tools: tools.map((tool) =>\n        isBuiltInTool(tool)\n          ? tool\n          : this._convertChatOpenAIToolToCompletionsTool(tool, { strict })\n      ),\n      ...kwargs,\n    } as Partial<CallOptions>);\n  }\n\n  override async stream(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.stream(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  override async invoke(input: BaseLanguageModelInput, options?: CallOptions) {\n    return super.invoke(\n      input,\n      this._combineCallOptions(options) as CallOptions\n    );\n  }\n\n  /** @ignore */\n  _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput {\n    return llmOutputs.reduce<{\n      [key in keyof OpenAILLMOutput]: Required<OpenAILLMOutput[key]>;\n    }>(\n      (acc, llmOutput) => {\n        if (llmOutput && llmOutput.tokenUsage) {\n          acc.tokenUsage.completionTokens +=\n            llmOutput.tokenUsage.completionTokens ?? 0;\n          acc.tokenUsage.promptTokens += llmOutput.tokenUsage.promptTokens ?? 0;\n          acc.tokenUsage.totalTokens += llmOutput.tokenUsage.totalTokens ?? 0;\n        }\n        return acc;\n      },\n      {\n        tokenUsage: {\n          completionTokens: 0,\n          promptTokens: 0,\n          totalTokens: 0,\n        },\n      }\n    );\n  }\n\n  async getNumTokensFromMessages(messages: BaseMessage[]) {\n    let totalCount = 0;\n    let tokensPerMessage = 0;\n    let tokensPerName = 0;\n\n    // From: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb\n    if (this.model === \"gpt-3.5-turbo-0301\") {\n      tokensPerMessage = 4;\n      tokensPerName = -1;\n    } else {\n      tokensPerMessage = 3;\n      tokensPerName = 1;\n    }\n\n    const countPerMessage = await Promise.all(\n      messages.map(async (message) => {\n        const textCount = await this.getNumTokens(message.content);\n        const roleCount = await this.getNumTokens(messageToOpenAIRole(message));\n        const nameCount =\n          message.name !== undefined\n            ? tokensPerName + (await this.getNumTokens(message.name))\n            : 0;\n        let count = textCount + tokensPerMessage + roleCount + nameCount;\n\n        // From: https://github.com/hmarr/openai-chat-tokens/blob/main/src/index.ts messageTokenEstimate\n        const openAIMessage = message;\n        if (openAIMessage._getType() === \"function\") {\n          count -= 2;\n        }\n        if (openAIMessage.additional_kwargs?.function_call) {\n          count += 3;\n        }\n        if (openAIMessage?.additional_kwargs.function_call?.name) {\n          count += await this.getNumTokens(\n            openAIMessage.additional_kwargs.function_call?.name\n          );\n        }\n        if (openAIMessage.additional_kwargs.function_call?.arguments) {\n          try {\n            count += await this.getNumTokens(\n              // Remove newlines and spaces\n              JSON.stringify(\n                JSON.parse(\n                  openAIMessage.additional_kwargs.function_call?.arguments\n                )\n              )\n            );\n          } catch (error) {\n            console.error(\n              \"Error parsing function arguments\",\n              error,\n              JSON.stringify(openAIMessage.additional_kwargs.function_call)\n            );\n            count += await this.getNumTokens(\n              openAIMessage.additional_kwargs.function_call?.arguments\n            );\n          }\n        }\n\n        totalCount += count;\n        return count;\n      })\n    );\n\n    totalCount += 3; // every reply is primed with <|start|>assistant<|message|>\n\n    return { totalCount, countPerMessage };\n  }\n\n  /** @internal */\n  protected async _getNumTokensFromGenerations(generations: ChatGeneration[]) {\n    const generationUsages = await Promise.all(\n      generations.map(async (generation) => {\n        if (generation.message.additional_kwargs?.function_call) {\n          return (await this.getNumTokensFromMessages([generation.message]))\n            .countPerMessage[0];\n        } else {\n          return await this.getNumTokens(generation.message.content);\n        }\n      })\n    );\n\n    return generationUsages.reduce((a, b) => a + b, 0);\n  }\n\n  /** @internal */\n  protected async _getEstimatedTokenCountFromPrompt(\n    messages: BaseMessage[],\n    functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[],\n    function_call?:\n      | \"none\"\n      | \"auto\"\n      | OpenAIClient.Chat.ChatCompletionFunctionCallOption\n  ): Promise<number> {\n    // It appears that if functions are present, the first system message is padded with a trailing newline. This\n    // was inferred by trying lots of combinations of messages and functions and seeing what the token counts were.\n\n    let tokens = (await this.getNumTokensFromMessages(messages)).totalCount;\n\n    // If there are functions, add the function definitions as they count towards token usage\n    if (functions && function_call !== \"auto\") {\n      const promptDefinitions = formatFunctionDefinitions(\n        functions as unknown as FunctionDef[]\n      );\n      tokens += await this.getNumTokens(promptDefinitions);\n      tokens += 9; // Add nine per completion\n    }\n\n    // If there's a system message _and_ functions are present, subtract four tokens. I assume this is because\n    // functions typically add a system message, but reuse the first one if it's already there. This offsets\n    // the extra 9 tokens added by the function definitions.\n    if (functions && messages.find((m) => m._getType() === \"system\")) {\n      tokens -= 4;\n    }\n\n    // If function_call is 'none', add one token.\n    // If it's a FunctionCall object, add 4 + the number of tokens in the function name.\n    // If it's undefined or 'auto', don't add anything.\n    if (function_call === \"none\") {\n      tokens += 1;\n    } else if (typeof function_call === \"object\") {\n      tokens += (await this.getNumTokens(function_call.name)) + 4;\n    }\n\n    return tokens;\n  }\n\n  /** @internal */\n  protected _getStructuredOutputMethod(\n    config: StructuredOutputMethodOptions<boolean>\n  ) {\n    const ensuredConfig = { ...config };\n    if (\n      !this.model.startsWith(\"gpt-3\") &&\n      !this.model.startsWith(\"gpt-4-\") &&\n      this.model !== \"gpt-4\"\n    ) {\n      if (ensuredConfig?.method === undefined) {\n        return \"jsonSchema\";\n      }\n    } else if (ensuredConfig.method === \"jsonSchema\") {\n      console.warn(\n        `[WARNING]: JSON Schema is not supported for model \"${this.model}\". Falling back to tool calling.`\n      );\n    }\n    return ensuredConfig.method;\n  }\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<false>\n  ): Runnable<BaseLanguageModelInput, RunOutput>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<true>\n  ): Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>\n  >(\n    outputSchema:\n      | InteropZodType<RunOutput>\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      | Record<string, any>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ):\n    | Runnable<BaseLanguageModelInput, RunOutput>\n    | Runnable<BaseLanguageModelInput, { raw: BaseMessage; parsed: RunOutput }>;\n\n  /**\n   * Add structured output to the model.\n   *\n   * The OpenAI model family supports the following structured output methods:\n   * - `jsonSchema`: Use the `response_format` field in the response to return a JSON schema. Only supported with the `gpt-4o-mini`,\n   *   `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots and later.\n   * - `functionCalling`: Function calling is useful when you are building an application that bridges the models and functionality\n   *   of your application.\n   * - `jsonMode`: JSON mode is a more basic version of the Structured Outputs feature. While JSON mode ensures that model\n   *   output is valid JSON, Structured Outputs reliably matches the model's output to the schema you specify.\n   *   We recommend you use `functionCalling` or `jsonSchema` if it is supported for your use case.\n   *\n   * The default method is `functionCalling`.\n   *\n   * @see https://platform.openai.com/docs/guides/structured-outputs\n   * @param outputSchema - The schema to use for structured output.\n   * @param config - The structured output method options.\n   * @returns The model with structured output.\n   */\n  withStructuredOutput<\n    RunOutput extends Record<string, unknown> = Record<string, unknown>\n  >(\n    outputSchema: InteropZodType<RunOutput> | Record<string, unknown>,\n    config?: StructuredOutputMethodOptions<boolean>\n  ) {\n    let llm: Runnable<BaseLanguageModelInput>;\n    let outputParser: Runnable<AIMessageChunk, RunOutput>;\n\n    const { schema, name, includeRaw } = {\n      ...config,\n      schema: outputSchema,\n    };\n\n    if (config?.strict !== undefined && config.method === \"jsonMode\") {\n      throw new Error(\n        \"Argument `strict` is only supported for `method` = 'function_calling'\"\n      );\n    }\n\n    const method = getStructuredOutputMethod(this.model, config?.method);\n\n    if (method === \"jsonMode\") {\n      if (isInteropZodSchema(schema)) {\n        outputParser = StructuredOutputParser.fromZodSchema(schema);\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n      const asJsonSchema = toJsonSchema(schema);\n      llm = this.withConfig({\n        response_format: { type: \"json_object\" },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_mode\" },\n          schema: { title: name ?? \"extract\", ...asJsonSchema },\n        },\n      } as Partial<CallOptions>);\n    } else if (method === \"jsonSchema\") {\n      const openaiJsonSchemaParams = {\n        name: name ?? \"extract\",\n        description: getSchemaDescription(schema),\n        schema,\n        strict: config?.strict,\n      };\n      const asJsonSchema = toJsonSchema(openaiJsonSchemaParams.schema);\n      llm = this.withConfig({\n        response_format: {\n          type: \"json_schema\",\n          json_schema: openaiJsonSchemaParams,\n        },\n        ls_structured_output_format: {\n          kwargs: { method: \"json_schema\" },\n          schema: {\n            title: openaiJsonSchemaParams.name,\n            description: openaiJsonSchemaParams.description,\n            ...asJsonSchema,\n          },\n        },\n      } as Partial<CallOptions>);\n      if (isInteropZodSchema(schema)) {\n        const altParser = StructuredOutputParser.fromZodSchema(schema);\n        outputParser = RunnableLambda.from<AIMessageChunk, RunOutput>(\n          (aiMessage: AIMessageChunk) => {\n            if (\"parsed\" in aiMessage.additional_kwargs) {\n              return aiMessage.additional_kwargs.parsed as RunOutput;\n            }\n            return altParser;\n          }\n        );\n      } else {\n        outputParser = new JsonOutputParser<RunOutput>();\n      }\n    } else {\n      let functionName = name ?? \"extract\";\n      // Is function calling\n      if (isInteropZodSchema(schema)) {\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          tools: [\n            {\n              type: \"function\" as const,\n              function: {\n                name: functionName,\n                description: asJsonSchema.description,\n                parameters: asJsonSchema,\n              },\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser({\n          returnSingle: true,\n          keyName: functionName,\n          zodSchema: schema,\n        });\n      } else {\n        let openAIFunctionDefinition: FunctionDefinition;\n        if (\n          typeof schema.name === \"string\" &&\n          typeof schema.parameters === \"object\" &&\n          schema.parameters != null\n        ) {\n          openAIFunctionDefinition = schema as unknown as FunctionDefinition;\n          functionName = schema.name;\n        } else {\n          functionName = (schema.title as string) ?? functionName;\n          openAIFunctionDefinition = {\n            name: functionName,\n            description: (schema.description as string) ?? \"\",\n            parameters: schema,\n          };\n        }\n        const asJsonSchema = toJsonSchema(schema);\n        llm = this.withConfig({\n          tools: [\n            {\n              type: \"function\" as const,\n              function: openAIFunctionDefinition,\n            },\n          ],\n          tool_choice: {\n            type: \"function\" as const,\n            function: {\n              name: functionName,\n            },\n          },\n          ls_structured_output_format: {\n            kwargs: { method: \"function_calling\" },\n            schema: { title: functionName, ...asJsonSchema },\n          },\n          // Do not pass `strict` argument to OpenAI if `config.strict` is undefined\n          ...(config?.strict !== undefined ? { strict: config.strict } : {}),\n        } as Partial<CallOptions>);\n        outputParser = new JsonOutputKeyToolsParser<RunOutput>({\n          returnSingle: true,\n          keyName: functionName,\n        });\n      }\n    }\n\n    if (!includeRaw) {\n      return llm.pipe(outputParser) as Runnable<\n        BaseLanguageModelInput,\n        RunOutput\n      >;\n    }\n\n    const parserAssign = RunnablePassthrough.assign({\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      parsed: (input: any, config) => outputParser.invoke(input.raw, config),\n    });\n    const parserNone = RunnablePassthrough.assign({\n      parsed: () => null,\n    });\n    const parsedWithFallback = parserAssign.withFallbacks({\n      fallbacks: [parserNone],\n    });\n    return RunnableSequence.from<\n      BaseLanguageModelInput,\n      { raw: BaseMessage; parsed: RunOutput }\n    >([{ raw: llm }, parsedWithFallback]);\n  }\n}\n\nexport interface ChatOpenAIResponsesCallOptions\n  extends BaseChatOpenAICallOptions {\n  /**\n   * Configuration options for a text response from the model. Can be plain text or\n   * structured JSON data.\n   */\n  text?: OpenAIClient.Responses.ResponseCreateParams[\"text\"];\n\n  /**\n   * The truncation strategy to use for the model response.\n   */\n  truncation?: OpenAIClient.Responses.ResponseCreateParams[\"truncation\"];\n\n  /**\n   * Specify additional output data to include in the model response.\n   */\n  include?: OpenAIClient.Responses.ResponseCreateParams[\"include\"];\n\n  /**\n   * The unique ID of the previous response to the model. Use this to create multi-turn\n   * conversations.\n   */\n  previous_response_id?: OpenAIClient.Responses.ResponseCreateParams[\"previous_response_id\"];\n\n  /**\n   * The verbosity of the model's response.\n   */\n  verbosity?: OpenAIVerbosityParam;\n}\n\ntype ChatResponsesInvocationParams = Omit<\n  OpenAIClient.Responses.ResponseCreateParams,\n  \"input\"\n>;\n\ntype ExcludeController<T> = T extends { controller: unknown } ? never : T;\n\ntype ResponsesCreate = OpenAIClient.Responses[\"create\"];\ntype ResponsesParse = OpenAIClient.Responses[\"parse\"];\n\ntype ResponsesCreateInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesCreate>>\n>;\ntype ResponsesParseInvoke = ExcludeController<\n  Awaited<ReturnType<ResponsesParse>>\n>;\n\n/**\n * OpenAI Responses API implementation.\n *\n * Will be exported in a later version of @langchain/openai.\n *\n * @internal\n */\nexport class ChatOpenAIResponses<\n  CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"]\n  ): ChatResponsesInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    const params: ChatResponsesInvocationParams = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      user: this.user,\n\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      previous_response_id: options?.previous_response_id,\n      truncation: options?.truncation,\n      include: options?.include,\n      tools: options?.tools?.length\n        ? this._reduceChatOpenAITools(options.tools, {\n            stream: this.streaming,\n            strict,\n          })\n        : undefined,\n      tool_choice: isBuiltInToolChoice(options?.tool_choice)\n        ? options?.tool_choice\n        : (() => {\n            const formatted = formatToOpenAIToolChoice(options?.tool_choice);\n            if (typeof formatted === \"object\" && \"type\" in formatted) {\n              if (formatted.type === \"function\") {\n                return { type: \"function\", name: formatted.function.name };\n              } else if (formatted.type === \"allowed_tools\") {\n                return {\n                  type: \"allowed_tools\",\n                  mode: formatted.allowed_tools.mode,\n                  tools: formatted.allowed_tools.tools,\n                };\n              } else if (formatted.type === \"custom\") {\n                return {\n                  type: \"custom\",\n                  name: formatted.custom.name,\n                };\n              }\n            }\n            return undefined;\n          })(),\n      text: (() => {\n        if (options?.text) return options.text;\n        const format = this._getResponseFormat(options?.response_format);\n        if (format?.type === \"json_schema\") {\n          if (format.json_schema.schema != null) {\n            return {\n              format: {\n                type: \"json_schema\",\n                schema: format.json_schema.schema,\n                description: format.json_schema.description,\n                name: format.json_schema.name,\n                strict: format.json_schema.strict,\n              },\n              verbosity: options?.verbosity,\n            };\n          }\n          return undefined;\n        }\n        return { format, verbosity: options?.verbosity };\n      })(),\n      parallel_tool_calls: options?.parallel_tool_calls,\n      max_output_tokens: this.maxTokens === -1 ? undefined : this.maxTokens,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      ...(this.zdrEnabled ? { store: false } : {}),\n      ...this.modelKwargs,\n    };\n\n    const reasoning = this._getReasoningParams(options);\n\n    if (reasoning !== undefined) {\n      params.reasoning = reasoning;\n    }\n\n    return params;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"]\n  ): Promise<ChatResult> {\n    const invocationParams = this.invocationParams(options);\n    if (invocationParams.stream) {\n      const stream = this._streamResponseChunks(messages, options);\n      let finalChunk: ChatGenerationChunk | undefined;\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        finalChunk = finalChunk?.concat(chunk) ?? chunk;\n      }\n\n      return {\n        generations: finalChunk ? [finalChunk] : [],\n        llmOutput: {\n          estimatedTokenUsage: (finalChunk?.message as AIMessage | undefined)\n            ?.usage_metadata,\n        },\n      };\n    } else {\n      const input = this._convertMessagesToResponsesParams(messages);\n      const data = await this.completionWithRetry(\n        {\n          input,\n          ...invocationParams,\n          stream: false,\n        },\n        { signal: options?.signal, ...options?.options }\n      );\n\n      return {\n        generations: [\n          {\n            text: data.output_text,\n            message: this._convertResponsesMessageToBaseMessage(data),\n          },\n        ],\n        llmOutput: {\n          id: data.id,\n          estimatedTokenUsage: data.usage\n            ? {\n                promptTokens: data.usage.input_tokens,\n                completionTokens: data.usage.output_tokens,\n                totalTokens: data.usage.total_tokens,\n              }\n            : undefined,\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const streamIterable = await this.completionWithRetry(\n      {\n        ...this.invocationParams(options),\n        input: this._convertMessagesToResponsesParams(messages),\n        stream: true,\n      },\n      options\n    );\n\n    for await (const data of streamIterable) {\n      const chunk = this._convertResponsesDeltaToBaseMessageChunk(data);\n      if (chunk == null) continue;\n      yield chunk;\n      await runManager?.handleLLMNewToken(\n        chunk.text || \"\",\n        {\n          prompt: options.promptIndex ?? 0,\n          completion: 0,\n        },\n        undefined,\n        undefined,\n        undefined,\n        { chunk }\n      );\n    }\n  }\n\n  /**\n   * Calls the Responses API with retry logic in case of failures.\n   * @param request The request to send to the OpenAI API.\n   * @param options Optional configuration for the API call.\n   * @returns The response from the OpenAI API.\n   */\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Responses.Response>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Responses.ResponseCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>\n    | OpenAIClient.Responses.Response\n  > {\n    return this.caller.call(async () => {\n      const clientOptions = this._getClientOptions(requestOptions);\n      try {\n        // use parse if dealing with json_schema\n        if (request.text?.format?.type === \"json_schema\" && !request.stream) {\n          return await this.client.responses.parse(request, clientOptions);\n        }\n        return await this.client.responses.create(request, clientOptions);\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /** @internal */\n  protected _convertResponsesMessageToBaseMessage(\n    response: ResponsesCreateInvoke | ResponsesParseInvoke\n  ): BaseMessage {\n    if (response.error) {\n      // TODO: add support for `addLangChainErrorFields`\n      const error = new Error(response.error.message);\n      error.name = response.error.code;\n      throw error;\n    }\n\n    let messageId: string | undefined;\n    const content: MessageContent = [];\n    const tool_calls: ToolCall[] = [];\n    const invalid_tool_calls: InvalidToolCall[] = [];\n    const response_metadata: Record<string, unknown> = {\n      model_provider: \"openai\",\n      model: response.model,\n      created_at: response.created_at,\n      id: response.id,\n      incomplete_details: response.incomplete_details,\n      metadata: response.metadata,\n      object: response.object,\n      status: response.status,\n      user: response.user,\n      service_tier: response.service_tier,\n      // for compatibility with chat completion calls.\n      model_name: response.model,\n    };\n\n    const additional_kwargs: {\n      [key: string]: unknown;\n      refusal?: string;\n      reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n      tool_outputs?: unknown[];\n      parsed?: unknown;\n      [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n    } = {};\n\n    for (const item of response.output) {\n      if (item.type === \"message\") {\n        messageId = item.id;\n        content.push(\n          ...item.content.flatMap((part) => {\n            if (part.type === \"output_text\") {\n              if (\"parsed\" in part && part.parsed != null) {\n                additional_kwargs.parsed = part.parsed;\n              }\n              return {\n                type: \"text\",\n                text: part.text,\n                annotations: part.annotations,\n              };\n            }\n\n            if (part.type === \"refusal\") {\n              additional_kwargs.refusal = part.refusal;\n              return [];\n            }\n\n            return part;\n          })\n        );\n      } else if (item.type === \"function_call\") {\n        const fnAdapter = {\n          function: { name: item.name, arguments: item.arguments },\n          id: item.call_id,\n        };\n\n        try {\n          tool_calls.push(parseToolCall(fnAdapter, { returnId: true }));\n        } catch (e: unknown) {\n          let errMessage: string | undefined;\n          if (\n            typeof e === \"object\" &&\n            e != null &&\n            \"message\" in e &&\n            typeof e.message === \"string\"\n          ) {\n            errMessage = e.message;\n          }\n          invalid_tool_calls.push(makeInvalidToolCall(fnAdapter, errMessage));\n        }\n\n        additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] ??= {};\n        if (item.id) {\n          additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY][item.call_id] = item.id;\n        }\n      } else if (item.type === \"reasoning\") {\n        additional_kwargs.reasoning = item;\n      } else if (item.type === \"custom_tool_call\") {\n        const parsed = parseCustomToolCall(item);\n        if (parsed) {\n          tool_calls.push(parsed);\n        } else {\n          invalid_tool_calls.push(\n            makeInvalidToolCall(item, \"Malformed custom tool call\")\n          );\n        }\n      } else {\n        additional_kwargs.tool_outputs ??= [];\n        additional_kwargs.tool_outputs.push(item);\n      }\n    }\n\n    return new AIMessage({\n      id: messageId,\n      content,\n      tool_calls,\n      invalid_tool_calls,\n      usage_metadata: _convertOpenAIResponsesUsageToLangChainUsage(\n        response.usage\n      ),\n      additional_kwargs,\n      response_metadata,\n    });\n  }\n\n  /** @internal */\n  protected _convertResponsesDeltaToBaseMessageChunk(\n    chunk: OpenAIClient.Responses.ResponseStreamEvent\n  ) {\n    const content: Record<string, unknown>[] = [];\n    let generationInfo: Record<string, unknown> = {};\n    let usage_metadata: UsageMetadata | undefined;\n    const tool_call_chunks: ToolCallChunk[] = [];\n    const response_metadata: Record<string, unknown> = {\n      model_provider: \"openai\",\n    };\n    const additional_kwargs: {\n      [key: string]: unknown;\n      reasoning?: Partial<ChatOpenAIReasoningSummary>;\n      tool_outputs?: unknown[];\n    } = {};\n    let id: string | undefined;\n    if (chunk.type === \"response.output_text.delta\") {\n      content.push({\n        type: \"text\",\n        text: chunk.delta,\n        index: chunk.content_index,\n      });\n    } else if (chunk.type === \"response.output_text.annotation.added\") {\n      content.push({\n        type: \"text\",\n        text: \"\",\n        annotations: [chunk.annotation],\n        index: chunk.content_index,\n      });\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      chunk.item.type === \"message\"\n    ) {\n      id = chunk.item.id;\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      chunk.item.type === \"function_call\"\n    ) {\n      tool_call_chunks.push({\n        type: \"tool_call_chunk\",\n        name: chunk.item.name,\n        args: chunk.item.arguments,\n        id: chunk.item.call_id,\n        index: chunk.output_index,\n      });\n\n      additional_kwargs[_FUNCTION_CALL_IDS_MAP_KEY] = {\n        [chunk.item.call_id]: chunk.item.id,\n      };\n    } else if (\n      chunk.type === \"response.output_item.done\" &&\n      [\n        \"web_search_call\",\n        \"file_search_call\",\n        \"computer_call\",\n        \"code_interpreter_call\",\n        \"mcp_call\",\n        \"mcp_list_tools\",\n        \"mcp_approval_request\",\n        \"image_generation_call\",\n        \"custom_tool_call\",\n      ].includes(chunk.item.type)\n    ) {\n      additional_kwargs.tool_outputs = [chunk.item];\n    } else if (chunk.type === \"response.created\") {\n      response_metadata.id = chunk.response.id;\n      response_metadata.model_name = chunk.response.model;\n      response_metadata.model = chunk.response.model;\n    } else if (chunk.type === \"response.completed\") {\n      const msg = this._convertResponsesMessageToBaseMessage(chunk.response);\n\n      usage_metadata = _convertOpenAIResponsesUsageToLangChainUsage(\n        chunk.response.usage\n      );\n\n      if (chunk.response.text?.format?.type === \"json_schema\") {\n        additional_kwargs.parsed ??= JSON.parse(msg.text);\n      }\n      for (const [key, value] of Object.entries(chunk.response)) {\n        if (key !== \"id\") response_metadata[key] = value;\n      }\n    } else if (\n      chunk.type === \"response.function_call_arguments.delta\" ||\n      chunk.type === \"response.custom_tool_call_input.delta\"\n    ) {\n      tool_call_chunks.push({\n        type: \"tool_call_chunk\",\n        args: chunk.delta,\n        index: chunk.output_index,\n      });\n    } else if (\n      chunk.type === \"response.web_search_call.completed\" ||\n      chunk.type === \"response.file_search_call.completed\"\n    ) {\n      generationInfo = {\n        tool_outputs: {\n          id: chunk.item_id,\n          type: chunk.type.replace(\"response.\", \"\").replace(\".completed\", \"\"),\n          status: \"completed\",\n        },\n      };\n    } else if (chunk.type === \"response.refusal.done\") {\n      additional_kwargs.refusal = chunk.refusal;\n    } else if (\n      chunk.type === \"response.output_item.added\" &&\n      \"item\" in chunk &&\n      chunk.item.type === \"reasoning\"\n    ) {\n      const summary: ChatOpenAIReasoningSummary[\"summary\"] | undefined = chunk\n        .item.summary\n        ? chunk.item.summary.map((s, index) => ({\n            ...s,\n            index,\n          }))\n        : undefined;\n\n      additional_kwargs.reasoning = {\n        // We only capture ID in the first chunk or else the concatenated result of all chunks will\n        // have an ID field that is repeated once per chunk. There is special handling for the `type`\n        // field that prevents this, however.\n        id: chunk.item.id,\n        type: chunk.item.type,\n        ...(summary ? { summary } : {}),\n      };\n    } else if (chunk.type === \"response.reasoning_summary_part.added\") {\n      additional_kwargs.reasoning = {\n        type: \"reasoning\",\n        summary: [{ ...chunk.part, index: chunk.summary_index }],\n      };\n    } else if (chunk.type === \"response.reasoning_summary_text.delta\") {\n      additional_kwargs.reasoning = {\n        type: \"reasoning\",\n        summary: [\n          {\n            text: chunk.delta,\n            type: \"summary_text\",\n            index: chunk.summary_index,\n          },\n        ],\n      };\n    } else if (chunk.type === \"response.image_generation_call.partial_image\") {\n      // noop/fixme: retaining partial images in a message chunk means that _all_\n      // partial images get kept in history, so we don't do anything here.\n      return null;\n    } else {\n      return null;\n    }\n\n    return new ChatGenerationChunk({\n      // Legacy reasons, `onLLMNewToken` should pulls this out\n      text: content.map((part) => part.text).join(\"\"),\n      message: new AIMessageChunk({\n        id,\n        content: content as MessageContent,\n        tool_call_chunks,\n        usage_metadata,\n        additional_kwargs,\n        response_metadata,\n      }),\n      generationInfo,\n    });\n  }\n\n  /** @internal */\n  protected _convertMessagesToResponsesParams(messages: BaseMessage[]) {\n    return messages.flatMap(\n      (lcMsg): ResponsesInputItem | ResponsesInputItem[] => {\n        const responseMetadata = lcMsg.response_metadata as\n          | Record<string, unknown>\n          | undefined;\n        if (responseMetadata?.output_version === \"v1\") {\n          return _convertToResponsesMessageFromV1(lcMsg);\n        }\n\n        const additional_kwargs = lcMsg.additional_kwargs as\n          | BaseMessageFields[\"additional_kwargs\"] & {\n              [_FUNCTION_CALL_IDS_MAP_KEY]?: Record<string, string>;\n              reasoning?: OpenAIClient.Responses.ResponseReasoningItem;\n              type?: string;\n              refusal?: string;\n            };\n\n        let role = messageToOpenAIRole(lcMsg);\n        if (role === \"system\" && isReasoningModel(this.model))\n          role = \"developer\";\n\n        if (role === \"function\") {\n          throw new Error(\n            \"Function messages are not supported in Responses API\"\n          );\n        }\n\n        if (role === \"tool\") {\n          const toolMessage = lcMsg as ToolMessage;\n\n          // Handle computer call output\n          if (additional_kwargs?.type === \"computer_call_output\") {\n            const output = (() => {\n              if (typeof toolMessage.content === \"string\") {\n                return {\n                  type: \"computer_screenshot\" as const,\n                  image_url: toolMessage.content,\n                };\n              }\n\n              if (Array.isArray(toolMessage.content)) {\n                const oaiScreenshot = toolMessage.content.find(\n                  (i) => i.type === \"computer_screenshot\"\n                ) as { type: \"computer_screenshot\"; image_url: string };\n\n                if (oaiScreenshot) return oaiScreenshot;\n\n                const lcImage = toolMessage.content.find(\n                  (i) => i.type === \"image_url\"\n                ) as MessageContentImageUrl;\n\n                if (lcImage) {\n                  return {\n                    type: \"computer_screenshot\" as const,\n                    image_url:\n                      typeof lcImage.image_url === \"string\"\n                        ? lcImage.image_url\n                        : lcImage.image_url.url,\n                  };\n                }\n              }\n\n              throw new Error(\"Invalid computer call output\");\n            })();\n\n            return {\n              type: \"computer_call_output\",\n              output,\n              call_id: toolMessage.tool_call_id,\n            };\n          }\n\n          // Handle custom tool output\n          if (toolMessage.additional_kwargs?.customTool) {\n            return {\n              type: \"custom_tool_call_output\",\n              call_id: toolMessage.tool_call_id,\n              output: toolMessage.content as string,\n            };\n          }\n\n          return {\n            type: \"function_call_output\",\n            call_id: toolMessage.tool_call_id,\n            id: toolMessage.id?.startsWith(\"fc_\") ? toolMessage.id : undefined,\n            output:\n              typeof toolMessage.content !== \"string\"\n                ? JSON.stringify(toolMessage.content)\n                : toolMessage.content,\n          };\n        }\n\n        if (role === \"assistant\") {\n          // if we have the original response items, just reuse them\n          if (\n            !this.zdrEnabled &&\n            responseMetadata?.output != null &&\n            Array.isArray(responseMetadata?.output) &&\n            responseMetadata?.output.length > 0 &&\n            responseMetadata?.output.every((item) => \"type\" in item)\n          ) {\n            return responseMetadata?.output;\n          }\n\n          // otherwise, try to reconstruct the response from what we have\n\n          const input: ResponsesInputItem[] = [];\n\n          // reasoning items\n          if (additional_kwargs?.reasoning && !this.zdrEnabled) {\n            const reasoningItem = this._convertReasoningSummary(\n              additional_kwargs.reasoning\n            );\n            input.push(reasoningItem);\n          }\n\n          // ai content\n          let { content } = lcMsg;\n          if (additional_kwargs?.refusal) {\n            if (typeof content === \"string\") {\n              content = [\n                { type: \"output_text\", text: content, annotations: [] },\n              ];\n            }\n            content = [\n              ...content,\n              { type: \"refusal\", refusal: additional_kwargs.refusal },\n            ];\n          }\n\n          if (typeof content === \"string\" || content.length > 0) {\n            input.push({\n              type: \"message\",\n              role: \"assistant\",\n              ...(lcMsg.id && !this.zdrEnabled && lcMsg.id.startsWith(\"msg_\")\n                ? { id: lcMsg.id }\n                : {}),\n              content: iife(() => {\n                if (typeof content === \"string\") {\n                  return content;\n                }\n                return content.flatMap((item) => {\n                  if (item.type === \"text\") {\n                    return {\n                      type: \"output_text\",\n                      text: item.text,\n                      annotations: item.annotations ?? [],\n                    };\n                  }\n\n                  if (item.type === \"output_text\" || item.type === \"refusal\") {\n                    return item;\n                  }\n\n                  return [];\n                });\n              }) as ResponseInputMessageContentList,\n            });\n          }\n\n          const functionCallIds =\n            additional_kwargs?.[_FUNCTION_CALL_IDS_MAP_KEY];\n\n          if (isAIMessage(lcMsg) && !!lcMsg.tool_calls?.length) {\n            input.push(\n              ...lcMsg.tool_calls.map((toolCall): ResponsesInputItem => {\n                if (isCustomToolCall(toolCall)) {\n                  return {\n                    type: \"custom_tool_call\",\n                    id: toolCall.call_id,\n                    call_id: toolCall.id ?? \"\",\n                    input: toolCall.args.input,\n                    name: toolCall.name,\n                  };\n                }\n                return {\n                  type: \"function_call\",\n                  name: toolCall.name,\n                  arguments: JSON.stringify(toolCall.args),\n                  call_id: toolCall.id!,\n                  ...(this.zdrEnabled\n                    ? { id: functionCallIds?.[toolCall.id!] }\n                    : {}),\n                };\n              })\n            );\n          } else if (additional_kwargs?.tool_calls) {\n            input.push(\n              ...additional_kwargs.tool_calls.map(\n                (toolCall): ResponsesInputItem => ({\n                  type: \"function_call\",\n                  name: toolCall.function.name,\n                  call_id: toolCall.id,\n                  arguments: toolCall.function.arguments,\n                  ...(this.zdrEnabled\n                    ? { id: functionCallIds?.[toolCall.id] }\n                    : {}),\n                })\n              )\n            );\n          }\n\n          const toolOutputs = (\n            responseMetadata?.output as Array<ResponsesInputItem>\n          )?.length\n            ? responseMetadata?.output\n            : additional_kwargs.tool_outputs;\n\n          const fallthroughCallTypes: ResponsesInputItem[\"type\"][] = [\n            \"computer_call\",\n            \"mcp_call\",\n            \"code_interpreter_call\",\n            \"image_generation_call\",\n          ];\n\n          if (toolOutputs != null) {\n            const castToolOutputs = toolOutputs as Array<ResponsesInputItem>;\n            const fallthroughCalls = castToolOutputs?.filter((item) =>\n              fallthroughCallTypes.includes(item.type)\n            );\n            if (fallthroughCalls.length > 0) input.push(...fallthroughCalls);\n          }\n\n          return input;\n        }\n\n        if (role === \"user\" || role === \"system\" || role === \"developer\") {\n          if (typeof lcMsg.content === \"string\") {\n            return { type: \"message\", role, content: lcMsg.content };\n          }\n\n          const messages: ResponsesInputItem[] = [];\n          const content = lcMsg.content.flatMap((item) => {\n            if (item.type === \"mcp_approval_response\") {\n              messages.push({\n                type: \"mcp_approval_response\",\n                approval_request_id: item.approval_request_id as string,\n                approve: item.approve as boolean,\n              });\n            }\n            if (isDataContentBlock(item)) {\n              return convertToProviderContentBlock(\n                item,\n                completionsApiContentBlockConverter\n              );\n            }\n            if (item.type === \"text\") {\n              return {\n                type: \"input_text\",\n                text: item.text,\n              };\n            }\n            if (item.type === \"image_url\") {\n              const imageUrl = iife(() => {\n                if (typeof item.image_url === \"string\") {\n                  return item.image_url;\n                } else if (\n                  typeof item.image_url === \"object\" &&\n                  item.image_url !== null &&\n                  \"url\" in item.image_url\n                ) {\n                  return item.image_url.url;\n                }\n                return undefined;\n              });\n              const detail = iife(() => {\n                if (typeof item.image_url === \"string\") {\n                  return \"auto\";\n                } else if (\n                  typeof item.image_url === \"object\" &&\n                  item.image_url !== null &&\n                  \"detail\" in item.image_url\n                ) {\n                  return item.image_url.detail;\n                }\n                return undefined;\n              });\n              return {\n                type: \"input_image\",\n                image_url: imageUrl,\n                detail,\n              };\n            }\n            if (\n              item.type === \"input_text\" ||\n              item.type === \"input_image\" ||\n              item.type === \"input_file\"\n            ) {\n              return item;\n            }\n            return [];\n          });\n\n          if (content.length > 0) {\n            messages.push({\n              type: \"message\",\n              role,\n              content: content as ResponseInputMessageContentList,\n            });\n          }\n          return messages;\n        }\n\n        console.warn(\n          `Unsupported role found when converting to OpenAI Responses API: ${role}`\n        );\n        return [];\n      }\n    );\n  }\n\n  /** @internal */\n  protected _convertReasoningSummary(\n    reasoning: ChatOpenAIReasoningSummary\n  ): OpenAIClient.Responses.ResponseReasoningItem {\n    // combine summary parts that have the the same index and then remove the indexes\n    const summary = (\n      reasoning.summary.length > 1\n        ? reasoning.summary.reduce(\n            (acc, curr) => {\n              const last = acc.at(-1);\n\n              if (last!.index === curr.index) {\n                last!.text += curr.text;\n              } else {\n                acc.push(curr);\n              }\n              return acc;\n            },\n            [{ ...reasoning.summary[0] }]\n          )\n        : reasoning.summary\n    ).map((s) =>\n      Object.fromEntries(Object.entries(s).filter(([k]) => k !== \"index\"))\n    ) as OpenAIClient.Responses.ResponseReasoningItem.Summary[];\n\n    return {\n      ...reasoning,\n      summary,\n    } as OpenAIClient.Responses.ResponseReasoningItem;\n  }\n\n  /** @internal */\n  protected _reduceChatOpenAITools(\n    tools: ChatOpenAIToolType[],\n    fields: { stream?: boolean; strict?: boolean }\n  ): ResponsesTool[] {\n    const reducedTools: ResponsesTool[] = [];\n    for (const tool of tools) {\n      if (isBuiltInTool(tool)) {\n        if (tool.type === \"image_generation\" && fields?.stream) {\n          // OpenAI sends a 400 error if partial_images is not set and we want to stream.\n          // We also set it to 1 since we don't support partial images yet.\n          tool.partial_images = 1;\n        }\n        reducedTools.push(tool);\n      } else if (isOpenAIFunctionTool(tool)) {\n        reducedTools.push({\n          type: \"function\",\n          name: tool.function.name,\n          parameters: tool.function.parameters,\n          description: tool.function.description,\n          strict: fields?.strict ?? null,\n        });\n      } else if (isOpenAICustomTool(tool)) {\n        reducedTools.push(convertCompletionsCustomTool(tool));\n      }\n    }\n    return reducedTools;\n  }\n}\n\nexport interface ChatOpenAICompletionsCallOptions\n  extends BaseChatOpenAICallOptions {}\n\ntype ChatCompletionsInvocationParams = Omit<\n  OpenAIClient.Chat.Completions.ChatCompletionCreateParams,\n  \"messages\"\n>;\n\n/**\n * OpenAI Completions API implementation.\n * @internal\n */\nexport class ChatOpenAICompletions<\n  CallOptions extends ChatOpenAICompletionsCallOptions = ChatOpenAICompletionsCallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  /** @internal */\n  override invocationParams(\n    options?: this[\"ParsedCallOptions\"],\n    extra?: { streaming?: boolean }\n  ): ChatCompletionsInvocationParams {\n    let strict: boolean | undefined;\n    if (options?.strict !== undefined) {\n      strict = options.strict;\n    } else if (this.supportsStrictToolCalling !== undefined) {\n      strict = this.supportsStrictToolCalling;\n    }\n\n    let streamOptionsConfig = {};\n    if (options?.stream_options !== undefined) {\n      streamOptionsConfig = { stream_options: options.stream_options };\n    } else if (this.streamUsage && (this.streaming || extra?.streaming)) {\n      streamOptionsConfig = { stream_options: { include_usage: true } };\n    }\n\n    const params: Partial<ChatCompletionsInvocationParams> = {\n      model: this.model,\n      temperature: this.temperature,\n      top_p: this.topP,\n      frequency_penalty: this.frequencyPenalty,\n      presence_penalty: this.presencePenalty,\n      logprobs: this.logprobs,\n      top_logprobs: this.topLogprobs,\n      n: this.n,\n      logit_bias: this.logitBias,\n      stop: options?.stop ?? this.stopSequences,\n      user: this.user,\n      // if include_usage is set or streamUsage then stream must be set to true.\n      stream: this.streaming,\n      functions: options?.functions,\n      function_call: options?.function_call,\n      tools: options?.tools?.length\n        ? options.tools.map((tool) =>\n            this._convertChatOpenAIToolToCompletionsTool(tool, { strict })\n          )\n        : undefined,\n      tool_choice: formatToOpenAIToolChoice(\n        options?.tool_choice as OpenAIToolChoice\n      ),\n      response_format: this._getResponseFormat(options?.response_format),\n      seed: options?.seed,\n      ...streamOptionsConfig,\n      parallel_tool_calls: options?.parallel_tool_calls,\n      ...(this.audio || options?.audio\n        ? { audio: this.audio || options?.audio }\n        : {}),\n      ...(this.modalities || options?.modalities\n        ? { modalities: this.modalities || options?.modalities }\n        : {}),\n      ...this.modelKwargs,\n      prompt_cache_key: options?.promptCacheKey ?? this.promptCacheKey,\n      verbosity: options?.verbosity ?? this.verbosity,\n    };\n    if (options?.prediction !== undefined) {\n      params.prediction = options.prediction;\n    }\n    if (this.service_tier !== undefined) {\n      params.service_tier = this.service_tier;\n    }\n    if (options?.service_tier !== undefined) {\n      params.service_tier = options.service_tier;\n    }\n    const reasoning = this._getReasoningParams(options);\n    if (reasoning !== undefined && reasoning.effort !== undefined) {\n      params.reasoning_effort = reasoning.effort;\n    }\n    if (isReasoningModel(params.model)) {\n      params.max_completion_tokens =\n        this.maxTokens === -1 ? undefined : this.maxTokens;\n    } else {\n      params.max_tokens = this.maxTokens === -1 ? undefined : this.maxTokens;\n    }\n\n    return params as ChatCompletionsInvocationParams;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const usageMetadata = {} as UsageMetadata;\n    const params = this.invocationParams(options);\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      _convertMessagesToOpenAIParams(messages, this.model);\n\n    if (params.stream) {\n      const stream = this._streamResponseChunks(messages, options, runManager);\n      const finalChunks: Record<number, ChatGenerationChunk> = {};\n      for await (const chunk of stream) {\n        chunk.message.response_metadata = {\n          ...chunk.generationInfo,\n          ...chunk.message.response_metadata,\n        };\n        const index =\n          (chunk.generationInfo as NewTokenIndices)?.completion ?? 0;\n        if (finalChunks[index] === undefined) {\n          finalChunks[index] = chunk;\n        } else {\n          finalChunks[index] = finalChunks[index].concat(chunk);\n        }\n      }\n      const generations = Object.entries(finalChunks)\n        .sort(([aKey], [bKey]) => parseInt(aKey, 10) - parseInt(bKey, 10))\n        .map(([_, value]) => value);\n\n      const { functions, function_call } = this.invocationParams(options);\n\n      // OpenAI does not support token usage report under stream mode,\n      // fallback to estimation.\n\n      const promptTokenUsage = await this._getEstimatedTokenCountFromPrompt(\n        messages,\n        functions,\n        function_call\n      );\n      const completionTokenUsage = await this._getNumTokensFromGenerations(\n        generations\n      );\n\n      usageMetadata.input_tokens = promptTokenUsage;\n      usageMetadata.output_tokens = completionTokenUsage;\n      usageMetadata.total_tokens = promptTokenUsage + completionTokenUsage;\n      return {\n        generations,\n        llmOutput: {\n          estimatedTokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    } else {\n      const data = await this.completionWithRetry(\n        {\n          ...params,\n          stream: false,\n          messages: messagesMapped,\n        },\n        {\n          signal: options?.signal,\n          ...options?.options,\n        }\n      );\n\n      const {\n        completion_tokens: completionTokens,\n        prompt_tokens: promptTokens,\n        total_tokens: totalTokens,\n        prompt_tokens_details: promptTokensDetails,\n        completion_tokens_details: completionTokensDetails,\n      } = data?.usage ?? {};\n\n      if (completionTokens) {\n        usageMetadata.output_tokens =\n          (usageMetadata.output_tokens ?? 0) + completionTokens;\n      }\n\n      if (promptTokens) {\n        usageMetadata.input_tokens =\n          (usageMetadata.input_tokens ?? 0) + promptTokens;\n      }\n\n      if (totalTokens) {\n        usageMetadata.total_tokens =\n          (usageMetadata.total_tokens ?? 0) + totalTokens;\n      }\n\n      if (\n        promptTokensDetails?.audio_tokens !== null ||\n        promptTokensDetails?.cached_tokens !== null\n      ) {\n        usageMetadata.input_token_details = {\n          ...(promptTokensDetails?.audio_tokens !== null && {\n            audio: promptTokensDetails?.audio_tokens,\n          }),\n          ...(promptTokensDetails?.cached_tokens !== null && {\n            cache_read: promptTokensDetails?.cached_tokens,\n          }),\n        };\n      }\n\n      if (\n        completionTokensDetails?.audio_tokens !== null ||\n        completionTokensDetails?.reasoning_tokens !== null\n      ) {\n        usageMetadata.output_token_details = {\n          ...(completionTokensDetails?.audio_tokens !== null && {\n            audio: completionTokensDetails?.audio_tokens,\n          }),\n          ...(completionTokensDetails?.reasoning_tokens !== null && {\n            reasoning: completionTokensDetails?.reasoning_tokens,\n          }),\n        };\n      }\n\n      const generations: ChatGeneration[] = [];\n      for (const part of data?.choices ?? []) {\n        const text = part.message?.content ?? \"\";\n        const generation: ChatGeneration = {\n          text,\n          message: this._convertCompletionsMessageToBaseMessage(\n            part.message ?? { role: \"assistant\" },\n            data\n          ),\n        };\n        generation.generationInfo = {\n          ...(part.finish_reason ? { finish_reason: part.finish_reason } : {}),\n          ...(part.logprobs ? { logprobs: part.logprobs } : {}),\n        };\n        if (isAIMessage(generation.message)) {\n          generation.message.usage_metadata = usageMetadata;\n        }\n        // Fields are not serialized unless passed to the constructor\n        // Doing this ensures all fields on the message are serialized\n        generation.message = new AIMessage(\n          Object.fromEntries(\n            Object.entries(generation.message).filter(\n              ([key]) => !key.startsWith(\"lc_\")\n            )\n          ) as BaseMessageFields\n        );\n        generations.push(generation);\n      }\n      return {\n        generations,\n        llmOutput: {\n          tokenUsage: {\n            promptTokens: usageMetadata.input_tokens,\n            completionTokens: usageMetadata.output_tokens,\n            totalTokens: usageMetadata.total_tokens,\n          },\n        },\n      };\n    }\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const messagesMapped: OpenAIClient.Chat.Completions.ChatCompletionMessageParam[] =\n      _convertMessagesToOpenAIParams(messages, this.model);\n\n    const params = {\n      ...this.invocationParams(options, {\n        streaming: true,\n      }),\n      messages: messagesMapped,\n      stream: true as const,\n    };\n    let defaultRole: OpenAIClient.Chat.ChatCompletionRole | undefined;\n\n    const streamIterable = await this.completionWithRetry(params, options);\n    let usage: OpenAIClient.Completions.CompletionUsage | undefined;\n    for await (const data of streamIterable) {\n      const choice = data?.choices?.[0];\n      if (data.usage) {\n        usage = data.usage;\n      }\n      if (!choice) {\n        continue;\n      }\n\n      const { delta } = choice;\n      if (!delta) {\n        continue;\n      }\n      const chunk = this._convertCompletionsDeltaToBaseMessageChunk(\n        delta,\n        data,\n        defaultRole\n      );\n      defaultRole = delta.role ?? defaultRole;\n      const newTokenIndices = {\n        prompt: options.promptIndex ?? 0,\n        completion: choice.index ?? 0,\n      };\n      if (typeof chunk.content !== \"string\") {\n        console.log(\n          \"[WARNING]: Received non-string content from OpenAI. This is currently not supported.\"\n        );\n        continue;\n      }\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      const generationInfo: Record<string, any> = { ...newTokenIndices };\n      if (choice.finish_reason != null) {\n        generationInfo.finish_reason = choice.finish_reason;\n        // Only include system fingerprint in the last chunk for now\n        // to avoid concatenation issues\n        generationInfo.system_fingerprint = data.system_fingerprint;\n        generationInfo.model_name = data.model;\n        generationInfo.service_tier = data.service_tier;\n      }\n      if (this.logprobs) {\n        generationInfo.logprobs = choice.logprobs;\n      }\n      const generationChunk = new ChatGenerationChunk({\n        message: chunk,\n        text: chunk.content,\n        generationInfo,\n      });\n      yield generationChunk;\n      await runManager?.handleLLMNewToken(\n        generationChunk.text ?? \"\",\n        newTokenIndices,\n        undefined,\n        undefined,\n        undefined,\n        { chunk: generationChunk }\n      );\n    }\n    if (usage) {\n      const inputTokenDetails = {\n        ...(usage.prompt_tokens_details?.audio_tokens !== null && {\n          audio: usage.prompt_tokens_details?.audio_tokens,\n        }),\n        ...(usage.prompt_tokens_details?.cached_tokens !== null && {\n          cache_read: usage.prompt_tokens_details?.cached_tokens,\n        }),\n      };\n      const outputTokenDetails = {\n        ...(usage.completion_tokens_details?.audio_tokens !== null && {\n          audio: usage.completion_tokens_details?.audio_tokens,\n        }),\n        ...(usage.completion_tokens_details?.reasoning_tokens !== null && {\n          reasoning: usage.completion_tokens_details?.reasoning_tokens,\n        }),\n      };\n      const generationChunk = new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: \"\",\n          response_metadata: {\n            usage: { ...usage },\n          },\n          usage_metadata: {\n            input_tokens: usage.prompt_tokens,\n            output_tokens: usage.completion_tokens,\n            total_tokens: usage.total_tokens,\n            ...(Object.keys(inputTokenDetails).length > 0 && {\n              input_token_details: inputTokenDetails,\n            }),\n            ...(Object.keys(outputTokenDetails).length > 0 && {\n              output_token_details: outputTokenDetails,\n            }),\n          },\n        }),\n        text: \"\",\n      });\n      yield generationChunk;\n    }\n    if (options.signal?.aborted) {\n      throw new Error(\"AbortError\");\n    }\n  }\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<OpenAIClient.Chat.Completions.ChatCompletion>;\n\n  async completionWithRetry(\n    request: OpenAIClient.Chat.ChatCompletionCreateParams,\n    requestOptions?: OpenAIClient.RequestOptions\n  ): Promise<\n    | AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>\n    | OpenAIClient.Chat.Completions.ChatCompletion\n  > {\n    const clientOptions = this._getClientOptions(requestOptions);\n    const isParseableFormat =\n      request.response_format && request.response_format.type === \"json_schema\";\n    return this.caller.call(async () => {\n      try {\n        if (isParseableFormat && !request.stream) {\n          return await this.client.chat.completions.parse(\n            request,\n            clientOptions\n          );\n        } else {\n          return await this.client.chat.completions.create(\n            request,\n            clientOptions\n          );\n        }\n      } catch (e) {\n        const error = wrapOpenAIClientError(e);\n        throw error;\n      }\n    });\n  }\n\n  /** @internal */\n  protected _convertCompletionsMessageToBaseMessage(\n    message: OpenAIClient.Chat.Completions.ChatCompletionMessage,\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletion\n  ): BaseMessage {\n    const rawToolCalls: OpenAIToolCall[] | undefined = message.tool_calls as\n      | OpenAIToolCall[]\n      | undefined;\n    switch (message.role) {\n      case \"assistant\": {\n        const toolCalls = [];\n        const invalidToolCalls = [];\n        for (const rawToolCall of rawToolCalls ?? []) {\n          try {\n            toolCalls.push(parseToolCall(rawToolCall, { returnId: true }));\n            // eslint-disable-next-line @typescript-eslint/no-explicit-any\n          } catch (e: any) {\n            invalidToolCalls.push(makeInvalidToolCall(rawToolCall, e.message));\n          }\n        }\n        const additional_kwargs: Record<string, unknown> = {\n          function_call: message.function_call,\n          tool_calls: rawToolCalls,\n        };\n        if (this.__includeRawResponse !== undefined) {\n          additional_kwargs.__raw_response = rawResponse;\n        }\n        const response_metadata: Record<string, unknown> | undefined = {\n          model_provider: \"openai\",\n          model_name: rawResponse.model,\n          ...(rawResponse.system_fingerprint\n            ? {\n                usage: { ...rawResponse.usage },\n                system_fingerprint: rawResponse.system_fingerprint,\n              }\n            : {}),\n        };\n\n        if (message.audio) {\n          additional_kwargs.audio = message.audio;\n        }\n\n        const content = handleMultiModalOutput(\n          message.content || \"\",\n          rawResponse.choices?.[0]?.message\n        );\n        return new AIMessage({\n          content,\n          tool_calls: toolCalls,\n          invalid_tool_calls: invalidToolCalls,\n          additional_kwargs,\n          response_metadata,\n          id: rawResponse.id,\n        });\n      }\n      default:\n        return new ChatMessage(\n          message.content || \"\",\n          message.role ?? \"unknown\"\n        );\n    }\n  }\n\n  /** @internal */\n  protected _convertCompletionsDeltaToBaseMessageChunk(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>,\n    rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk,\n    defaultRole?: OpenAIClient.Chat.ChatCompletionRole\n  ) {\n    const role = delta.role ?? defaultRole;\n    const content = delta.content ?? \"\";\n    let additional_kwargs: Record<string, unknown>;\n    if (delta.function_call) {\n      additional_kwargs = {\n        function_call: delta.function_call,\n      };\n    } else if (delta.tool_calls) {\n      additional_kwargs = {\n        tool_calls: delta.tool_calls,\n      };\n    } else {\n      additional_kwargs = {};\n    }\n    if (this.__includeRawResponse) {\n      additional_kwargs.__raw_response = rawResponse;\n    }\n\n    if (delta.audio) {\n      additional_kwargs.audio = {\n        ...delta.audio,\n        index: rawResponse.choices[0].index,\n      };\n    }\n\n    const response_metadata = {\n      model_provider: \"openai\",\n      usage: { ...rawResponse.usage },\n    };\n    if (role === \"user\") {\n      return new HumanMessageChunk({ content, response_metadata });\n    } else if (role === \"assistant\") {\n      const toolCallChunks: ToolCallChunk[] = [];\n      if (Array.isArray(delta.tool_calls)) {\n        for (const rawToolCall of delta.tool_calls) {\n          toolCallChunks.push({\n            name: rawToolCall.function?.name,\n            args: rawToolCall.function?.arguments,\n            id: rawToolCall.id,\n            index: rawToolCall.index,\n            type: \"tool_call_chunk\",\n          });\n        }\n      }\n      return new AIMessageChunk({\n        content,\n        tool_call_chunks: toolCallChunks,\n        additional_kwargs,\n        id: rawResponse.id,\n        response_metadata,\n      });\n    } else if (role === \"system\") {\n      return new SystemMessageChunk({ content, response_metadata });\n    } else if (role === \"developer\") {\n      return new SystemMessageChunk({\n        content,\n        response_metadata,\n        additional_kwargs: {\n          __openai_role__: \"developer\",\n        },\n      });\n    } else if (role === \"function\") {\n      return new FunctionMessageChunk({\n        content,\n        additional_kwargs,\n        name: delta.name,\n        response_metadata,\n      });\n    } else if (role === \"tool\") {\n      return new ToolMessageChunk({\n        content,\n        additional_kwargs,\n        tool_call_id: delta.tool_call_id,\n        response_metadata,\n      });\n    } else {\n      return new ChatMessageChunk({ content, role, response_metadata });\n    }\n  }\n}\n\nexport type ChatOpenAICallOptions = ChatOpenAICompletionsCallOptions &\n  ChatOpenAIResponsesCallOptions;\n\nexport interface ChatOpenAIFields extends BaseChatOpenAIFields {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi?: boolean;\n  /**\n   * The completions chat instance\n   * @internal\n   */\n  completions?: ChatOpenAICompletions;\n  /**\n   * The responses chat instance\n   * @internal\n   */\n  responses?: ChatOpenAIResponses;\n}\n\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // configuration: {\n *   //   baseURL: \"...\",\n *   // }\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport class ChatOpenAI<\n  CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions\n> extends BaseChatOpenAI<CallOptions> {\n  /**\n   * Whether to use the responses API for all requests. If `false` the responses API will be used\n   * only when required in order to fulfill the request.\n   */\n  useResponsesApi = false;\n\n  protected responses: ChatOpenAIResponses;\n\n  protected completions: ChatOpenAICompletions;\n\n  get lc_serializable_keys(): string[] {\n    return [...super.lc_serializable_keys, \"useResponsesApi\"];\n  }\n\n  constructor(protected fields?: ChatOpenAIFields) {\n    super(fields);\n    this.useResponsesApi = fields?.useResponsesApi ?? false;\n    this.responses = fields?.responses ?? new ChatOpenAIResponses(fields);\n    this.completions = fields?.completions ?? new ChatOpenAICompletions(fields);\n  }\n\n  protected _useResponsesApi(options: this[\"ParsedCallOptions\"] | undefined) {\n    const usesBuiltInTools = options?.tools?.some(isBuiltInTool);\n    const hasResponsesOnlyKwargs =\n      options?.previous_response_id != null ||\n      options?.text != null ||\n      options?.truncation != null ||\n      options?.include != null ||\n      options?.reasoning?.summary != null ||\n      this.reasoning?.summary != null;\n    const hasCustomTools = options?.tools?.some(isOpenAICustomTool);\n\n    return (\n      this.useResponsesApi ||\n      usesBuiltInTools ||\n      hasResponsesOnlyKwargs ||\n      hasCustomTools\n    );\n  }\n\n  override getLsParams(options: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.getLsParams(optionsWithDefaults);\n    }\n    return this.completions.getLsParams(optionsWithDefaults);\n  }\n\n  override invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    const optionsWithDefaults = this._combineCallOptions(options);\n    if (this._useResponsesApi(options)) {\n      return this.responses.invocationParams(optionsWithDefaults);\n    }\n    return this.completions.invocationParams(optionsWithDefaults);\n  }\n\n  /** @ignore */\n  override async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (this._useResponsesApi(options)) {\n      return this.responses._generate(messages, options);\n    }\n    return this.completions._generate(messages, options, runManager);\n  }\n\n  override async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (this._useResponsesApi(options)) {\n      yield* this.responses._streamResponseChunks(\n        messages,\n        this._combineCallOptions(options),\n        runManager\n      );\n      return;\n    }\n    yield* this.completions._streamResponseChunks(\n      messages,\n      this._combineCallOptions(options),\n      runManager\n    );\n  }\n\n  override withConfig(\n    config: Partial<CallOptions>\n  ): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions> {\n    const newModel = new ChatOpenAI<CallOptions>(this.fields);\n    newModel.defaultOptions = { ...this.defaultOptions, ...config };\n    return newModel;\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;AA+GA,MAAM,6BAA6B;;AA6InC,IAAsB,iBAAtB,cAGUA,2DAEV;CACE;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA,QAAQ;CAER;CAEA;CAEA;CAEA;CAEA;CAEA,YAAY;CAEZ,cAAc;CAEd;CAEA;CAEA;CAEA;CAEA;CAEA;;CAGA;;CAGA;;;;;CAMA;CAEA;CAEA;CAEA;;;;;;;;;;;;CAaA;;;;;CAMA;;;;;;CAOA;;;;CAKA;CAEA,AAAU;CAEV,WAAW;AACT,SAAO;CACR;CAED,OAAO,UAAU;AACf,SAAO;CACR;CAED,IAAI,WAAW;AACb,SAAO;GACL,GAAG,MAAM;GACT;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;EACD;CACF;CAED,kBAAkB;CAElB,IAAI,aAAoD;AACtD,SAAO;GACL,QAAQ;GACR,cAAc;EACf;CACF;CAED,IAAI,aAAqC;AACvC,SAAO;GACL,QAAQ;GACR,WAAW;EACZ;CACF;CAED,IAAI,uBAAiC;AACnC,SAAO;GACL;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;EACD;CACF;CAED,YAAYC,SAAqD;EAC/D,MAAM,SAAS,KAAK,iBAAiB,QAAQ;AAC7C,SAAO;GACL,aAAa;GACb,eAAe,KAAK;GACpB,eAAe;GACf,gBAAgB,OAAO,eAAe;GACtC,eAAe,OAAO,cAAc;GACpC,SAAS,QAAQ;EAClB;CACF;;CAGD,qBAKkB;AAChB,SAAO;GACL,YAAY,KAAK;GACjB,GAAG,KAAK,kBAAkB;GAC1B,GAAG,KAAK;EACT;CACF;;;;CAKD,oBAAoB;AAClB,SAAO,KAAK,oBAAoB;CACjC;CAED,YAAYC,QAA+B;EACzC,MAAM,UAAU,CAAE,EAAC;EAEnB,KAAK,SACH,QAAQ,UACR,QAAQ,eAAe,iEACA,iBAAiB;EAC1C,KAAK,eACH,QAAQ,eAAe,uEACA,sBAAsB;EAE/C,KAAK,QAAQ,QAAQ,SAAS,QAAQ,aAAa,KAAK;EACxD,KAAK,cAAc,QAAQ,eAAe,CAAE;EAC5C,KAAK,UAAU,QAAQ;EAEvB,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,OAAO,QAAQ,QAAQ,KAAK;EACjC,KAAK,mBAAmB,QAAQ,oBAAoB,KAAK;EACzD,KAAK,kBAAkB,QAAQ,mBAAmB,KAAK;EACvD,KAAK,WAAW,QAAQ;EACxB,KAAK,cAAc,QAAQ;EAC3B,KAAK,IAAI,QAAQ,KAAK,KAAK;EAC3B,KAAK,YAAY,QAAQ;EACzB,KAAK,OAAO,QAAQ,iBAAiB,QAAQ;EAC7C,KAAK,gBAAgB,KAAK;EAC1B,KAAK,OAAO,QAAQ;EACpB,KAAK,uBAAuB,QAAQ;EACpC,KAAK,QAAQ,QAAQ;EACrB,KAAK,aAAa,QAAQ;EAC1B,KAAK,YAAY,QAAQ;EACzB,KAAK,YAAY,QAAQ,uBAAuB,QAAQ;EACxD,KAAK,iBAAiB,QAAQ,kBAAkB,KAAK;EACrD,KAAK,YAAY,QAAQ,aAAa,KAAK;EAE3C,KAAK,mBAAmB,QAAQ,qBAAqB;EACrD,KAAK,YAAY,QAAQ,cAAc;AACvC,MAAI,KAAK,kBAAkB,KAAK,YAAY;AAE5C,MAAI,QAAQ,cAAc,OAAO,KAAK,mBAAmB;EAEzD,KAAK,cAAc,QAAQ,eAAe,KAAK;AAC/C,MAAI,KAAK,kBAAkB,KAAK,cAAc;EAE9C,KAAK,eAAe;GAClB,QAAQ,KAAK;GACb,cAAc,KAAK;GACnB,yBAAyB;GACzB,GAAG,QAAQ;EACZ;AAID,MAAI,QAAQ,8BAA8B,QACxC,KAAK,4BAA4B,OAAO;AAG1C,MAAI,QAAQ,iBAAiB,QAC3B,KAAK,eAAe,OAAO;EAG7B,KAAK,aAAa,QAAQ,cAAc;CACzC;;;;;CAMD,AAAU,oBACRC,SACoC;AACpC,MAAI,CAACC,8BAAiB,KAAK,MAAM,CAC/B;EAIF,IAAIC;AACJ,MAAI,KAAK,cAAc,QACrB,YAAY;GACV,GAAG;GACH,GAAG,KAAK;EACT;AAEH,MAAI,SAAS,cAAc,QACzB,YAAY;GACV,GAAG;GACH,GAAG,QAAQ;EACZ;AAGH,SAAO;CACR;;;;;CAMD,AAAU,mBACRC,WACyC;AACzC,MACE,aACA,UAAU,SAAS,iBACnB,UAAU,YAAY,+DACH,UAAU,YAAY,OAAO,CAEhD,QAAOC,wCACL,UAAU,YAAY,QACtB,UAAU,YAAY,MACtB,EACE,aAAa,UAAU,YAAY,YACpC,EACF;AAEH,SAAO;CACR;CAED,AAAU,oBACRC,mBAC2B;AAC3B,SAAO;GACL,GAAG,KAAK;GACR,GAAI,qBAAqB,CAAE;EAC5B;CACF;;CAGD,kBACEC,SAC0B;AAC1B,MAAI,CAAC,KAAK,QAAQ;GAChB,MAAMC,uBAA6C,EACjD,SAAS,KAAK,aAAa,QAC5B;GAED,MAAM,WAAWC,0BAAY,qBAAqB;GAClD,MAAM,SAAS;IACb,GAAG,KAAK;IACR,SAAS;IACT,SAAS,KAAK;IACd,YAAY;GACb;AACD,OAAI,CAAC,OAAO,SACV,OAAO,OAAO;GAGhB,KAAK,SAAS,IAAIC,cAAa;EAChC;EACD,MAAM,iBAAiB;GACrB,GAAG,KAAK;GACR,GAAG;EACJ;AACD,SAAO;CACR;CAGD,AAAU,wCACRC,MACAC,QACiC;AACjC,MAAIC,2BAAa,KAAK,CACpB,QAAOC,yCAA2B,KAAK,SAAS,WAAW;AAE7D,8DAAyB,KAAK,EAAE;AAC9B,OAAI,QAAQ,WAAW,OACrB,QAAO;IACL,GAAG;IACH,UAAU;KACR,GAAG,KAAK;KACR,QAAQ,OAAO;IAChB;GACF;AAGH,UAAO;EACR;AACD,SAAOC,mCAAqB,MAAM,OAAO;CAC1C;CAED,AAAS,UACPC,OACAC,QAC+D;EAC/D,IAAIC;AACJ,MAAI,QAAQ,WAAW,QACrB,SAAS,OAAO;WACP,KAAK,8BAA8B,QAC5C,SAAS,KAAK;AAEhB,SAAO,KAAK,WAAW;GACrB,OAAO,MAAM,IAAI,CAAC,SAChBC,4BAAc,KAAK,GACf,OACA,KAAK,wCAAwC,MAAM,EAAE,OAAQ,EAAC,CACnE;GACD,GAAG;EACJ,EAAyB;CAC3B;CAED,MAAe,OAAOC,OAA+BC,SAAuB;AAC1E,SAAO,MAAM,OACX,OACA,KAAK,oBAAoB,QAAQ,CAClC;CACF;CAED,MAAe,OAAOD,OAA+BC,SAAuB;AAC1E,SAAO,MAAM,OACX,OACA,KAAK,oBAAoB,QAAQ,CAClC;CACF;;CAGD,kBAAkB,GAAG,YAAgD;AACnE,SAAO,WAAW,OAGhB,CAAC,KAAK,cAAc;AAClB,OAAI,aAAa,UAAU,YAAY;IACrC,IAAI,WAAW,oBACb,UAAU,WAAW,oBAAoB;IAC3C,IAAI,WAAW,gBAAgB,UAAU,WAAW,gBAAgB;IACpE,IAAI,WAAW,eAAe,UAAU,WAAW,eAAe;GACnE;AACD,UAAO;EACR,GACD,EACE,YAAY;GACV,kBAAkB;GAClB,cAAc;GACd,aAAa;EACd,EACF,EACF;CACF;CAED,MAAM,yBAAyBC,UAAyB;EACtD,IAAI,aAAa;EACjB,IAAI,mBAAmB;EACvB,IAAI,gBAAgB;AAGpB,MAAI,KAAK,UAAU,sBAAsB;GACvC,mBAAmB;GACnB,gBAAgB;EACjB,OAAM;GACL,mBAAmB;GACnB,gBAAgB;EACjB;EAED,MAAM,kBAAkB,MAAM,QAAQ,IACpC,SAAS,IAAI,OAAO,YAAY;GAC9B,MAAM,YAAY,MAAM,KAAK,aAAa,QAAQ,QAAQ;GAC1D,MAAM,YAAY,MAAM,KAAK,aAAaC,iCAAoB,QAAQ,CAAC;GACvE,MAAM,YACJ,QAAQ,SAAS,SACb,gBAAiB,MAAM,KAAK,aAAa,QAAQ,KAAK,GACtD;GACN,IAAI,QAAQ,YAAY,mBAAmB,YAAY;GAGvD,MAAM,gBAAgB;AACtB,OAAI,cAAc,UAAU,KAAK,YAC/B,SAAS;AAEX,OAAI,cAAc,mBAAmB,eACnC,SAAS;AAEX,OAAI,eAAe,kBAAkB,eAAe,MAClD,SAAS,MAAM,KAAK,aAClB,cAAc,kBAAkB,eAAe,KAChD;AAEH,OAAI,cAAc,kBAAkB,eAAe,UACjD,KAAI;IACF,SAAS,MAAM,KAAK,aAElB,KAAK,UACH,KAAK,MACH,cAAc,kBAAkB,eAAe,UAChD,CACF,CACF;GACF,SAAQ,OAAO;IACd,QAAQ,MACN,oCACA,OACA,KAAK,UAAU,cAAc,kBAAkB,cAAc,CAC9D;IACD,SAAS,MAAM,KAAK,aAClB,cAAc,kBAAkB,eAAe,UAChD;GACF;GAGH,cAAc;AACd,UAAO;EACR,EAAC,CACH;EAED,cAAc;AAEd,SAAO;GAAE;GAAY;EAAiB;CACvC;;CAGD,MAAgB,6BAA6BC,aAA+B;EAC1E,MAAM,mBAAmB,MAAM,QAAQ,IACrC,YAAY,IAAI,OAAO,eAAe;AACpC,OAAI,WAAW,QAAQ,mBAAmB,cACxC,SAAQ,MAAM,KAAK,yBAAyB,CAAC,WAAW,OAAQ,EAAC,EAC9D,gBAAgB;OAEnB,QAAO,MAAM,KAAK,aAAa,WAAW,QAAQ,QAAQ;EAE7D,EAAC,CACH;AAED,SAAO,iBAAiB,OAAO,CAAC,GAAG,MAAM,IAAI,GAAG,EAAE;CACnD;;CAGD,MAAgB,kCACdF,UACAG,WACAC,eAIiB;EAIjB,IAAI,UAAU,MAAM,KAAK,yBAAyB,SAAS,EAAE;AAG7D,MAAI,aAAa,kBAAkB,QAAQ;GACzC,MAAM,oBAAoBC,wCACxB,UACD;GACD,UAAU,MAAM,KAAK,aAAa,kBAAkB;GACpD,UAAU;EACX;AAKD,MAAI,aAAa,SAAS,KAAK,CAAC,MAAM,EAAE,UAAU,KAAK,SAAS,EAC9D,UAAU;AAMZ,MAAI,kBAAkB,QACpB,UAAU;WACD,OAAO,kBAAkB,UAClC,UAAW,MAAM,KAAK,aAAa,cAAc,KAAK,GAAI;AAG5D,SAAO;CACR;;CAGD,AAAU,2BACRC,QACA;EACA,MAAM,gBAAgB,EAAE,GAAG,OAAQ;AACnC,MACE,CAAC,KAAK,MAAM,WAAW,QAAQ,IAC/B,CAAC,KAAK,MAAM,WAAW,SAAS,IAChC,KAAK,UAAU,SAEf;OAAI,eAAe,WAAW,OAC5B,QAAO;EACR,WACQ,cAAc,WAAW,cAClC,QAAQ,KACN,CAAC,mDAAmD,EAAE,KAAK,MAAM,gCAAgC,CAAC,CACnG;AAEH,SAAO,cAAc;CACtB;;;;;;;;;;;;;;;;;;;;CAwDD,qBAGEC,cACAC,QACA;EACA,IAAIC;EACJ,IAAIC;EAEJ,MAAM,EAAE,QAAQ,MAAM,YAAY,GAAG;GACnC,GAAG;GACH,QAAQ;EACT;AAED,MAAI,QAAQ,WAAW,UAAa,OAAO,WAAW,WACpD,OAAM,IAAI,MACR;EAIJ,MAAM,SAASC,yCAA0B,KAAK,OAAO,QAAQ,OAAO;AAEpE,MAAI,WAAW,YAAY;AACzB,4DAAuB,OAAO,EAC5B,eAAeC,uDAAuB,cAAc,OAAO;QAE3D,eAAe,IAAIC;GAErB,MAAM,oEAA4B,OAAO;GACzC,MAAM,KAAK,WAAW;IACpB,iBAAiB,EAAE,MAAM,cAAe;IACxC,6BAA6B;KAC3B,QAAQ,EAAE,QAAQ,YAAa;KAC/B,QAAQ;MAAE,OAAO,QAAQ;MAAW,GAAG;KAAc;IACtD;GACF,EAAyB;EAC3B,WAAU,WAAW,cAAc;GAClC,MAAM,yBAAyB;IAC7B,MAAM,QAAQ;IACd,oEAAkC,OAAO;IACzC;IACA,QAAQ,QAAQ;GACjB;GACD,MAAM,oEAA4B,uBAAuB,OAAO;GAChE,MAAM,KAAK,WAAW;IACpB,iBAAiB;KACf,MAAM;KACN,aAAa;IACd;IACD,6BAA6B;KAC3B,QAAQ,EAAE,QAAQ,cAAe;KACjC,QAAQ;MACN,OAAO,uBAAuB;MAC9B,aAAa,uBAAuB;MACpC,GAAG;KACJ;IACF;GACF,EAAyB;AAC1B,4DAAuB,OAAO,EAAE;IAC9B,MAAM,YAAYD,uDAAuB,cAAc,OAAO;IAC9D,eAAeE,0CAAe,KAC5B,CAACC,cAA8B;AAC7B,SAAI,YAAY,UAAU,kBACxB,QAAO,UAAU,kBAAkB;AAErC,YAAO;IACR,EACF;GACF,OACC,eAAe,IAAIF;EAEtB,OAAM;GACL,IAAI,eAAe,QAAQ;AAE3B,4DAAuB,OAAO,EAAE;IAC9B,MAAM,oEAA4B,OAAO;IACzC,MAAM,KAAK,WAAW;KACpB,OAAO,CACL;MACE,MAAM;MACN,UAAU;OACR,MAAM;OACN,aAAa,aAAa;OAC1B,YAAY;MACb;KACF,CACF;KACD,aAAa;MACX,MAAM;MACN,UAAU,EACR,MAAM,aACP;KACF;KACD,6BAA6B;MAC3B,QAAQ,EAAE,QAAQ,mBAAoB;MACtC,QAAQ;OAAE,OAAO;OAAc,GAAG;MAAc;KACjD;KAED,GAAI,QAAQ,WAAW,SAAY,EAAE,QAAQ,OAAO,OAAQ,IAAG,CAAE;IAClE,EAAyB;IAC1B,eAAe,IAAIG,sEAAyB;KAC1C,cAAc;KACd,SAAS;KACT,WAAW;IACZ;GACF,OAAM;IACL,IAAIC;AACJ,QACE,OAAO,OAAO,SAAS,YACvB,OAAO,OAAO,eAAe,YAC7B,OAAO,cAAc,MACrB;KACA,2BAA2B;KAC3B,eAAe,OAAO;IACvB,OAAM;KACL,eAAgB,OAAO,SAAoB;KAC3C,2BAA2B;MACzB,MAAM;MACN,aAAc,OAAO,eAA0B;MAC/C,YAAY;KACb;IACF;IACD,MAAM,oEAA4B,OAAO;IACzC,MAAM,KAAK,WAAW;KACpB,OAAO,CACL;MACE,MAAM;MACN,UAAU;KACX,CACF;KACD,aAAa;MACX,MAAM;MACN,UAAU,EACR,MAAM,aACP;KACF;KACD,6BAA6B;MAC3B,QAAQ,EAAE,QAAQ,mBAAoB;MACtC,QAAQ;OAAE,OAAO;OAAc,GAAG;MAAc;KACjD;KAED,GAAI,QAAQ,WAAW,SAAY,EAAE,QAAQ,OAAO,OAAQ,IAAG,CAAE;IAClE,EAAyB;IAC1B,eAAe,IAAID,sEAAoC;KACrD,cAAc;KACd,SAAS;IACV;GACF;EACF;AAED,MAAI,CAAC,WACH,QAAO,IAAI,KAAK,aAAa;EAM/B,MAAM,eAAeE,+CAAoB,OAAO,EAE9C,QAAQ,CAACC,OAAYC,aAAW,aAAa,OAAO,MAAM,KAAKA,SAAO,CACvE,EAAC;EACF,MAAM,aAAaF,+CAAoB,OAAO,EAC5C,QAAQ,MAAM,KACf,EAAC;EACF,MAAM,qBAAqB,aAAa,cAAc,EACpD,WAAW,CAAC,UAAW,EACxB,EAAC;AACF,SAAOG,4CAAiB,KAGtB,CAAC,EAAE,KAAK,IAAK,GAAE,kBAAmB,EAAC;CACtC;AACF;;;;;;;;AAwDD,IAAa,sBAAb,cAEU,eAA4B;CACpC,AAAS,iBACP1C,SAC+B;EAC/B,IAAIiB;AACJ,MAAI,SAAS,WAAW,QACtB,SAAS,QAAQ;WACR,KAAK,8BAA8B,QAC5C,SAAS,KAAK;EAGhB,MAAM0B,SAAwC;GAC5C,OAAO,KAAK;GACZ,aAAa,KAAK;GAClB,OAAO,KAAK;GACZ,MAAM,KAAK;GAGX,QAAQ,KAAK;GACb,sBAAsB,SAAS;GAC/B,YAAY,SAAS;GACrB,SAAS,SAAS;GAClB,OAAO,SAAS,OAAO,SACnB,KAAK,uBAAuB,QAAQ,OAAO;IACzC,QAAQ,KAAK;IACb;GACD,EAAC,GACF;GACJ,aAAaC,kCAAoB,SAAS,YAAY,GAClD,SAAS,eACR,MAAM;IACL,MAAM,YAAYC,uCAAyB,SAAS,YAAY;AAChE,QAAI,OAAO,cAAc,YAAY,UAAU,WAC7C;SAAI,UAAU,SAAS,WACrB,QAAO;MAAE,MAAM;MAAY,MAAM,UAAU,SAAS;KAAM;cACjD,UAAU,SAAS,gBAC5B,QAAO;MACL,MAAM;MACN,MAAM,UAAU,cAAc;MAC9B,OAAO,UAAU,cAAc;KAChC;cACQ,UAAU,SAAS,SAC5B,QAAO;MACL,MAAM;MACN,MAAM,UAAU,OAAO;KACxB;IACF;AAEH,WAAO;GACR,IAAG;GACR,OAAO,MAAM;AACX,QAAI,SAAS,KAAM,QAAO,QAAQ;IAClC,MAAM,SAAS,KAAK,mBAAmB,SAAS,gBAAgB;AAChE,QAAI,QAAQ,SAAS,eAAe;AAClC,SAAI,OAAO,YAAY,UAAU,KAC/B,QAAO;MACL,QAAQ;OACN,MAAM;OACN,QAAQ,OAAO,YAAY;OAC3B,aAAa,OAAO,YAAY;OAChC,MAAM,OAAO,YAAY;OACzB,QAAQ,OAAO,YAAY;MAC5B;MACD,WAAW,SAAS;KACrB;AAEH,YAAO;IACR;AACD,WAAO;KAAE;KAAQ,WAAW,SAAS;IAAW;GACjD,IAAG;GACJ,qBAAqB,SAAS;GAC9B,mBAAmB,KAAK,cAAc,KAAK,SAAY,KAAK;GAC5D,kBAAkB,SAAS,kBAAkB,KAAK;GAClD,GAAI,KAAK,aAAa,EAAE,OAAO,MAAO,IAAG,CAAE;GAC3C,GAAG,KAAK;EACT;EAED,MAAM,YAAY,KAAK,oBAAoB,QAAQ;AAEnD,MAAI,cAAc,QAChB,OAAO,YAAY;AAGrB,SAAO;CACR;CAED,MAAM,UACJxB,UACAvB,SACqB;EACrB,MAAM,mBAAmB,KAAK,iBAAiB,QAAQ;AACvD,MAAI,iBAAiB,QAAQ;GAC3B,MAAM,SAAS,KAAK,sBAAsB,UAAU,QAAQ;GAC5D,IAAIgD;AACJ,cAAW,MAAM,SAAS,QAAQ;IAChC,MAAM,QAAQ,oBAAoB;KAChC,GAAG,MAAM;KACT,GAAG,MAAM,QAAQ;IAClB;IACD,aAAa,YAAY,OAAO,MAAM,IAAI;GAC3C;AAED,UAAO;IACL,aAAa,aAAa,CAAC,UAAW,IAAG,CAAE;IAC3C,WAAW,EACT,sBAAsB,YAAY,UAC9B,eACL;GACF;EACF,OAAM;GACL,MAAM,QAAQ,KAAK,kCAAkC,SAAS;GAC9D,MAAM,OAAO,MAAM,KAAK,oBACtB;IACE;IACA,GAAG;IACH,QAAQ;GACT,GACD;IAAE,QAAQ,SAAS;IAAQ,GAAG,SAAS;GAAS,EACjD;AAED,UAAO;IACL,aAAa,CACX;KACE,MAAM,KAAK;KACX,SAAS,KAAK,sCAAsC,KAAK;IAC1D,CACF;IACD,WAAW;KACT,IAAI,KAAK;KACT,qBAAqB,KAAK,QACtB;MACE,cAAc,KAAK,MAAM;MACzB,kBAAkB,KAAK,MAAM;MAC7B,aAAa,KAAK,MAAM;KACzB,IACD;IACL;GACF;EACF;CACF;CAED,OAAO,sBACLzB,UACAvB,SACAiD,YACqC;EACrC,MAAM,iBAAiB,MAAM,KAAK,oBAChC;GACE,GAAG,KAAK,iBAAiB,QAAQ;GACjC,OAAO,KAAK,kCAAkC,SAAS;GACvD,QAAQ;EACT,GACD,QACD;AAED,aAAW,MAAM,QAAQ,gBAAgB;GACvC,MAAM,QAAQ,KAAK,yCAAyC,KAAK;AACjE,OAAI,SAAS,KAAM;GACnB,MAAM;GACN,MAAM,YAAY,kBAChB,MAAM,QAAQ,IACd;IACE,QAAQ,QAAQ,eAAe;IAC/B,YAAY;GACb,GACD,QACA,QACA,QACA,EAAE,MAAO,EACV;EACF;CACF;CAkBD,MAAM,oBACJC,SACAC,gBAIA;AACA,SAAO,KAAK,OAAO,KAAK,YAAY;GAClC,MAAM,gBAAgB,KAAK,kBAAkB,eAAe;AAC5D,OAAI;AAEF,QAAI,QAAQ,MAAM,QAAQ,SAAS,iBAAiB,CAAC,QAAQ,OAC3D,QAAO,MAAM,KAAK,OAAO,UAAU,MAAM,SAAS,cAAc;AAElE,WAAO,MAAM,KAAK,OAAO,UAAU,OAAO,SAAS,cAAc;GAClE,SAAQ,GAAG;IACV,MAAM,QAAQC,qCAAsB,EAAE;AACtC,UAAM;GACP;EACF,EAAC;CACH;;CAGD,AAAU,sCACRC,UACa;AACb,MAAI,SAAS,OAAO;GAElB,MAAM,QAAQ,IAAI,MAAM,SAAS,MAAM;GACvC,MAAM,OAAO,SAAS,MAAM;AAC5B,SAAM;EACP;EAED,IAAIC;EACJ,MAAMC,UAA0B,CAAE;EAClC,MAAMC,aAAyB,CAAE;EACjC,MAAMC,qBAAwC,CAAE;EAChD,MAAMC,oBAA6C;GACjD,gBAAgB;GAChB,OAAO,SAAS;GAChB,YAAY,SAAS;GACrB,IAAI,SAAS;GACb,oBAAoB,SAAS;GAC7B,UAAU,SAAS;GACnB,QAAQ,SAAS;GACjB,QAAQ,SAAS;GACjB,MAAM,SAAS;GACf,cAAc,SAAS;GAEvB,YAAY,SAAS;EACtB;EAED,MAAMC,oBAOF,CAAE;AAEN,OAAK,MAAM,QAAQ,SAAS,OAC1B,KAAI,KAAK,SAAS,WAAW;GAC3B,YAAY,KAAK;GACjB,QAAQ,KACN,GAAG,KAAK,QAAQ,QAAQ,CAAC,SAAS;AAChC,QAAI,KAAK,SAAS,eAAe;AAC/B,SAAI,YAAY,QAAQ,KAAK,UAAU,MACrC,kBAAkB,SAAS,KAAK;AAElC,YAAO;MACL,MAAM;MACN,MAAM,KAAK;MACX,aAAa,KAAK;KACnB;IACF;AAED,QAAI,KAAK,SAAS,WAAW;KAC3B,kBAAkB,UAAU,KAAK;AACjC,YAAO,CAAE;IACV;AAED,WAAO;GACR,EAAC,CACH;EACF,WAAU,KAAK,SAAS,iBAAiB;GACxC,MAAM,YAAY;IAChB,UAAU;KAAE,MAAM,KAAK;KAAM,WAAW,KAAK;IAAW;IACxD,IAAI,KAAK;GACV;AAED,OAAI;IACF,WAAW,qEAAmB,WAAW,EAAE,UAAU,KAAM,EAAC,CAAC;GAC9D,SAAQC,GAAY;IACnB,IAAIC;AACJ,QACE,OAAO,MAAM,YACb,KAAK,QACL,aAAa,KACb,OAAO,EAAE,YAAY,UAErB,aAAa,EAAE;IAEjB,mBAAmB,2EAAyB,WAAW,WAAW,CAAC;GACpE;GAED,kBAAkB,gCAAgC,CAAE;AACpD,OAAI,KAAK,IACP,kBAAkB,4BAA4B,KAAK,WAAW,KAAK;EAEtE,WAAU,KAAK,SAAS,aACvB,kBAAkB,YAAY;WACrB,KAAK,SAAS,oBAAoB;GAC3C,MAAM,SAASC,kCAAoB,KAAK;AACxC,OAAI,QACF,WAAW,KAAK,OAAO;QAEvB,mBAAmB,2EACG,MAAM,6BAA6B,CACxD;EAEJ,OAAM;GACL,kBAAkB,iBAAiB,CAAE;GACrC,kBAAkB,aAAa,KAAK,KAAK;EAC1C;AAGH,SAAO,IAAIC,oCAAU;GACnB,IAAI;GACJ;GACA;GACA;GACA,gBAAgBC,4DACd,SAAS,MACV;GACD;GACA;EACD;CACF;;CAGD,AAAU,yCACRC,OACA;EACA,MAAMC,UAAqC,CAAE;EAC7C,IAAIC,iBAA0C,CAAE;EAChD,IAAIC;EACJ,MAAMC,mBAAoC,CAAE;EAC5C,MAAMX,oBAA6C,EACjD,gBAAgB,SACjB;EACD,MAAMY,oBAIF,CAAE;EACN,IAAIC;AACJ,MAAI,MAAM,SAAS,8BACjB,QAAQ,KAAK;GACX,MAAM;GACN,MAAM,MAAM;GACZ,OAAO,MAAM;EACd,EAAC;WACO,MAAM,SAAS,yCACxB,QAAQ,KAAK;GACX,MAAM;GACN,MAAM;GACN,aAAa,CAAC,MAAM,UAAW;GAC/B,OAAO,MAAM;EACd,EAAC;WAEF,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,WAEpB,KAAK,MAAM,KAAK;WAEhB,MAAM,SAAS,gCACf,MAAM,KAAK,SAAS,iBACpB;GACA,iBAAiB,KAAK;IACpB,MAAM;IACN,MAAM,MAAM,KAAK;IACjB,MAAM,MAAM,KAAK;IACjB,IAAI,MAAM,KAAK;IACf,OAAO,MAAM;GACd,EAAC;GAEF,kBAAkB,8BAA8B,GAC7C,MAAM,KAAK,UAAU,MAAM,KAAK,GAClC;EACF,WACC,MAAM,SAAS,+BACf;GACE;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;EACD,EAAC,SAAS,MAAM,KAAK,KAAK,EAE3B,kBAAkB,eAAe,CAAC,MAAM,IAAK;WACpC,MAAM,SAAS,oBAAoB;GAC5C,kBAAkB,KAAK,MAAM,SAAS;GACtC,kBAAkB,aAAa,MAAM,SAAS;GAC9C,kBAAkB,QAAQ,MAAM,SAAS;EAC1C,WAAU,MAAM,SAAS,sBAAsB;GAC9C,MAAM,MAAM,KAAK,sCAAsC,MAAM,SAAS;GAEtE,iBAAiBP,4DACf,MAAM,SAAS,MAChB;AAED,OAAI,MAAM,SAAS,MAAM,QAAQ,SAAS,eACxC,kBAAkB,WAAW,KAAK,MAAM,IAAI,KAAK;AAEnD,QAAK,MAAM,CAAC,KAAK,MAAM,IAAI,OAAO,QAAQ,MAAM,SAAS,CACvD,KAAI,QAAQ,MAAM,kBAAkB,OAAO;EAE9C,WACC,MAAM,SAAS,4CACf,MAAM,SAAS,yCAEf,iBAAiB,KAAK;GACpB,MAAM;GACN,MAAM,MAAM;GACZ,OAAO,MAAM;EACd,EAAC;WAEF,MAAM,SAAS,wCACf,MAAM,SAAS,uCAEf,iBAAiB,EACf,cAAc;GACZ,IAAI,MAAM;GACV,MAAM,MAAM,KAAK,QAAQ,aAAa,GAAG,CAAC,QAAQ,cAAc,GAAG;GACnE,QAAQ;EACT,EACF;WACQ,MAAM,SAAS,yBACxB,kBAAkB,UAAU,MAAM;WAElC,MAAM,SAAS,gCACf,UAAU,SACV,MAAM,KAAK,SAAS,aACpB;GACA,MAAMQ,UAA6D,MAChE,KAAK,UACJ,MAAM,KAAK,QAAQ,IAAI,CAAC,GAAG,WAAW;IACpC,GAAG;IACH;GACD,GAAE,GACH;GAEJ,kBAAkB,YAAY;IAI5B,IAAI,MAAM,KAAK;IACf,MAAM,MAAM,KAAK;IACjB,GAAI,UAAU,EAAE,QAAS,IAAG,CAAE;GAC/B;EACF,WAAU,MAAM,SAAS,yCACxB,kBAAkB,YAAY;GAC5B,MAAM;GACN,SAAS,CAAC;IAAE,GAAG,MAAM;IAAM,OAAO,MAAM;GAAe,CAAC;EACzD;WACQ,MAAM,SAAS,yCACxB,kBAAkB,YAAY;GAC5B,MAAM;GACN,SAAS,CACP;IACE,MAAM,MAAM;IACZ,MAAM;IACN,OAAO,MAAM;GACd,CACF;EACF;WACQ,MAAM,SAAS,+CAGxB,QAAO;MAEP,QAAO;AAGT,SAAO,IAAIC,6CAAoB;GAE7B,MAAM,QAAQ,IAAI,CAAC,SAAS,KAAK,KAAK,CAAC,KAAK,GAAG;GAC/C,SAAS,IAAIC,yCAAe;IAC1B;IACS;IACT;IACA;IACA;IACA;GACD;GACD;EACD;CACF;;CAGD,AAAU,kCAAkCnD,UAAyB;AACnE,SAAO,SAAS,QACd,CAAC,UAAqD;GACpD,MAAM,mBAAmB,MAAM;AAG/B,OAAI,kBAAkB,mBAAmB,KACvC,QAAOoD,kDAAiC,MAAM;GAGhD,MAAM,oBAAoB,MAAM;GAQhC,IAAI,OAAOnD,iCAAoB,MAAM;AACrC,OAAI,SAAS,YAAYrB,8BAAiB,KAAK,MAAM,EACnD,OAAO;AAET,OAAI,SAAS,WACX,OAAM,IAAI,MACR;AAIJ,OAAI,SAAS,QAAQ;IACnB,MAAM,cAAc;AAGpB,QAAI,mBAAmB,SAAS,wBAAwB;KACtD,MAAM,UAAU,MAAM;AACpB,UAAI,OAAO,YAAY,YAAY,SACjC,QAAO;OACL,MAAM;OACN,WAAW,YAAY;MACxB;AAGH,UAAI,MAAM,QAAQ,YAAY,QAAQ,EAAE;OACtC,MAAM,gBAAgB,YAAY,QAAQ,KACxC,CAAC,MAAM,EAAE,SAAS,sBACnB;AAED,WAAI,cAAe,QAAO;OAE1B,MAAM,UAAU,YAAY,QAAQ,KAClC,CAAC,MAAM,EAAE,SAAS,YACnB;AAED,WAAI,QACF,QAAO;QACL,MAAM;QACN,WACE,OAAO,QAAQ,cAAc,WACzB,QAAQ,YACR,QAAQ,UAAU;OACzB;MAEJ;AAED,YAAM,IAAI,MAAM;KACjB,IAAG;AAEJ,YAAO;MACL,MAAM;MACN;MACA,SAAS,YAAY;KACtB;IACF;AAGD,QAAI,YAAY,mBAAmB,WACjC,QAAO;KACL,MAAM;KACN,SAAS,YAAY;KACrB,QAAQ,YAAY;IACrB;AAGH,WAAO;KACL,MAAM;KACN,SAAS,YAAY;KACrB,IAAI,YAAY,IAAI,WAAW,MAAM,GAAG,YAAY,KAAK;KACzD,QACE,OAAO,YAAY,YAAY,WAC3B,KAAK,UAAU,YAAY,QAAQ,GACnC,YAAY;IACnB;GACF;AAED,OAAI,SAAS,aAAa;AAExB,QACE,CAAC,KAAK,cACN,kBAAkB,UAAU,QAC5B,MAAM,QAAQ,kBAAkB,OAAO,IACvC,kBAAkB,OAAO,SAAS,KAClC,kBAAkB,OAAO,MAAM,CAAC,SAAS,UAAU,KAAK,CAExD,QAAO,kBAAkB;IAK3B,MAAMyE,QAA8B,CAAE;AAGtC,QAAI,mBAAmB,aAAa,CAAC,KAAK,YAAY;KACpD,MAAM,gBAAgB,KAAK,yBACzB,kBAAkB,UACnB;KACD,MAAM,KAAK,cAAc;IAC1B;IAGD,IAAI,EAAE,SAAS,GAAG;AAClB,QAAI,mBAAmB,SAAS;AAC9B,SAAI,OAAO,YAAY,UACrB,UAAU,CACR;MAAE,MAAM;MAAe,MAAM;MAAS,aAAa,CAAE;KAAE,CACxD;KAEH,UAAU,CACR,GAAG,SACH;MAAE,MAAM;MAAW,SAAS,kBAAkB;KAAS,CACxD;IACF;AAED,QAAI,OAAO,YAAY,YAAY,QAAQ,SAAS,GAClD,MAAM,KAAK;KACT,MAAM;KACN,MAAM;KACN,GAAI,MAAM,MAAM,CAAC,KAAK,cAAc,MAAM,GAAG,WAAW,OAAO,GAC3D,EAAE,IAAI,MAAM,GAAI,IAChB,CAAE;KACN,SAASC,kBAAK,MAAM;AAClB,UAAI,OAAO,YAAY,SACrB,QAAO;AAET,aAAO,QAAQ,QAAQ,CAAC,SAAS;AAC/B,WAAI,KAAK,SAAS,OAChB,QAAO;QACL,MAAM;QACN,MAAM,KAAK;QACX,aAAa,KAAK,eAAe,CAAE;OACpC;AAGH,WAAI,KAAK,SAAS,iBAAiB,KAAK,SAAS,UAC/C,QAAO;AAGT,cAAO,CAAE;MACV,EAAC;KACH,EAAC;IACH,EAAC;IAGJ,MAAM,kBACJ,oBAAoB;AAEtB,mDAAgB,MAAM,IAAI,CAAC,CAAC,MAAM,YAAY,QAC5C,MAAM,KACJ,GAAG,MAAM,WAAW,IAAI,CAAC,aAAiC;AACxD,SAAIC,+BAAiB,SAAS,CAC5B,QAAO;MACL,MAAM;MACN,IAAI,SAAS;MACb,SAAS,SAAS,MAAM;MACxB,OAAO,SAAS,KAAK;MACrB,MAAM,SAAS;KAChB;AAEH,YAAO;MACL,MAAM;MACN,MAAM,SAAS;MACf,WAAW,KAAK,UAAU,SAAS,KAAK;MACxC,SAAS,SAAS;MAClB,GAAI,KAAK,aACL,EAAE,IAAI,kBAAkB,SAAS,IAAM,IACvC,CAAE;KACP;IACF,EAAC,CACH;aACQ,mBAAmB,YAC5B,MAAM,KACJ,GAAG,kBAAkB,WAAW,IAC9B,CAAC,cAAkC;KACjC,MAAM;KACN,MAAM,SAAS,SAAS;KACxB,SAAS,SAAS;KAClB,WAAW,SAAS,SAAS;KAC7B,GAAI,KAAK,aACL,EAAE,IAAI,kBAAkB,SAAS,IAAK,IACtC,CAAE;IACP,GACF,CACF;IAGH,MAAM,eACJ,kBAAkB,SACjB,SACC,kBAAkB,SAClB,kBAAkB;IAEtB,MAAMC,uBAAqD;KACzD;KACA;KACA;KACA;IACD;AAED,QAAI,eAAe,MAAM;KACvB,MAAM,kBAAkB;KACxB,MAAM,mBAAmB,iBAAiB,OAAO,CAAC,SAChD,qBAAqB,SAAS,KAAK,KAAK,CACzC;AACD,SAAI,iBAAiB,SAAS,GAAG,MAAM,KAAK,GAAG,iBAAiB;IACjE;AAED,WAAO;GACR;AAED,OAAI,SAAS,UAAU,SAAS,YAAY,SAAS,aAAa;AAChE,QAAI,OAAO,MAAM,YAAY,SAC3B,QAAO;KAAE,MAAM;KAAW;KAAM,SAAS,MAAM;IAAS;IAG1D,MAAMC,aAAiC,CAAE;IACzC,MAAM,UAAU,MAAM,QAAQ,QAAQ,CAAC,SAAS;AAC9C,SAAI,KAAK,SAAS,yBAChBC,WAAS,KAAK;MACZ,MAAM;MACN,qBAAqB,KAAK;MAC1B,SAAS,KAAK;KACf,EAAC;AAEJ,2DAAuB,KAAK,CAC1B,qEACE,MACAC,2DACD;AAEH,SAAI,KAAK,SAAS,OAChB,QAAO;MACL,MAAM;MACN,MAAM,KAAK;KACZ;AAEH,SAAI,KAAK,SAAS,aAAa;MAC7B,MAAM,WAAWL,kBAAK,MAAM;AAC1B,WAAI,OAAO,KAAK,cAAc,SAC5B,QAAO,KAAK;gBAEZ,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,SAAS,KAAK,UAEd,QAAO,KAAK,UAAU;AAExB,cAAO;MACR,EAAC;MACF,MAAM,SAASA,kBAAK,MAAM;AACxB,WAAI,OAAO,KAAK,cAAc,SAC5B,QAAO;gBAEP,OAAO,KAAK,cAAc,YAC1B,KAAK,cAAc,QACnB,YAAY,KAAK,UAEjB,QAAO,KAAK,UAAU;AAExB,cAAO;MACR,EAAC;AACF,aAAO;OACL,MAAM;OACN,WAAW;OACX;MACD;KACF;AACD,SACE,KAAK,SAAS,gBACd,KAAK,SAAS,iBACd,KAAK,SAAS,aAEd,QAAO;AAET,YAAO,CAAE;IACV,EAAC;AAEF,QAAI,QAAQ,SAAS,GACnBI,WAAS,KAAK;KACZ,MAAM;KACN;KACS;IACV,EAAC;AAEJ,WAAOA;GACR;GAED,QAAQ,KACN,CAAC,gEAAgE,EAAE,MAAM,CAC1E;AACD,UAAO,CAAE;EACV,EACF;CACF;;CAGD,AAAU,yBACRE,WAC8C;EAE9C,MAAM,WACJ,UAAU,QAAQ,SAAS,IACvB,UAAU,QAAQ,OAChB,CAAC,KAAK,SAAS;GACb,MAAM,OAAO,IAAI,GAAG,GAAG;AAEvB,OAAI,KAAM,UAAU,KAAK,OACvB,KAAM,QAAQ,KAAK;QAEnB,IAAI,KAAK,KAAK;AAEhB,UAAO;EACR,GACD,CAAC,EAAE,GAAG,UAAU,QAAQ,GAAI,CAAC,EAC9B,GACD,UAAU,SACd,IAAI,CAAC,MACL,OAAO,YAAY,OAAO,QAAQ,EAAE,CAAC,OAAO,CAAC,CAAC,EAAE,KAAK,MAAM,QAAQ,CAAC,CACrE;AAED,SAAO;GACL,GAAG;GACH;EACD;CACF;;CAGD,AAAU,uBACRlE,OACAmE,QACiB;EACjB,MAAMC,eAAgC,CAAE;AACxC,OAAK,MAAM,QAAQ,MACjB,KAAIjE,4BAAc,KAAK,EAAE;AACvB,OAAI,KAAK,SAAS,sBAAsB,QAAQ,QAG9C,KAAK,iBAAiB;GAExB,aAAa,KAAK,KAAK;EACxB,mEAA+B,KAAK,EACnC,aAAa,KAAK;GAChB,MAAM;GACN,MAAM,KAAK,SAAS;GACpB,YAAY,KAAK,SAAS;GAC1B,aAAa,KAAK,SAAS;GAC3B,QAAQ,QAAQ,UAAU;EAC3B,EAAC;WACOkE,iCAAmB,KAAK,EACjC,aAAa,KAAKC,2CAA6B,KAAK,CAAC;AAGzD,SAAO;CACR;AACF;;;;;AAcD,IAAa,wBAAb,cAEU,eAA4B;;CAEpC,AAAS,iBACPrF,SACAsF,OACiC;EACjC,IAAIrE;AACJ,MAAI,SAAS,WAAW,QACtB,SAAS,QAAQ;WACR,KAAK,8BAA8B,QAC5C,SAAS,KAAK;EAGhB,IAAI,sBAAsB,CAAE;AAC5B,MAAI,SAAS,mBAAmB,QAC9B,sBAAsB,EAAE,gBAAgB,QAAQ,eAAgB;WACvD,KAAK,gBAAgB,KAAK,aAAa,OAAO,YACvD,sBAAsB,EAAE,gBAAgB,EAAE,eAAe,KAAM,EAAE;EAGnE,MAAMsE,SAAmD;GACvD,OAAO,KAAK;GACZ,aAAa,KAAK;GAClB,OAAO,KAAK;GACZ,mBAAmB,KAAK;GACxB,kBAAkB,KAAK;GACvB,UAAU,KAAK;GACf,cAAc,KAAK;GACnB,GAAG,KAAK;GACR,YAAY,KAAK;GACjB,MAAM,SAAS,QAAQ,KAAK;GAC5B,MAAM,KAAK;GAEX,QAAQ,KAAK;GACb,WAAW,SAAS;GACpB,eAAe,SAAS;GACxB,OAAO,SAAS,OAAO,SACnB,QAAQ,MAAM,IAAI,CAAC,SACjB,KAAK,wCAAwC,MAAM,EAAE,OAAQ,EAAC,CAC/D,GACD;GACJ,aAAa1C,uCACX,SAAS,YACV;GACD,iBAAiB,KAAK,mBAAmB,SAAS,gBAAgB;GAClE,MAAM,SAAS;GACf,GAAG;GACH,qBAAqB,SAAS;GAC9B,GAAI,KAAK,SAAS,SAAS,QACvB,EAAE,OAAO,KAAK,SAAS,SAAS,MAAO,IACvC,CAAE;GACN,GAAI,KAAK,cAAc,SAAS,aAC5B,EAAE,YAAY,KAAK,cAAc,SAAS,WAAY,IACtD,CAAE;GACN,GAAG,KAAK;GACR,kBAAkB,SAAS,kBAAkB,KAAK;GAClD,WAAW,SAAS,aAAa,KAAK;EACvC;AACD,MAAI,SAAS,eAAe,QAC1B,OAAO,aAAa,QAAQ;AAE9B,MAAI,KAAK,iBAAiB,QACxB,OAAO,eAAe,KAAK;AAE7B,MAAI,SAAS,iBAAiB,QAC5B,OAAO,eAAe,QAAQ;EAEhC,MAAM,YAAY,KAAK,oBAAoB,QAAQ;AACnD,MAAI,cAAc,UAAa,UAAU,WAAW,QAClD,OAAO,mBAAmB,UAAU;AAEtC,MAAI5C,8BAAiB,OAAO,MAAM,EAChC,OAAO,wBACL,KAAK,cAAc,KAAK,SAAY,KAAK;OAE3C,OAAO,aAAa,KAAK,cAAc,KAAK,SAAY,KAAK;AAG/D,SAAO;CACR;CAED,MAAM,UACJoB,UACAvB,SACAiD,YACqB;EACrB,MAAM,gBAAgB,CAAE;EACxB,MAAM,SAAS,KAAK,iBAAiB,QAAQ;EAC7C,MAAMyC,iBACJC,sDAA+B,UAAU,KAAK,MAAM;AAEtD,MAAI,OAAO,QAAQ;GACjB,MAAM,SAAS,KAAK,sBAAsB,UAAU,SAAS,WAAW;GACxE,MAAMC,cAAmD,CAAE;AAC3D,cAAW,MAAM,SAAS,QAAQ;IAChC,MAAM,QAAQ,oBAAoB;KAChC,GAAG,MAAM;KACT,GAAG,MAAM,QAAQ;IAClB;IACD,MAAM,QACH,MAAM,gBAAoC,cAAc;AAC3D,QAAI,YAAY,WAAW,QACzB,YAAY,SAAS;SAErB,YAAY,SAAS,YAAY,OAAO,OAAO,MAAM;GAExD;GACD,MAAM,cAAc,OAAO,QAAQ,YAAY,CAC5C,KAAK,CAAC,CAAC,KAAK,EAAE,CAAC,KAAK,KAAK,SAAS,MAAM,GAAG,GAAG,SAAS,MAAM,GAAG,CAAC,CACjE,IAAI,CAAC,CAAC,GAAG,MAAM,KAAK,MAAM;GAE7B,MAAM,EAAE,WAAW,eAAe,GAAG,KAAK,iBAAiB,QAAQ;GAKnE,MAAM,mBAAmB,MAAM,KAAK,kCAClC,UACA,WACA,cACD;GACD,MAAM,uBAAuB,MAAM,KAAK,6BACtC,YACD;GAED,cAAc,eAAe;GAC7B,cAAc,gBAAgB;GAC9B,cAAc,eAAe,mBAAmB;AAChD,UAAO;IACL;IACA,WAAW,EACT,qBAAqB;KACnB,cAAc,cAAc;KAC5B,kBAAkB,cAAc;KAChC,aAAa,cAAc;IAC5B,EACF;GACF;EACF,OAAM;GACL,MAAM,OAAO,MAAM,KAAK,oBACtB;IACE,GAAG;IACH,QAAQ;IACR,UAAU;GACX,GACD;IACE,QAAQ,SAAS;IACjB,GAAG,SAAS;GACb,EACF;GAED,MAAM,EACJ,mBAAmB,kBACnB,eAAe,cACf,cAAc,aACd,uBAAuB,qBACvB,2BAA2B,yBAC5B,GAAG,MAAM,SAAS,CAAE;AAErB,OAAI,kBACF,cAAc,iBACX,cAAc,iBAAiB,KAAK;AAGzC,OAAI,cACF,cAAc,gBACX,cAAc,gBAAgB,KAAK;AAGxC,OAAI,aACF,cAAc,gBACX,cAAc,gBAAgB,KAAK;AAGxC,OACE,qBAAqB,iBAAiB,QACtC,qBAAqB,kBAAkB,MAEvC,cAAc,sBAAsB;IAClC,GAAI,qBAAqB,iBAAiB,QAAQ,EAChD,OAAO,qBAAqB,aAC7B;IACD,GAAI,qBAAqB,kBAAkB,QAAQ,EACjD,YAAY,qBAAqB,cAClC;GACF;AAGH,OACE,yBAAyB,iBAAiB,QAC1C,yBAAyB,qBAAqB,MAE9C,cAAc,uBAAuB;IACnC,GAAI,yBAAyB,iBAAiB,QAAQ,EACpD,OAAO,yBAAyB,aACjC;IACD,GAAI,yBAAyB,qBAAqB,QAAQ,EACxD,WAAW,yBAAyB,iBACrC;GACF;GAGH,MAAMnE,cAAgC,CAAE;AACxC,QAAK,MAAM,QAAQ,MAAM,WAAW,CAAE,GAAE;IACtC,MAAM,OAAO,KAAK,SAAS,WAAW;IACtC,MAAMoE,aAA6B;KACjC;KACA,SAAS,KAAK,wCACZ,KAAK,WAAW,EAAE,MAAM,YAAa,GACrC,KACD;IACF;IACD,WAAW,iBAAiB;KAC1B,GAAI,KAAK,gBAAgB,EAAE,eAAe,KAAK,cAAe,IAAG,CAAE;KACnE,GAAI,KAAK,WAAW,EAAE,UAAU,KAAK,SAAU,IAAG,CAAE;IACrD;AACD,mDAAgB,WAAW,QAAQ,EACjC,WAAW,QAAQ,iBAAiB;IAItC,WAAW,UAAU,IAAI9B,oCACvB,OAAO,YACL,OAAO,QAAQ,WAAW,QAAQ,CAAC,OACjC,CAAC,CAAC,IAAI,KAAK,CAAC,IAAI,WAAW,MAAM,CAClC,CACF;IAEH,YAAY,KAAK,WAAW;GAC7B;AACD,UAAO;IACL;IACA,WAAW,EACT,YAAY;KACV,cAAc,cAAc;KAC5B,kBAAkB,cAAc;KAChC,aAAa,cAAc;IAC5B,EACF;GACF;EACF;CACF;CAED,OAAO,sBACLxC,UACAvB,SACAiD,YACqC;EACrC,MAAMyC,iBACJC,sDAA+B,UAAU,KAAK,MAAM;EAEtD,MAAM,SAAS;GACb,GAAG,KAAK,iBAAiB,SAAS,EAChC,WAAW,KACZ,EAAC;GACF,UAAU;GACV,QAAQ;EACT;EACD,IAAIG;EAEJ,MAAM,iBAAiB,MAAM,KAAK,oBAAoB,QAAQ,QAAQ;EACtE,IAAIC;AACJ,aAAW,MAAM,QAAQ,gBAAgB;GACvC,MAAM,SAAS,MAAM,UAAU;AAC/B,OAAI,KAAK,OACP,QAAQ,KAAK;AAEf,OAAI,CAAC,OACH;GAGF,MAAM,EAAE,OAAO,GAAG;AAClB,OAAI,CAAC,MACH;GAEF,MAAM,QAAQ,KAAK,2CACjB,OACA,MACA,YACD;GACD,cAAc,MAAM,QAAQ;GAC5B,MAAM,kBAAkB;IACtB,QAAQ,QAAQ,eAAe;IAC/B,YAAY,OAAO,SAAS;GAC7B;AACD,OAAI,OAAO,MAAM,YAAY,UAAU;IACrC,QAAQ,IACN,uFACD;AACD;GACD;GAED,MAAMC,iBAAsC,EAAE,GAAG,gBAAiB;AAClE,OAAI,OAAO,iBAAiB,MAAM;IAChC,eAAe,gBAAgB,OAAO;IAGtC,eAAe,qBAAqB,KAAK;IACzC,eAAe,aAAa,KAAK;IACjC,eAAe,eAAe,KAAK;GACpC;AACD,OAAI,KAAK,UACP,eAAe,WAAW,OAAO;GAEnC,MAAM,kBAAkB,IAAIvB,6CAAoB;IAC9C,SAAS;IACT,MAAM,MAAM;IACZ;GACD;GACD,MAAM;GACN,MAAM,YAAY,kBAChB,gBAAgB,QAAQ,IACxB,iBACA,QACA,QACA,QACA,EAAE,OAAO,gBAAiB,EAC3B;EACF;AACD,MAAI,OAAO;GACT,MAAM,oBAAoB;IACxB,GAAI,MAAM,uBAAuB,iBAAiB,QAAQ,EACxD,OAAO,MAAM,uBAAuB,aACrC;IACD,GAAI,MAAM,uBAAuB,kBAAkB,QAAQ,EACzD,YAAY,MAAM,uBAAuB,cAC1C;GACF;GACD,MAAM,qBAAqB;IACzB,GAAI,MAAM,2BAA2B,iBAAiB,QAAQ,EAC5D,OAAO,MAAM,2BAA2B,aACzC;IACD,GAAI,MAAM,2BAA2B,qBAAqB,QAAQ,EAChE,WAAW,MAAM,2BAA2B,iBAC7C;GACF;GACD,MAAM,kBAAkB,IAAIA,6CAAoB;IAC9C,SAAS,IAAIC,yCAAe;KAC1B,SAAS;KACT,mBAAmB,EACjB,OAAO,EAAE,GAAG,MAAO,EACpB;KACD,gBAAgB;MACd,cAAc,MAAM;MACpB,eAAe,MAAM;MACrB,cAAc,MAAM;MACpB,GAAI,OAAO,KAAK,kBAAkB,CAAC,SAAS,KAAK,EAC/C,qBAAqB,kBACtB;MACD,GAAI,OAAO,KAAK,mBAAmB,CAAC,SAAS,KAAK,EAChD,sBAAsB,mBACvB;KACF;IACF;IACD,MAAM;GACP;GACD,MAAM;EACP;AACD,MAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM;CAEnB;CAYD,MAAM,oBACJuB,SACA9C,gBAIA;EACA,MAAM,gBAAgB,KAAK,kBAAkB,eAAe;EAC5D,MAAM,oBACJ,QAAQ,mBAAmB,QAAQ,gBAAgB,SAAS;AAC9D,SAAO,KAAK,OAAO,KAAK,YAAY;AAClC,OAAI;AACF,QAAI,qBAAqB,CAAC,QAAQ,OAChC,QAAO,MAAM,KAAK,OAAO,KAAK,YAAY,MACxC,SACA,cACD;QAED,QAAO,MAAM,KAAK,OAAO,KAAK,YAAY,OACxC,SACA,cACD;GAEJ,SAAQ,GAAG;IACV,MAAM,QAAQC,qCAAsB,EAAE;AACtC,UAAM;GACP;EACF,EAAC;CACH;;CAGD,AAAU,wCACR8C,SACAC,aACa;EACb,MAAMC,eAA6C,QAAQ;AAG3D,UAAQ,QAAQ,MAAhB;GACE,KAAK,aAAa;IAChB,MAAM,YAAY,CAAE;IACpB,MAAM,mBAAmB,CAAE;AAC3B,SAAK,MAAM,eAAe,gBAAgB,CAAE,EAC1C,KAAI;KACF,UAAU,qEAAmB,aAAa,EAAE,UAAU,KAAM,EAAC,CAAC;IAE/D,SAAQC,GAAQ;KACf,iBAAiB,2EAAyB,aAAa,EAAE,QAAQ,CAAC;IACnE;IAEH,MAAMC,oBAA6C;KACjD,eAAe,QAAQ;KACvB,YAAY;IACb;AACD,QAAI,KAAK,yBAAyB,QAChC,kBAAkB,iBAAiB;IAErC,MAAMC,oBAAyD;KAC7D,gBAAgB;KAChB,YAAY,YAAY;KACxB,GAAI,YAAY,qBACZ;MACE,OAAO,EAAE,GAAG,YAAY,MAAO;MAC/B,oBAAoB,YAAY;KACjC,IACD,CAAE;IACP;AAED,QAAI,QAAQ,OACV,kBAAkB,QAAQ,QAAQ;IAGpC,MAAM,UAAUC,sCACd,QAAQ,WAAW,IACnB,YAAY,UAAU,IAAI,QAC3B;AACD,WAAO,IAAIzC,oCAAU;KACnB;KACA,YAAY;KACZ,oBAAoB;KACpB;KACA;KACA,IAAI,YAAY;IACjB;GACF;GACD,QACE,QAAO,IAAI0C,sCACT,QAAQ,WAAW,IACnB,QAAQ,QAAQ;EAErB;CACF;;CAGD,AAAU,2CAERC,OACAC,aACAC,aACA;EACA,MAAM,OAAO,MAAM,QAAQ;EAC3B,MAAM,UAAU,MAAM,WAAW;EACjC,IAAIN;AACJ,MAAI,MAAM,eACR,oBAAoB,EAClB,eAAe,MAAM,cACtB;WACQ,MAAM,YACf,oBAAoB,EAClB,YAAY,MAAM,WACnB;OAED,oBAAoB,CAAE;AAExB,MAAI,KAAK,sBACP,kBAAkB,iBAAiB;AAGrC,MAAI,MAAM,OACR,kBAAkB,QAAQ;GACxB,GAAG,MAAM;GACT,OAAO,YAAY,QAAQ,GAAG;EAC/B;EAGH,MAAM,oBAAoB;GACxB,gBAAgB;GAChB,OAAO,EAAE,GAAG,YAAY,MAAO;EAChC;AACD,MAAI,SAAS,OACX,QAAO,IAAIO,4CAAkB;GAAE;GAAS;EAAmB;WAClD,SAAS,aAAa;GAC/B,MAAMC,iBAAkC,CAAE;AAC1C,OAAI,MAAM,QAAQ,MAAM,WAAW,CACjC,MAAK,MAAM,eAAe,MAAM,YAC9B,eAAe,KAAK;IAClB,MAAM,YAAY,UAAU;IAC5B,MAAM,YAAY,UAAU;IAC5B,IAAI,YAAY;IAChB,OAAO,YAAY;IACnB,MAAM;GACP,EAAC;AAGN,UAAO,IAAIpC,yCAAe;IACxB;IACA,kBAAkB;IAClB;IACA,IAAI,YAAY;IAChB;GACD;EACF,WAAU,SAAS,SAClB,QAAO,IAAIqC,6CAAmB;GAAE;GAAS;EAAmB;WACnD,SAAS,YAClB,QAAO,IAAIA,6CAAmB;GAC5B;GACA;GACA,mBAAmB,EACjB,iBAAiB,YAClB;EACF;WACQ,SAAS,WAClB,QAAO,IAAIC,+CAAqB;GAC9B;GACA;GACA,MAAM,MAAM;GACZ;EACD;WACQ,SAAS,OAClB,QAAO,IAAIC,2CAAiB;GAC1B;GACA;GACA,cAAc,MAAM;GACpB;EACD;MAED,QAAO,IAAIC,2CAAiB;GAAE;GAAS;GAAM;EAAmB;CAEnE;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAmjBD,IAAa,aAAb,MAAa,mBAEH,eAA4B;;;;;CAKpC,kBAAkB;CAElB,AAAU;CAEV,AAAU;CAEV,IAAI,uBAAiC;AACnC,SAAO,CAAC,GAAG,MAAM,sBAAsB,iBAAkB;CAC1D;CAED,YAAsBC,QAA2B;EAC/C,MAAM,OAAO;EADO;EAEpB,KAAK,kBAAkB,QAAQ,mBAAmB;EAClD,KAAK,YAAY,QAAQ,aAAa,IAAI,oBAAoB;EAC9D,KAAK,cAAc,QAAQ,eAAe,IAAI,sBAAsB;CACrE;CAED,AAAU,iBAAiBC,SAAgD;EACzE,MAAM,mBAAmB,SAAS,OAAO,KAAKhG,4BAAc;EAC5D,MAAM,yBACJ,SAAS,wBAAwB,QACjC,SAAS,QAAQ,QACjB,SAAS,cAAc,QACvB,SAAS,WAAW,QACpB,SAAS,WAAW,WAAW,QAC/B,KAAK,WAAW,WAAW;EAC7B,MAAM,iBAAiB,SAAS,OAAO,KAAKkE,iCAAmB;AAE/D,SACE,KAAK,mBACL,oBACA,0BACA;CAEH;CAED,AAAS,YAAYtF,SAAoC;EACvD,MAAM,sBAAsB,KAAK,oBAAoB,QAAQ;AAC7D,MAAI,KAAK,iBAAiB,QAAQ,CAChC,QAAO,KAAK,UAAU,YAAY,oBAAoB;AAExD,SAAO,KAAK,YAAY,YAAY,oBAAoB;CACzD;CAED,AAAS,iBAAiBE,SAAqC;EAC7D,MAAM,sBAAsB,KAAK,oBAAoB,QAAQ;AAC7D,MAAI,KAAK,iBAAiB,QAAQ,CAChC,QAAO,KAAK,UAAU,iBAAiB,oBAAoB;AAE7D,SAAO,KAAK,YAAY,iBAAiB,oBAAoB;CAC9D;;CAGD,MAAe,UACbqB,UACAvB,SACAiD,YACqB;AACrB,MAAI,KAAK,iBAAiB,QAAQ,CAChC,QAAO,KAAK,UAAU,UAAU,UAAU,QAAQ;AAEpD,SAAO,KAAK,YAAY,UAAU,UAAU,SAAS,WAAW;CACjE;CAED,OAAgB,sBACd1B,UACAvB,SACAiD,YACqC;AACrC,MAAI,KAAK,iBAAiB,QAAQ,EAAE;GAClC,OAAO,KAAK,UAAU,sBACpB,UACA,KAAK,oBAAoB,QAAQ,EACjC,WACD;AACD;EACD;EACD,OAAO,KAAK,YAAY,sBACtB,UACA,KAAK,oBAAoB,QAAQ,EACjC,WACD;CACF;CAED,AAAS,WACPoE,QAC+D;EAC/D,MAAM,WAAW,IAAI,WAAwB,KAAK;EAClD,SAAS,iBAAiB;GAAE,GAAG,KAAK;GAAgB,GAAG;EAAQ;AAC/D,SAAO;CACR;AACF"}
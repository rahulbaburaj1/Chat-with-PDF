{"version":3,"file":"chat_models.d.ts","names":["_________langchain_core_dist_utils_stream_js0","ClientOptions","OpenAI","OpenAIClient","CallbackManagerForLLMRun","AIMessageChunk","BaseMessage","ChatMessageChunk","FunctionMessageChunk","HumanMessageChunk","SystemMessageChunk","ToolMessageChunk","ChatGenerationChunk","ChatGeneration","ChatResult","BaseChatModel","LangSmithParams","BaseChatModelParams","BaseFunctionCallOptions","BaseLanguageModelInput","StructuredOutputMethodOptions","Runnable","InteropZodType","OpenAICallOptions","OpenAIChatInput","OpenAICoreRequestOptions","ChatOpenAIResponseFormat","ChatOpenAIReasoningSummary","ResponseFormatConfiguration","OpenAIVerbosityParam","OpenAIToolChoice","ChatOpenAIToolType","ResponsesTool","ResponsesToolChoice","OpenAILLMOutput","BaseChatOpenAICallOptions","Chat","ChatCompletionStreamOptions","ChatCompletionModality","Array","ChatCompletionAudioParam","ChatCompletionPredictionContent","Reasoning","ChatCompletionCreateParams","BaseChatOpenAIFields","Partial","BaseChatOpenAI","CallOptions","Record","Omit","ChatCompletionTool","_langchain_core_messages0","MessageStructure","IterableReadableStream","Promise","Function","ChatCompletionFunctionCallOption","RunOutput","ChatOpenAIResponsesCallOptions","Responses","ResponseCreateParams","ChatResponsesInvocationParams","ExcludeController","T","ResponsesCreate","ResponsesParse","ResponsesCreateInvoke","ReturnType","Awaited","ResponsesParseInvoke","ChatOpenAIResponses","AsyncGenerator","ResponseCreateParamsStreaming","RequestOptions","ResponseStreamEvent","AsyncIterable","ResponseCreateParamsNonStreaming","Response","ResponseReasoningItem","ChatOpenAICompletionsCallOptions","ChatCompletionsInvocationParams","Completions","ChatOpenAICompletions","ChatCompletionCreateParamsStreaming","ChatCompletionChunk","ChatCompletionCreateParamsNonStreaming","ChatCompletion","ChatCompletionMessage","ChatCompletionRole","ChatOpenAICallOptions","ChatOpenAIFields","ChatOpenAI"],"sources":["../src/chat_models.d.ts"],"sourcesContent":["import { type ClientOptions, OpenAI as OpenAIClient } from \"openai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessageChunk, type BaseMessage, ChatMessageChunk, FunctionMessageChunk, HumanMessageChunk, SystemMessageChunk, ToolMessageChunk } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, type ChatGeneration, type ChatResult } from \"@langchain/core/outputs\";\nimport { BaseChatModel, type LangSmithParams, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { type BaseFunctionCallOptions, type BaseLanguageModelInput, type StructuredOutputMethodOptions } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { InteropZodType } from \"@langchain/core/utils/types\";\nimport { type OpenAICallOptions, type OpenAIChatInput, type OpenAICoreRequestOptions, type ChatOpenAIResponseFormat, ChatOpenAIReasoningSummary, ResponseFormatConfiguration, OpenAIVerbosityParam } from \"./types.js\";\nimport { OpenAIToolChoice, ChatOpenAIToolType, ResponsesTool, ResponsesToolChoice } from \"./utils/tools.js\";\ninterface OpenAILLMOutput {\n    tokenUsage: {\n        completionTokens?: number;\n        promptTokens?: number;\n        totalTokens?: number;\n    };\n}\nexport type { OpenAICallOptions, OpenAIChatInput };\nexport interface BaseChatOpenAICallOptions extends OpenAICallOptions, BaseFunctionCallOptions {\n    /**\n     * A list of tools that the model may use to generate responses.\n     * Each tool can be a function, a built-in tool, or a custom tool definition.\n     * If not provided, the model will not use any tools.\n     */\n    tools?: ChatOpenAIToolType[];\n    /**\n     * Specifies which tool the model should use to respond.\n     * Can be an {@link OpenAIToolChoice} or a {@link ResponsesToolChoice}.\n     * If not set, the model will decide which tool to use automatically.\n     */\n    // TODO: break OpenAIToolChoice and ResponsesToolChoice into options sub classes\n    tool_choice?: OpenAIToolChoice | ResponsesToolChoice;\n    /**\n     * Adds a prompt index to prompts passed to the model to track\n     * what prompt is being used for a given generation.\n     */\n    promptIndex?: number;\n    /**\n     * An object specifying the format that the model must output.\n     */\n    response_format?: ChatOpenAIResponseFormat;\n    /**\n     * When provided, the completions API will make a best effort to sample\n     * deterministically, such that repeated requests with the same `seed`\n     * and parameters should return the same result.\n     */\n    seed?: number;\n    /**\n     * Additional options to pass to streamed completions.\n     * If provided, this takes precedence over \"streamUsage\" set at\n     * initialization time.\n     */\n    stream_options?: OpenAIClient.Chat.ChatCompletionStreamOptions;\n    /**\n     * The model may choose to call multiple functions in a single turn. You can\n     * set parallel_tool_calls to false which ensures only one tool is called at most.\n     * [Learn more](https://platform.openai.com/docs/guides/function-calling#parallel-function-calling)\n     */\n    parallel_tool_calls?: boolean;\n    /**\n     * If `true`, model output is guaranteed to exactly match the JSON Schema\n     * provided in the tool definition. If `true`, the input schema will also be\n     * validated according to\n     * https://platform.openai.com/docs/guides/structured-outputs/supported-schemas.\n     *\n     * If `false`, input schema will not be validated and model output will not\n     * be validated.\n     *\n     * If `undefined`, `strict` argument will not be passed to the model.\n     */\n    strict?: boolean;\n    /**\n     * Output types that you would like the model to generate for this request. Most\n     * models are capable of generating text, which is the default:\n     *\n     * `[\"text\"]`\n     *\n     * The `gpt-4o-audio-preview` model can also be used to\n     * [generate audio](https://platform.openai.com/docs/guides/audio). To request that\n     * this model generate both text and audio responses, you can use:\n     *\n     * `[\"text\", \"audio\"]`\n     */\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    /**\n     * Parameters for audio output. Required when audio output is requested with\n     * `modalities: [\"audio\"]`.\n     * [Learn more](https://platform.openai.com/docs/guides/audio).\n     */\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    /**\n     * Static predicted output content, such as the content of a text file that is being regenerated.\n     * [Learn more](https://platform.openai.com/docs/guides/latency-optimization#use-predicted-outputs).\n     */\n    prediction?: OpenAIClient.ChatCompletionPredictionContent;\n    /**\n     * Options for reasoning models.\n     *\n     * Note that some options, like reasoning summaries, are only available when using the responses\n     * API. If these options are set, the responses API will be used to fulfill the request.\n     *\n     * These options will be ignored when not using a reasoning model.\n     */\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\"\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates. Replaces the `user` field.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey?: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\nexport interface BaseChatOpenAIFields extends Partial<OpenAIChatInput>, BaseChatModelParams {\n    /**\n     * Optional configuration options for the OpenAI client.\n     */\n    configuration?: ClientOptions;\n}\n/** @internal */\nexport declare abstract class BaseChatOpenAI<CallOptions extends BaseChatOpenAICallOptions> extends BaseChatModel<CallOptions, AIMessageChunk> implements Partial<OpenAIChatInput> {\n    temperature?: number;\n    topP?: number;\n    frequencyPenalty?: number;\n    presencePenalty?: number;\n    n?: number;\n    logitBias?: Record<string, number>;\n    model: string;\n    modelKwargs?: OpenAIChatInput[\"modelKwargs\"];\n    stop?: string[];\n    stopSequences?: string[];\n    user?: string;\n    timeout?: number;\n    streaming: boolean;\n    streamUsage: boolean;\n    maxTokens?: number;\n    logprobs?: boolean;\n    topLogprobs?: number;\n    apiKey?: string;\n    organization?: string;\n    __includeRawResponse?: boolean;\n    /** @internal */\n    client: OpenAIClient;\n    /** @internal */\n    clientConfig: ClientOptions;\n    /**\n     * Whether the model supports the `strict` argument when passing in tools.\n     * If `undefined` the `strict` argument will not be passed to OpenAI.\n     */\n    supportsStrictToolCalling?: boolean;\n    audio?: OpenAIClient.Chat.ChatCompletionAudioParam;\n    modalities?: Array<OpenAIClient.Chat.ChatCompletionModality>;\n    reasoning?: OpenAIClient.Reasoning;\n    /**\n     * Must be set to `true` in tenancies with Zero Data Retention. Setting to `true` will disable\n     * output storage in the Responses API, but this DOES NOT enable Zero Data Retention in your\n     * OpenAI organization or project. This must be configured directly with OpenAI.\n     *\n     * See:\n     * https://platform.openai.com/docs/guides/your-data\n     * https://platform.openai.com/docs/api-reference/responses/create#responses-create-store\n     *\n     * @default false\n     */\n    zdrEnabled?: boolean | undefined;\n    /**\n     * Service tier to use for this request. Can be \"auto\", \"default\", or \"flex\" or \"priority\".\n     * Specifies the service tier for prioritization and latency optimization.\n     */\n    service_tier?: OpenAIClient.Chat.ChatCompletionCreateParams[\"service_tier\"];\n    /**\n     * Used by OpenAI to cache responses for similar requests to optimize your cache\n     * hit rates.\n     * [Learn more](https://platform.openai.com/docs/guides/prompt-caching).\n     */\n    promptCacheKey: string;\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n    protected defaultOptions: CallOptions;\n    _llmType(): string;\n    static lc_name(): string;\n    get callKeys(): string[];\n    lc_serializable: boolean;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_aliases(): Record<string, string>;\n    get lc_serializable_keys(): string[];\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    /** @ignore */\n    _identifyingParams(): Omit<OpenAIClient.Chat.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    /**\n     * Get the identifying parameters for the model\n     */\n    identifyingParams(): Omit<OpenAIClient.ChatCompletionCreateParams, \"messages\"> & {\n        model_name: string;\n    } & ClientOptions;\n    constructor(fields?: BaseChatOpenAIFields);\n    /**\n     * Returns backwards compatible reasoning parameters from constructor params and call options\n     * @internal\n     */\n    protected _getReasoningParams(options?: this[\"ParsedCallOptions\"]): OpenAIClient.Reasoning | undefined;\n    /**\n     * Returns an openai compatible response format from a set of options\n     * @internal\n     */\n    protected _getResponseFormat(resFormat?: CallOptions[\"response_format\"]): ResponseFormatConfiguration | undefined;\n    protected _combineCallOptions(additionalOptions?: this[\"ParsedCallOptions\"]): this[\"ParsedCallOptions\"];\n    /** @internal */\n    _getClientOptions(options: OpenAICoreRequestOptions | undefined): OpenAICoreRequestOptions;\n    // TODO: move to completions class\n    protected _convertChatOpenAIToolToCompletionsTool(tool: ChatOpenAIToolType, fields?: {\n        strict?: boolean;\n    }): OpenAIClient.ChatCompletionTool;\n    bindTools(tools: ChatOpenAIToolType[], kwargs?: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;\n    stream(input: BaseLanguageModelInput, options?: CallOptions): Promise<import(\"../../../langchain-core/dist/utils/stream.js\").IterableReadableStream<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>>;\n    invoke(input: BaseLanguageModelInput, options?: CallOptions): Promise<AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure>>;\n    /** @ignore */\n    _combineLLMOutput(...llmOutputs: OpenAILLMOutput[]): OpenAILLMOutput;\n    getNumTokensFromMessages(messages: BaseMessage[]): Promise<{\n        totalCount: number;\n        countPerMessage: number[];\n    }>;\n    /** @internal */\n    protected _getNumTokensFromGenerations(generations: ChatGeneration[]): Promise<number>;\n    /** @internal */\n    protected _getEstimatedTokenCountFromPrompt(messages: BaseMessage[], functions?: OpenAIClient.Chat.ChatCompletionCreateParams.Function[], function_call?: \"none\" | \"auto\" | OpenAIClient.Chat.ChatCompletionFunctionCallOption): Promise<number>;\n    /** @internal */\n    protected _getStructuredOutputMethod(config: StructuredOutputMethodOptions<boolean>): string | undefined;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<false>): Runnable<BaseLanguageModelInput, RunOutput>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<true>): Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n    withStructuredOutput<\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    RunOutput extends Record<string, any> = Record<string, any>>(outputSchema: InteropZodType<RunOutput>\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n     | Record<string, any>, config?: StructuredOutputMethodOptions<boolean>): Runnable<BaseLanguageModelInput, RunOutput> | Runnable<BaseLanguageModelInput, {\n        raw: BaseMessage;\n        parsed: RunOutput;\n    }>;\n}\nexport interface ChatOpenAIResponsesCallOptions extends BaseChatOpenAICallOptions {\n    /**\n     * Configuration options for a text response from the model. Can be plain text or\n     * structured JSON data.\n     */\n    text?: OpenAIClient.Responses.ResponseCreateParams[\"text\"];\n    /**\n     * The truncation strategy to use for the model response.\n     */\n    truncation?: OpenAIClient.Responses.ResponseCreateParams[\"truncation\"];\n    /**\n     * Specify additional output data to include in the model response.\n     */\n    include?: OpenAIClient.Responses.ResponseCreateParams[\"include\"];\n    /**\n     * The unique ID of the previous response to the model. Use this to create multi-turn\n     * conversations.\n     */\n    previous_response_id?: OpenAIClient.Responses.ResponseCreateParams[\"previous_response_id\"];\n    /**\n     * The verbosity of the model's response.\n     */\n    verbosity?: OpenAIVerbosityParam;\n}\ntype ChatResponsesInvocationParams = Omit<OpenAIClient.Responses.ResponseCreateParams, \"input\">;\ntype ExcludeController<T> = T extends {\n    controller: unknown;\n} ? never : T;\ntype ResponsesCreate = OpenAIClient.Responses[\"create\"];\ntype ResponsesParse = OpenAIClient.Responses[\"parse\"];\ntype ResponsesCreateInvoke = ExcludeController<Awaited<ReturnType<ResponsesCreate>>>;\ntype ResponsesParseInvoke = ExcludeController<Awaited<ReturnType<ResponsesParse>>>;\n/**\n * OpenAI Responses API implementation.\n *\n * Will be exported in a later version of @langchain/openai.\n *\n * @internal\n */\nexport declare class ChatOpenAIResponses<CallOptions extends ChatOpenAIResponsesCallOptions = ChatOpenAIResponsesCallOptions> extends BaseChatOpenAI<CallOptions> {\n    invocationParams(options?: this[\"ParsedCallOptions\"]): ChatResponsesInvocationParams;\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"]): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    /**\n     * Calls the Responses API with retry logic in case of failures.\n     * @param request The request to send to the OpenAI API.\n     * @param options Optional configuration for the API call.\n     * @returns The response from the OpenAI API.\n     */\n    completionWithRetry(request: OpenAIClient.Responses.ResponseCreateParamsStreaming, requestOptions?: OpenAIClient.RequestOptions): Promise<AsyncIterable<OpenAIClient.Responses.ResponseStreamEvent>>;\n    completionWithRetry(request: OpenAIClient.Responses.ResponseCreateParamsNonStreaming, requestOptions?: OpenAIClient.RequestOptions): Promise<OpenAIClient.Responses.Response>;\n    /** @internal */\n    protected _convertResponsesMessageToBaseMessage(response: ResponsesCreateInvoke | ResponsesParseInvoke): BaseMessage;\n    /** @internal */\n    protected _convertResponsesDeltaToBaseMessageChunk(chunk: OpenAIClient.Responses.ResponseStreamEvent): ChatGenerationChunk | null;\n    /** @internal */\n    protected _convertMessagesToResponsesParams(messages: BaseMessage[]): any[];\n    /** @internal */\n    protected _convertReasoningSummary(reasoning: ChatOpenAIReasoningSummary): OpenAIClient.Responses.ResponseReasoningItem;\n    /** @internal */\n    protected _reduceChatOpenAITools(tools: ChatOpenAIToolType[], fields: {\n        stream?: boolean;\n        strict?: boolean;\n    }): ResponsesTool[];\n}\nexport interface ChatOpenAICompletionsCallOptions extends BaseChatOpenAICallOptions {\n}\ntype ChatCompletionsInvocationParams = Omit<OpenAIClient.Chat.Completions.ChatCompletionCreateParams, \"messages\">;\n/**\n * OpenAI Completions API implementation.\n * @internal\n */\nexport declare class ChatOpenAICompletions<CallOptions extends ChatOpenAICompletionsCallOptions = ChatOpenAICompletionsCallOptions> extends BaseChatOpenAI<CallOptions> {\n    /** @internal */\n    invocationParams(options?: this[\"ParsedCallOptions\"], extra?: {\n        streaming?: boolean;\n    }): ChatCompletionsInvocationParams;\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    completionWithRetry(request: OpenAIClient.Chat.ChatCompletionCreateParamsStreaming, requestOptions?: OpenAIClient.RequestOptions): Promise<AsyncIterable<OpenAIClient.Chat.Completions.ChatCompletionChunk>>;\n    completionWithRetry(request: OpenAIClient.Chat.ChatCompletionCreateParamsNonStreaming, requestOptions?: OpenAIClient.RequestOptions): Promise<OpenAIClient.Chat.Completions.ChatCompletion>;\n    /** @internal */\n    protected _convertCompletionsMessageToBaseMessage(message: OpenAIClient.Chat.Completions.ChatCompletionMessage, rawResponse: OpenAIClient.Chat.Completions.ChatCompletion): BaseMessage;\n    /** @internal */\n    protected _convertCompletionsDeltaToBaseMessageChunk(\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    delta: Record<string, any>, rawResponse: OpenAIClient.Chat.Completions.ChatCompletionChunk, defaultRole?: OpenAIClient.Chat.ChatCompletionRole): AIMessageChunk<import(\"@langchain/core/messages\").MessageStructure> | ChatMessageChunk<import(\"@langchain/core/messages\").MessageStructure> | FunctionMessageChunk<import(\"@langchain/core/messages\").MessageStructure> | HumanMessageChunk<import(\"@langchain/core/messages\").MessageStructure> | SystemMessageChunk<import(\"@langchain/core/messages\").MessageStructure> | ToolMessageChunk<import(\"@langchain/core/messages\").MessageStructure>;\n}\nexport type ChatOpenAICallOptions = ChatOpenAICompletionsCallOptions & ChatOpenAIResponsesCallOptions;\nexport interface ChatOpenAIFields extends BaseChatOpenAIFields {\n    /**\n     * Whether to use the responses API for all requests. If `false` the responses API will be used\n     * only when required in order to fulfill the request.\n     */\n    useResponsesApi?: boolean;\n    /**\n     * The completions chat instance\n     * @internal\n     */\n    completions?: ChatOpenAICompletions;\n    /**\n     * The responses chat instance\n     * @internal\n     */\n    responses?: ChatOpenAIResponses;\n}\n/**\n * OpenAI chat model integration.\n *\n * To use with Azure, import the `AzureChatOpenAI` class.\n *\n * Setup:\n * Install `@langchain/openai` and set an environment variable named `OPENAI_API_KEY`.\n *\n * ```bash\n * npm install @langchain/openai\n * export OPENAI_API_KEY=\"your-api-key\"\n * ```\n *\n * ## [Constructor args](https://api.js.langchain.com/classes/langchain_openai.ChatOpenAI.html#constructor)\n *\n * ## [Runtime args](https://api.js.langchain.com/interfaces/langchain_openai.ChatOpenAICallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     tool_choice: \"auto\",\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from '@langchain/openai';\n *\n * const llm = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   timeout: undefined,\n *   maxRetries: 2,\n *   // apiKey: \"...\",\n *   // configuration: {\n *   //   baseURL: \"...\",\n *   // }\n *   // organization: \"...\",\n *   // other params...\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"id\": \"chatcmpl-9u4Mpu44CbPjwYFkTbeoZgvzB00Tz\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"tokenUsage\": {\n *       \"completionTokens\": 5,\n *       \"promptTokens\": 28,\n *       \"totalTokens\": 33\n *     },\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_3aa7262c27\"\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4NWB7yUeHCKdLr6jP3HpaOYHTqs\",\n *   \"content\": \"\"\n * }\n * AIMessageChunk {\n *   \"content\": \"J\"\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore\"\n * }\n * AIMessageChunk {\n *   \"content\": \" la\"\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation\",,\n * }\n * AIMessageChunk {\n *   \"content\": \".\",,\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"finish_reason\": \"stop\",\n *     \"system_fingerprint\": \"fp_c9aa9c0491\"\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"id\": \"chatcmpl-9u4PnX6Fy7OmK46DASy0bH6cxn5Xu\",\n *   \"content\": \"J'adore la programmation.\",\n *   \"response_metadata\": {\n *     \"prompt\": 0,\n *     \"completion\": 0,\n *     \"finish_reason\": \"stop\",\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 28,\n *     \"output_tokens\": 5,\n *     \"total_tokens\": 33\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools(\n *   [GetWeather, GetPopulation],\n *   {\n *     // strict: true  // enforce tool args schema is respected\n *   }\n * );\n * const aiMsg = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_uPU4FiFzoKAtMxfmPnfQL6UK'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_UNkEwuQsHrGYqgDQuH9nPAtX'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     type: 'tool_call',\n *     id: 'call_kL3OXxaq9OjIKqRTpvjaCH14'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     type: 'tool_call',\n *     id: 'call_s9KQB1UWj45LLGaEnjz0179q'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n *\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().nullable().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke, {\n *   name: \"Joke\",\n *   strict: true, // Optionally enable OpenAI structured outputs\n * });\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: 'Why was the cat sitting on the computer?',\n *   punchline: 'Because it wanted to keep an eye on the mouse!',\n *   rating: 7\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Object Response Format</strong></summary>\n *\n * ```typescript\n * const jsonLlm = llm.withConfig({ response_format: { type: \"json_object\" } });\n * const jsonLlmAiMsg = await jsonLlm.invoke(\n *   \"Return a JSON object with key 'randomInts' and a value of 10 random ints in [0-99]\"\n * );\n * console.log(jsonLlmAiMsg.content);\n * ```\n *\n * ```txt\n * {\n *   \"randomInts\": [23, 87, 45, 12, 78, 34, 56, 90, 11, 67]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Multimodal</strong></summary>\n *\n * ```typescript\n * import { HumanMessage } from '@langchain/core/messages';\n *\n * const imageUrl = \"https://example.com/image.jpg\";\n * const imageData = await fetch(imageUrl).then(res => res.arrayBuffer());\n * const base64Image = Buffer.from(imageData).toString('base64');\n *\n * const message = new HumanMessage({\n *   content: [\n *     { type: \"text\", text: \"describe the weather in this image\" },\n *     {\n *       type: \"image_url\",\n *       image_url: { url: `data:image/jpeg;base64,${base64Image}` },\n *     },\n *   ]\n * });\n *\n * const imageDescriptionAiMsg = await llm.invoke([message]);\n * console.log(imageDescriptionAiMsg.content);\n * ```\n *\n * ```txt\n * The weather in the image appears to be clear and sunny. The sky is mostly blue with a few scattered white clouds, indicating fair weather. The bright sunlight is casting shadows on the green, grassy hill, suggesting it is a pleasant day with good visibility. There are no signs of rain or stormy conditions.\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Usage Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForMetadata = await llm.invoke(input);\n * console.log(aiMsgForMetadata.usage_metadata);\n * ```\n *\n * ```txt\n * { input_tokens: 28, output_tokens: 5, total_tokens: 33 }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Logprobs</strong></summary>\n *\n * ```typescript\n * const logprobsLlm = new ChatOpenAI({ model: \"gpt-4o-mini\", logprobs: true });\n * const aiMsgForLogprobs = await logprobsLlm.invoke(input);\n * console.log(aiMsgForLogprobs.response_metadata.logprobs);\n * ```\n *\n * ```txt\n * {\n *   content: [\n *     {\n *       token: 'J',\n *       logprob: -0.000050616763,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: \"'\",\n *       logprob: -0.01868736,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: 'ad',\n *       logprob: -0.0000030545007,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ore', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: ' la',\n *       logprob: -0.515404,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     {\n *       token: ' programm',\n *       logprob: -0.0000118755715,\n *       bytes: [Array],\n *       top_logprobs: []\n *     },\n *     { token: 'ation', logprob: 0, bytes: [Array], top_logprobs: [] },\n *     {\n *       token: '.',\n *       logprob: -0.0000037697225,\n *       bytes: [Array],\n *       top_logprobs: []\n *     }\n *   ],\n *   refusal: null\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * {\n *   tokenUsage: { completionTokens: 5, promptTokens: 28, totalTokens: 33 },\n *   finish_reason: 'stop',\n *   system_fingerprint: 'fp_3aa7262c27'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>JSON Schema Structured Output</strong></summary>\n *\n * ```typescript\n * const llmForJsonSchema = new ChatOpenAI({\n *   model: \"gpt-4o-2024-08-06\",\n * }).withStructuredOutput(\n *   z.object({\n *     command: z.string().describe(\"The command to execute\"),\n *     expectedOutput: z.string().describe(\"The expected output of the command\"),\n *     options: z\n *       .array(z.string())\n *       .describe(\"The options you can pass to the command\"),\n *   }),\n *   {\n *     method: \"jsonSchema\",\n *     strict: true, // Optional when using the `jsonSchema` method\n *   }\n * );\n *\n * const jsonSchemaRes = await llmForJsonSchema.invoke(\n *   \"What is the command to list files in a directory?\"\n * );\n * console.log(jsonSchemaRes);\n * ```\n *\n * ```txt\n * {\n *   command: 'ls',\n *   expectedOutput: 'A list of files and subdirectories within the specified directory.',\n *   options: [\n *     '-a: include directory entries whose names begin with a dot (.).',\n *     '-l: use a long listing format.',\n *     '-h: with -l, print sizes in human readable format (e.g., 1K, 234M, 2G).',\n *     '-t: sort by time, newest first.',\n *     '-r: reverse order while sorting.',\n *     '-S: sort by file size, largest first.',\n *     '-R: list subdirectories recursively.'\n *   ]\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castMessageContent = audioOutputResult.content[0] as Record<string, any>;\n *\n * console.log({\n *   ...castMessageContent,\n *   data: castMessageContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Audio Outputs</strong></summary>\n *\n * ```typescript\n * import { ChatOpenAI } from \"@langchain/openai\";\n *\n * const modelWithAudioOutput = new ChatOpenAI({\n *   model: \"gpt-4o-audio-preview\",\n *   // You may also pass these fields to `.withConfig` as a call argument.\n *   modalities: [\"text\", \"audio\"], // Specifies that the model should output audio.\n *   audio: {\n *     voice: \"alloy\",\n *     format: \"wav\",\n *   },\n * });\n *\n * const audioOutputResult = await modelWithAudioOutput.invoke(\"Tell me a joke about cats.\");\n * const castAudioContent = audioOutputResult.additional_kwargs.audio as Record<string, any>;\n *\n * console.log({\n *   ...castAudioContent,\n *   data: castAudioContent.data.slice(0, 100) // Sliced for brevity\n * })\n * ```\n *\n * ```txt\n * {\n *   id: 'audio_67117718c6008190a3afad3e3054b9b6',\n *   data: 'UklGRqYwBgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AAACABAATElTVBoAAABJTkZPSVNGVA4AAABMYXZmNTguMjkuMTAwAGRhdGFg',\n *   expires_at: 1729201448,\n *   transcript: 'Sure! Why did the cat sit on the computer? Because it wanted to keep an eye on the mouse!'\n * }\n * ```\n * </details>\n *\n * <br />\n */\nexport declare class ChatOpenAI<CallOptions extends ChatOpenAICallOptions = ChatOpenAICallOptions> extends BaseChatOpenAI<CallOptions> {\n    protected fields?: ChatOpenAIFields | undefined;\n    /**\n     * Whether to use the responses API for all requests. If `false` the responses API will be used\n     * only when required in order to fulfill the request.\n     */\n    useResponsesApi: boolean;\n    protected responses: ChatOpenAIResponses;\n    protected completions: ChatOpenAICompletions;\n    get lc_serializable_keys(): string[];\n    constructor(fields?: ChatOpenAIFields | undefined);\n    protected _useResponsesApi(options: this[\"ParsedCallOptions\"] | undefined): boolean | undefined;\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    invocationParams(options?: this[\"ParsedCallOptions\"]): ChatCompletionsInvocationParams | ChatResponsesInvocationParams;\n    /** @ignore */\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    withConfig(config: Partial<CallOptions>): Runnable<BaseLanguageModelInput, AIMessageChunk, CallOptions>;\n}\n"],"mappings":";;;;;;;;;;;;;;UAUUkC,eAAAA;;;;IAAAA,WAAAA,CAAAA,EAAe,MAAA;EAQRC,CAAAA;;AAaCL,UAbDK,yBAAAA,SAAkCZ,iBAajCO,EAboDZ,uBAapDY,CAAAA;EAAgB;;;;;EAoDZ,KAMV3B,CAAAA,EAjEA4B,kBAiEkBS,EAAAA;EAAwB;;;;;EAvEc;EAAyB,WAAA,CAAA,EAa3EV,gBAb2E,GAaxDG,mBAbwD;EAsG5EW;;;;EAIgB,WAJaC,CAAAA,EAAAA,MAAAA;EAAO;AAAsC;AAO3F;EAA4C,eAAA,CAAA,EAvFtBnB,wBAuFsB;EAAA;;;;;EAMtB,IAEJF,CAAAA,EAAAA,MAAAA;EAAe;;;;;EAuBX,cACOkB,CAAAA,EA3GRvC,QAAAA,CAAaiC,IAAAA,CAAKC,2BA2GVK;EAAS;;;;;EAsC8B,mBAEnBC,CAAAA,EAAAA,OAAAA;EAA0B;;;;;;;;;;;EAsBmB,MAElCZ,CAAAA,EAAAA,OAAAA;EAAkB;;;;;;;;;;;;EAIwF,UAAA,CAAA,EAhJrJQ,KAgJqJ,CAhJ/IpC,QAAAA,CAAaiC,IAAAA,CAAKE,sBAgJ6H,CAAA;EAAf;;;;;EAC/D,KAAtBgB,CAAAA,EA3ItDnD,QAAAA,CAAaiC,IAAAA,CAAKI,wBA2IoCc;EAAO;;;;EAGX,UAKNzC,CAAAA,EA9IvCV,QAAAA,CAAasC,+BA8I0B5B;EAAc;;;;;;;;EAOpB,SAA4C4C,CAAAA,EA5I9EtD,QAAAA,CAAauC,SA4IiEe;EAAS;;;;EAEI,YAAEA,CAAAA,EAzI1FtD,QAAAA,CAAaiC,IAAAA,CAAKO,0BAyIwEc,CAAAA,cAAAA,CAAAA;EAAS;;;;;EAGzB,cAEtFT,CAAAA,EAAAA,MAAAA;EAAM;;;EACW,SACRS,CAAAA,EAtIA5B,oBAsIA4B;;AAIMT,UAxILJ,oBAAAA,SAA6BC,OAwIxBG,CAxIgCxB,eAwIhCwB,CAAAA,EAxIkD/B,mBAwIlD+B,CAAAA;EAAM;;;EAAiE,aAEtFA,CAAAA,EAtIa/C,aAsIb+C;;;AAAwGS,uBAnIjFX,cAmIiFW,CAAAA,oBAnI9CtB,yBAmI8CsB,CAAAA,SAnIX1C,aAmIW0C,CAnIGV,WAmIHU,EAnIgBpD,cAmIhBoD,CAAAA,YAnI2CZ,OAmI3CY,CAnImDjC,eAmInDiC,CAAAA,CAAAA;EAAS,WAA1CpC,CAAAA,EAAAA,MAAAA;EAAQ,IAA+CF,CAAAA,EAAAA,MAAAA;EAAsB,gBAC9Ib,CAAAA,EAAAA,MAAAA;EAAW,eACRmD,CAAAA,EAAAA,MAAAA;EAAS,CAAA,CAAA,EAFmGpC,MAAAA;EAAQ,SAnIhCN,CAAAA,EAMpFiC,MANoFjC,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EAAa,KAAyC8B,EAAAA,MAAAA;EAAO,WAAA,CAAA,EAQ/IrB,eAR+I,CAAA,aAAA,CAAA;EAwIhJkC,IAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EAA8B,aAAA,CAAA,EAAA,MAAA,EAAA;EAAA,IAKpCvD,CAAAA,EAAAA,MAAawD;EAA8B,OAIrCxD,CAAAA,EAAAA,MAAawD;EAA8B,SAI9CxD,EAAAA,OAAawD;EAA8B,WAK9BxD,EAAawD,OAAAA;EAA8B,SAItD9B,CAAAA,EAAAA,MAAAA;EAAoB,QAtBoBM,CAAAA,EAAAA,OAAAA;EAAyB,WAAA,CAAA,EAAA,MAAA;EAwB5E0B,MAAAA,CAAAA,EAAAA,MAAAA;EAA6B,YAAA,CAAA,EAAA,MAAA;EAAA,oBAAqBF,CAAAA,EAAUC,OAAAA;EAAoB;EAA5C,MAAA,EA1I7BzD,QA0I6B;EACpC2D;EAAiB,YAAA,EAzIJ7D,aAyII;EAAA;;AAET;AAAA;EAERgE,yBAAc,CAAA,EAAG9D,OAAAA;EACjB+D,KAAAA,CAAAA,EAxIO/D,QAAAA,CAAaiC,IAAAA,CAAKI,wBAwIJ;EAAA,UAAA,CAAA,EAvITD,KAuIS,CAvIHpC,QAAAA,CAAaiC,IAAAA,CAAKE,sBAuIf,CAAA;EAAA,SAAwC0B,CAAAA,EAtIlD7D,QAAAA,CAAauC,SAsIqCsB;EAAe;;;AAAnC;AAAA;;;;;;AACD;EAQxBM,UAAAA,CAAAA,EAAAA,OAAAA,GAAmB,SAAA;EAAA;;;;EAAwH,YACrGT,CAAAA,EA/HxC1D,QAAAA,CAAaiC,IAAAA,CAAKO,0BA+HsBkB,CAAAA,cAAAA,CAAAA;EAA6B;;;;;EAEoC,cAAkBjD,EAAAA,MAAAA;EAAmB;;;EAO9B,SAAyBT,CAAAA,EA9H5I0B,oBA8HmK6C;EAAmB,UAAxDC,cAAAA,EA7HhH5B,WA6HgH4B;EAAa,QAArBrB,CAAAA,CAAAA,EAAAA,MAAAA;EAAO,OAC5GnD,OAAawD,CAAAA,CAAAA,EAAAA,MAAUiB;EAAgC,IAAmBzE,QAAasE,CAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EAAc,eAAwBd,EAAAA,OAAUkB;EAAQ,IAAvCvB,UAAAA,CAAAA,CAAAA,EAAAA;IAE3EY,CAAAA,GAAAA,EAAAA,MAAAA,CAAAA,EAAAA,MAAAA;EAAqB,CAAA,GAAGG,SAAAA;EAAoB,IAAG/D,UAAAA,CAAAA,CAAAA,EAxHvF0C,MAwHuF1C,CAAAA,MAAAA,EAAAA,MAAAA,CAAAA;EAAW,IAE1DH,oBAAuBuE,CAAAA,CAAAA,EAAAA,MAAAA,EAAAA;EAAmB,WAAG9D,CAAAA,OAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA,CAAAA,EAxHtDI,eAwHsDJ;EAAmB;EAEzD,kBAEnBe,CAAAA,CAAAA,EA1HxBsB,IA0HwBtB,CA1HnBxB,QAAAA,CAAaiC,IAAAA,CAAKO,0BA0HChB,EAAAA,UAAAA,CAAAA,GAAAA;IAA6BxB,UAAawD,EAAAA,MAAUmB;EAAqB,CAAA,GAxHnH7E,aA0HoC8B;EAAkB;;AArBsF;EA0BnIgD,iBAAAA,CAAAA,CAAAA,EA3HQ9B,IA2HR8B,CA3Ha5E,QAAAA,CAAawC,0BA2HeR,EAAAA,UAAyB,CAAA,GAAA;IAE9E6C,UAAAA,EAAAA,MAAAA;EAA+B,CAAA,GA3H5B/E,aA2H4B;EAAA,WAAQE,CAAaiC,MAAiBO,CAAZsC,EA1HrCrC,oBA0HiDD;EAA0B;AAAzD;AAK3C;;EAA0C,UAAqBoC,mBAAAA,CAAAA,OAAAA,CAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA,CAAAA,EA1HS5E,QAAAA,CAAauC,SA0HtBqC,GAAAA,SAAAA;EAAgC;;;;EAK5D,UAAqD3E,kBAAAA,CAAAA,SAAAA,CAAAA,EA1H3C2C,WA0H2C3C,CAAAA,iBAAAA,CAAAA,CAAAA,EA1HVwB,2BA0HUxB,GAAAA,SAAAA;EAAwB,UAAWU,mBAAAA,CAAAA,iBAAAA,CAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA,CAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA;EAAU;EAAX,iBACtFR,CAAAA,OAAAA,EAxHLmB,wBAwHKnB,GAAAA,SAAAA,CAAAA,EAxHkCmB,wBAwHlCnB;EAAW;EAA6E,UAAkBM,uCAAAA,CAAAA,IAAAA,EAtHlFmB,kBAsHkFnB,EAAAA,MAAD,CAACA,EAAAA;IAAf2D,MAAAA,CAAAA,EAAAA,OAAAA;EAAc,CAAA,CAAA,EApHrIpE,QAAAA,CAAa+C,kBAqH8BiC;EAAmC,SAAmBhF,CAAAA,KAAasE,EApHjG1C,kBAoHiG0C,EAAAA,EAAAA,MAAAA,CAAAA,EApHlE5B,OAoHkE4B,CApH1D1B,WAoH0D0B,CAAAA,CAAAA,EApH3CpD,QAoH2CoD,CApHlCtD,sBAoHkCsD,EApHVpE,cAoHUoE,EApHM1B,WAoHN0B,CAAAA;EAAc,MAAyBtE,CAAAA,KAAaiC,EAnHxJjB,sBAmHyKiE,EAAAA,OAAAA,CAAAA,EAnHvIrC,WAmHuIqC,CAAAA,EAnHzH9B,OAmHyH8B,CAnHrB,sBAmHqBA,CAnHnC/E,cAmHmC+E,CAnH5HjC,yBAAAA,CAA2IC,gBAAAA,CAmHfgC,CAAAA,CAAAA;EAAmB,MAA/DT,CAAAA,KAAAA,EAlH7HxD,sBAkH6HwD,EAAAA,OAAAA,CAAAA,EAlH3F5B,WAkH2F4B,CAAAA,EAlH7ErB,OAkH6EqB,CAlHrEtE,cAkHqEsE,CAlHhFxB,yBAAAA,CAA6DC,gBAAAA,CAkHmBuB,CAAAA;EAAa;EAAd,iBAC3FU,CAAAA,GAAAA,UAAAA,EAjHdnD,eAiHcmD,EAAAA,CAAAA,EAjHMnD,eAiHNmD;EAAsC,wBAAgCZ,CAAAA,QAAAA,EAhHlFnE,WAgHkFmE,EAAAA,CAAAA,EAhHlEnB,OAgHkEmB,CAAAA;IAAyBtE,UAAaiC,EAAAA,MAAK6C;IAA1B3B,eAAAA,EAAAA,MAAAA,EAAAA;EAAO,CAAA,CAAA;EAE/B;EAA2D,UAAGhD,4BAAAA,CAAAA,WAAAA,EA7GxHO,cA6GwHP,EAAAA,CAAAA,EA7GrGgD,OA6GqGhD,CAAAA,MAAAA,CAAAA;EAAW;EAI1K,UAA4BH,iCAA8BiF,CAAAA,QAAAA,EA/GjB9E,WA+GiB8E,EAAAA,EAAAA,SAAAA,CAAAA,EA/GUjF,QAAAA,CAAaiC,IAAAA,CAAKO,0BAAAA,CAA2BY,QA+GvD6B,EAAAA,EAAAA,aAAAA,CAAAA,EAAAA,MAAAA,GAAAA,MAAAA,GA/GqGjF,QAAAA,CAAaiC,IAAAA,CAAKoB,gCA+GvH4B,CAAAA,EA/G0J9B,OA+G1J8B,CAAAA,MAAAA,CAAAA;EAAmB;EAAoD,UAAAjC,0BAAqDC,CAAAA,MAAAA,EA7GtJhC,6BA6GsJgC,CAAAA,OAAAA,CAAAA,CAAAA,EAAAA,MAAAA,GAAAA,SAAAA;EAAgB,oBAAlE/C;EAAc;EAA4H,kBA1GzQ2C,MA0GqMzC,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GA1G/KyC,MA0G+KzC,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,CAAAA,CAAAA,YAAAA,EA1G5Ie,cA0G4If,CA1G7HkD,SA0G6HlD;EAAgB;EAAA,EAxGpOyC,MAwG4RxC,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,EAAAA,MAAAA,CAAAA,EAxG9PY,6BAwG8PZ,CAAAA,KAAAA,CAAAA,CAAAA,EAxGvNa,QAwGuNb,CAxG9MW,sBAwG8MX,EAxGtLiD,SAwGsLjD,CAAAA;EAAoB,oBAAA2C;EAA6H;EAApD,kBArG1WH,MAqG0WG,CAAAA,MAA8GC,EAAAA,GAAAA,CAAAA,GArGlcJ,MAqGkcI,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,CAAAA,CAAAA,YAAAA,EArG/Z9B,cAqG+Z8B,CArGhZK,SAqGgZL;EAAgB;EAAA,EAnGvfJ,MAmGmcG,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,EAAAA,MAA4GC,CAAAA,EAnGjhBhC,6BAmGihBgC,CAAAA,IAAAA,CAAAA,CAAAA,EAnG3e/B,QAmG2e+B,CAnGlejC,sBAmGkeiC,EAAAA;IAApDzC,GAAAA,EAlGrfL,WAkGqfK;IAdtXmC,MAAAA,EAnF5HW,SAmF4HX;EAAc,CAAA,CAAA;EAgB9I2C,oBAAAA;EAAqB;EAAA,kBA/FXzC,MA+Fc+B,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,GA/FQ/B,MA+FR+B,CAAAA,MAAAA,EAAAA,GAAAA,CAAAA,CAAAA,CAAAA,YAAAA,EA/F2CzD,cA+F3CyD,CA/F0DtB,SA+F1DsB;EAAgC;EAAA,EA7F7D/B,MA6F8F,CAAA,MAAA,EAAA,GAAA,CAAA,EAAA,MAAA,CAAA,EA7FhE5B,6BA6FgE,CAAA,OAAA,CAAA,CAAA,EA7FvBC,QA6FuB,CA7FdF,sBA6Fc,EA7FUsC,SA6FV,CAAA,GA7FuBpC,QA6FvB,CA7FgCF,sBA6FhC,EAAA;IACpFuE,GAAAA,EA7FJpF,WA6FoB;IAAA,MAAA,EA5FjBmD,SA4FiB;EAAA,CAAA,CAAA;;AAASb,UAzFzBc,8BAAAA,SAAuCvB,yBAyFdS,CAAAA;EAAoB;AA6iB9D;;;EAAyE,IAAG6C,CAAAA,EAjoBjEtF,QAAAA,CAAawD,SAAAA,CAAUC,oBAioB0C6B,CAAAA,MAAAA,CAAAA;EAAqB;;;EAOrD,UACjBP,CAAAA,EAroBV/E,QAAAA,CAAawD,SAAAA,CAAUC,oBAqoBbsB,CAAAA,YAAAA,CAAAA;EAAqB;;;EAK0C,OAAGrB,CAAAA,EAtoB/E1D,QAAAA,CAAawD,SAAAA,CAAUC,oBAsoBwDC,CAAAA,SAAAA,CAAAA;EAA6B;;;;EAEA,oBACtFvD,CAAAA,EApoBTH,QAAAA,CAAawD,SAAAA,CAAUC,oBAooBdtD,CAAAA,sBAAAA,CAAAA;EAAW;;;EAA8F,SAC9GyC,CAAAA,EAjoBflB,oBAioBekB;;KA/nB1Bc,6BAAAA,GAAgCZ,IA+nBkB9B,CA/nBbhB,QAAAA,CAAawD,SAAAA,CAAUC,oBA+nBVzC,EAAAA,OAAAA,CAAAA;KA9nBlD2C,iBA8nB0EzD,CAAAA,CAAAA,CAAAA,GA9nBnD0D,CA8nBmD1D,SAAAA;EAAc,UAAE0C,EAAAA,OAAAA;CAAW,GAAA,KAA5D1B,GA5nBlC0C,CA4nBkC1C;KA3nBzC2C,eAAAA,GAAkB7D,QAAAA,CAAawD,SA0mBuEb,CAAAA,QAAAA,CAAAA;AAAc,KAzmBpHmB,cAAAA,GAAiB9D,QAAAA,CAAawD,SAymBsF,CAAA,OAAA,CAAA;KAxmBpHO,qBAAAA,GAAwBJ,kBAAkBM,QAAQD,WAAWH;KAC7DK,oBAAAA,GAAuBP,kBAAkBM,QAAQD,WAAWF;;;;;;;;cAQ5CK,wCAAwCZ,iCAAiCA,wCAAwCZ,eAAeC;yDAC1Fc;sBACnCvD,oDAAoDgD,QAAQxC;kCAChDR,gEAAgEF,2BAA2BmE,eAAe3D;;;;;;;+BAO7GT,QAAAA,CAAawD,SAAAA,CAAUa,gDAAgDrE,QAAAA,CAAasE,iBAAiBnB,QAAQqB,cAAcxE,QAAAA,CAAawD,SAAAA,CAAUe;+BAClJvE,QAAAA,CAAawD,SAAAA,CAAUiB,mDAAmDzE,QAAAA,CAAasE,iBAAiBnB,QAAQnD,QAAAA,CAAawD,SAAAA,CAAUkB;;4DAE1GX,wBAAwBG,uBAAuB/D;;4DAE/CH,QAAAA,CAAawD,SAAAA,CAAUe,sBAAsB9D;;wDAEjDN;;gDAERqB,6BAA6BxB,QAAAA,CAAawD,SAAAA,CAAUmB;;0CAE1D/C;;;MAGpCC;;UAES+C,gCAAAA,SAAyC5C;KAErD6C,+BAAAA,GAAkC/B,KAAK9C,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYtC;;;;;cAKrDuC,0CAA0CH,mCAAmCA,0CAA0CjC,eAAeC;;;;MAInJiC;sBACgB1E,gEAAgEF,2BAA2BkD,QAAQxC;kCACvFR,gEAAgEF,2BAA2BmE,eAAe3D;+BAC7GT,QAAAA,CAAaiC,IAAAA,CAAK+C,sDAAsDhF,QAAAA,CAAasE,iBAAiBnB,QAAQqB,cAAcxE,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYG;+BAC1JjF,QAAAA,CAAaiC,IAAAA,CAAKiD,yDAAyDlF,QAAAA,CAAasE,iBAAiBnB,QAAQnD,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYK;;6DAEjHnF,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYM,oCAAoCpF,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYK,iBAAiBhF;;;;SAIrK0C,kCAAkC7C,QAAAA,CAAaiC,IAAAA,CAAK6C,WAAAA,CAAYG,mCAAmCjF,QAAAA,CAAaiC,IAAAA,CAAKoD,qBAAqBnF,eAAH8C,yBAAAA,CAAqDC,gBAAAA,IAAoB7C,iBAAxD4C,yBAAAA,CAA4GC,gBAAAA,IAAoB5C,qBAAxD2C,yBAAAA,CAAgHC,gBAAAA,IAAoB3C,kBAAxD0C,yBAAAA,CAA6GC,gBAAAA,IAAoB1C,mBAAxDyC,yBAAAA,CAA8GC,gBAAAA,IAAoBzC,iBAAxDwC,yBAAAA,CAA4GC,gBAAAA;;KAE1iBqC,qBAAAA,GAAwBV,mCAAmCrB;UACtDgC,gBAAAA,SAAyB9C;;;;;;;;;;gBAUxBsC;;;;;cAKFZ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cA8hBKqB,+BAA+BF,wBAAwBA,+BAA+B3C,eAAeC;qBACnG2C;;;;;;uBAMEpB;yBACEY;;uBAEFQ;;mDAE4B1E;yDACMgE,kCAAkCnB;;sBAErEvD,gEAAgEF,2BAA2BkD,QAAQxC;kCACvFR,gEAAgEF,2BAA2BmE,eAAe3D;qBACvHiC,QAAQE,eAAe1B,SAASF,wBAAwBd,gBAAgB0C"}
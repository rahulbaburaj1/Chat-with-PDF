const require_rolldown_runtime = require('../_virtual/rolldown_runtime.cjs');
const node_llama_cpp = require_rolldown_runtime.__toESM(require("node-llama-cpp"));

//#region src/utils/llama_cpp.ts
async function createLlamaModel(inputs, llama) {
	const options = {
		gpuLayers: inputs?.gpuLayers,
		modelPath: inputs.modelPath,
		useMlock: inputs?.useMlock,
		useMmap: inputs?.useMmap,
		vocabOnly: inputs?.vocabOnly
	};
	return llama.loadModel(options);
}
async function createLlamaContext(model, inputs) {
	const options = {
		batchSize: inputs?.batchSize,
		contextSize: inputs?.contextSize,
		threads: inputs?.threads
	};
	return model.createContext(options);
}
function createLlamaSession(context) {
	return new node_llama_cpp.LlamaChatSession({ contextSequence: context.getSequence() });
}
async function createLlamaJsonSchemaGrammar(schemaString, llama) {
	if (schemaString === void 0) return void 0;
	const schemaJSON = schemaString;
	return await llama.createGrammarForJsonSchema(schemaJSON);
}
async function createCustomGrammar(filePath, llama) {
	if (filePath === void 0) return void 0;
	return llama.createGrammar({ grammar: filePath });
}

//#endregion
exports.createCustomGrammar = createCustomGrammar;
exports.createLlamaContext = createLlamaContext;
exports.createLlamaJsonSchemaGrammar = createLlamaJsonSchemaGrammar;
exports.createLlamaModel = createLlamaModel;
exports.createLlamaSession = createLlamaSession;
//# sourceMappingURL=llama_cpp.cjs.map
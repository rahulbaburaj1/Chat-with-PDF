{"version":3,"file":"hf.js","names":["fields?: Partial<HFInput> & BaseLLMParams","options?: this[\"ParsedCallOptions\"]","prompt: string","options: this[\"ParsedCallOptions\"]","runManager?: CallbackManagerForLLMRun"],"sources":["../../src/llms/hf.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\n/**\n * Interface defining the parameters for configuring the Hugging Face\n * model for text generation.\n */\nexport interface HFInput {\n  /** Model to use */\n  model: string;\n\n  /** Custom inference endpoint URL to use */\n  endpointUrl?: string;\n\n  /** Sampling temperature to use */\n  temperature?: number;\n\n  /**\n   * Maximum number of tokens to generate in the completion.\n   */\n  maxTokens?: number;\n\n  /**\n   * The model will stop generating text when one of the strings in the list is generated.\n   */\n  stopSequences?: string[];\n\n  /** Total probability mass of tokens to consider at each step */\n  topP?: number;\n\n  /** Integer to define the top tokens considered within the sample operation to create new text. */\n  topK?: number;\n\n  /** Penalizes repeated tokens according to frequency */\n  frequencyPenalty?: number;\n\n  /** API key to use. */\n  apiKey?: string;\n\n  /**\n   * Credentials to use for the request. If this is a string, it will be passed straight on. If it's a boolean, true will be \"include\" and false will not send credentials at all.\n   */\n  includeCredentials?: string | boolean;\n}\n\n/**\n * Class implementing the Large Language Model (LLM) interface using the\n * Hugging Face Inference API for text generation.\n * @example\n * ```typescript\n * const model = new HuggingFaceInference({\n *   model: \"gpt2\",\n *   temperature: 0.7,\n *   maxTokens: 50,\n * });\n *\n * const res = await model.invoke(\n *   \"Question: What would be a good company name for a company that makes colorful socks?\\nAnswer:\"\n * );\n * console.log({ res });\n * ```\n */\nexport class HuggingFaceInference extends LLM implements HFInput {\n  lc_serializable = true;\n\n  get lc_secrets(): { [key: string]: string } | undefined {\n    return {\n      apiKey: \"HUGGINGFACEHUB_API_KEY\",\n    };\n  }\n\n  model = \"gpt2\";\n\n  temperature: number | undefined = undefined;\n\n  maxTokens: number | undefined = undefined;\n\n  stopSequences: string[] | undefined = undefined;\n\n  topP: number | undefined = undefined;\n\n  topK: number | undefined = undefined;\n\n  frequencyPenalty: number | undefined = undefined;\n\n  apiKey: string | undefined = undefined;\n\n  endpointUrl: string | undefined = undefined;\n\n  includeCredentials: string | boolean | undefined = undefined;\n\n  constructor(fields?: Partial<HFInput> & BaseLLMParams) {\n    super(fields ?? {});\n\n    this.model = fields?.model ?? this.model;\n    this.temperature = fields?.temperature ?? this.temperature;\n    this.maxTokens = fields?.maxTokens ?? this.maxTokens;\n    this.stopSequences = fields?.stopSequences ?? this.stopSequences;\n    this.topP = fields?.topP ?? this.topP;\n    this.topK = fields?.topK ?? this.topK;\n    this.frequencyPenalty = fields?.frequencyPenalty ?? this.frequencyPenalty;\n    this.apiKey =\n      fields?.apiKey ?? getEnvironmentVariable(\"HUGGINGFACEHUB_API_KEY\");\n    this.endpointUrl = fields?.endpointUrl;\n    this.includeCredentials = fields?.includeCredentials;\n\n    if (!this.apiKey) {\n      throw new Error(\n        `Please set an API key for HuggingFace Hub in the environment variable \"HUGGINGFACEHUB_API_KEY\" or in the apiKey field of the HuggingFaceInference constructor.`\n      );\n    }\n  }\n\n  _llmType() {\n    return \"hf\";\n  }\n\n  invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    return {\n      model: this.model,\n      parameters: {\n        // make it behave similar to openai, returning only the generated text\n        return_full_text: false,\n        temperature: this.temperature,\n        max_new_tokens: this.maxTokens,\n        stop: options?.stop ?? this.stopSequences,\n        top_p: this.topP,\n        top_k: this.topK,\n        repetition_penalty: this.frequencyPenalty,\n      },\n    };\n  }\n\n  async *_streamResponseChunks(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    const hfi = await this._prepareHFInference();\n    const stream = await this.caller.call(async () =>\n      hfi.textGenerationStream({\n        ...this.invocationParams(options),\n        inputs: prompt,\n      })\n    );\n    for await (const chunk of stream) {\n      const token = chunk.token.text;\n      yield new GenerationChunk({ text: token, generationInfo: chunk });\n      await runManager?.handleLLMNewToken(token ?? \"\");\n\n      // stream is done\n      if (chunk.generated_text)\n        yield new GenerationChunk({\n          text: \"\",\n          generationInfo: { finished: true },\n        });\n    }\n  }\n\n  /** @ignore */\n  async _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"]\n  ): Promise<string> {\n    const hfi = await this._prepareHFInference();\n    const args = { ...this.invocationParams(options), inputs: prompt };\n    const res = await this.caller.callWithOptions(\n      { signal: options.signal },\n      hfi.textGeneration.bind(hfi),\n      args\n    );\n    return res.generated_text;\n  }\n\n  /** @ignore */\n  private async _prepareHFInference() {\n    const { HfInference } = await HuggingFaceInference.imports();\n    const hfi = new HfInference(this.apiKey, {\n      includeCredentials: this.includeCredentials,\n    });\n    return this.endpointUrl ? hfi.endpoint(this.endpointUrl) : hfi;\n  }\n\n  /** @ignore */\n  static async imports(): Promise<{\n    HfInference: typeof import(\"@huggingface/inference\").HfInference;\n  }> {\n    try {\n      const { HfInference } = await import(\"@huggingface/inference\");\n      return { HfInference };\n    } catch {\n      throw new Error(\n        \"Please install huggingface as a dependency with, e.g. `pnpm install @huggingface/inference`\"\n      );\n    }\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;AAgEA,IAAa,uBAAb,MAAa,6BAA6B,IAAuB;CAC/D,kBAAkB;CAElB,IAAI,aAAoD;AACtD,SAAO,EACL,QAAQ,yBACT;CACF;CAED,QAAQ;CAER,cAAkC;CAElC,YAAgC;CAEhC,gBAAsC;CAEtC,OAA2B;CAE3B,OAA2B;CAE3B,mBAAuC;CAEvC,SAA6B;CAE7B,cAAkC;CAElC,qBAAmD;CAEnD,YAAYA,QAA2C;EACrD,MAAM,UAAU,CAAE,EAAC;EAEnB,KAAK,QAAQ,QAAQ,SAAS,KAAK;EACnC,KAAK,cAAc,QAAQ,eAAe,KAAK;EAC/C,KAAK,YAAY,QAAQ,aAAa,KAAK;EAC3C,KAAK,gBAAgB,QAAQ,iBAAiB,KAAK;EACnD,KAAK,OAAO,QAAQ,QAAQ,KAAK;EACjC,KAAK,OAAO,QAAQ,QAAQ,KAAK;EACjC,KAAK,mBAAmB,QAAQ,oBAAoB,KAAK;EACzD,KAAK,SACH,QAAQ,UAAU,uBAAuB,yBAAyB;EACpE,KAAK,cAAc,QAAQ;EAC3B,KAAK,qBAAqB,QAAQ;AAElC,MAAI,CAAC,KAAK,OACR,OAAM,IAAI,MACR,CAAC,8JAA8J,CAAC;CAGrK;CAED,WAAW;AACT,SAAO;CACR;CAED,iBAAiBC,SAAqC;AACpD,SAAO;GACL,OAAO,KAAK;GACZ,YAAY;IAEV,kBAAkB;IAClB,aAAa,KAAK;IAClB,gBAAgB,KAAK;IACrB,MAAM,SAAS,QAAQ,KAAK;IAC5B,OAAO,KAAK;IACZ,OAAO,KAAK;IACZ,oBAAoB,KAAK;GAC1B;EACF;CACF;CAED,OAAO,sBACLC,QACAC,SACAC,YACiC;EACjC,MAAM,MAAM,MAAM,KAAK,qBAAqB;EAC5C,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YACpC,IAAI,qBAAqB;GACvB,GAAG,KAAK,iBAAiB,QAAQ;GACjC,QAAQ;EACT,EAAC,CACH;AACD,aAAW,MAAM,SAAS,QAAQ;GAChC,MAAM,QAAQ,MAAM,MAAM;GAC1B,MAAM,IAAI,gBAAgB;IAAE,MAAM;IAAO,gBAAgB;GAAO;GAChE,MAAM,YAAY,kBAAkB,SAAS,GAAG;AAGhD,OAAI,MAAM,gBACR,MAAM,IAAI,gBAAgB;IACxB,MAAM;IACN,gBAAgB,EAAE,UAAU,KAAM;GACnC;EACJ;CACF;;CAGD,MAAM,MACJF,QACAC,SACiB;EACjB,MAAM,MAAM,MAAM,KAAK,qBAAqB;EAC5C,MAAM,OAAO;GAAE,GAAG,KAAK,iBAAiB,QAAQ;GAAE,QAAQ;EAAQ;EAClE,MAAM,MAAM,MAAM,KAAK,OAAO,gBAC5B,EAAE,QAAQ,QAAQ,OAAQ,GAC1B,IAAI,eAAe,KAAK,IAAI,EAC5B,KACD;AACD,SAAO,IAAI;CACZ;;CAGD,MAAc,sBAAsB;EAClC,MAAM,EAAE,aAAa,GAAG,MAAM,qBAAqB,SAAS;EAC5D,MAAM,MAAM,IAAI,YAAY,KAAK,QAAQ,EACvC,oBAAoB,KAAK,mBAC1B;AACD,SAAO,KAAK,cAAc,IAAI,SAAS,KAAK,YAAY,GAAG;CAC5D;;CAGD,aAAa,UAEV;AACD,MAAI;GACF,MAAM,EAAE,aAAa,GAAG,MAAM,OAAO;AACrC,UAAO,EAAE,YAAa;EACvB,QAAO;AACN,SAAM,IAAI,MACR;EAEH;CACF;AACF"}
{"version":3,"file":"summarization.cjs","names":["z","options: SummarizationMiddlewareConfig","createMiddleware","countTokensApproximately","RemoveMessage","REMOVE_ALL_MESSAGES","messages: BaseMessage[]","SystemMessage","systemPrompt: SystemMessage | null","conversationMessages: BaseMessage[]","cutoffIndex: number","originalSystemMessage: SystemMessage | null","summary: string","summaryPrefix: string","content","messagesToKeep: number","hasToolCalls","aiMessage: AIMessage","aiMessageIndex: number","toolCallIds: Set<string>","ToolMessage","messagesToSummarize: BaseMessage[]","model: BaseLanguageModel","summaryPrompt: string","tokenCounter: TokenCounter"],"sources":["../../../src/agents/middleware/summarization.ts"],"sourcesContent":["import { z } from \"zod/v3\";\nimport { v4 as uuid } from \"uuid\";\nimport {\n  BaseMessage,\n  AIMessage,\n  SystemMessage,\n  ToolMessage,\n  RemoveMessage,\n  trimMessages,\n} from \"@langchain/core/messages\";\nimport { BaseLanguageModel } from \"@langchain/core/language_models/base\";\nimport {\n  interopParse,\n  InferInteropZodOutput,\n  InferInteropZodInput,\n} from \"@langchain/core/utils/types\";\nimport { REMOVE_ALL_MESSAGES } from \"@langchain/langgraph\";\nimport { createMiddleware } from \"../middleware.js\";\nimport { countTokensApproximately } from \"./utils.js\";\nimport { hasToolCalls } from \"../utils.js\";\n\nconst DEFAULT_SUMMARY_PROMPT = `<role>\nContext Extraction Assistant\n</role>\n\n<primary_objective>\nYour sole objective in this task is to extract the highest quality/most relevant context from the conversation history below.\n</primary_objective>\n\n<objective_information>\nYou're nearing the total number of input tokens you can accept, so you must extract the highest quality/most relevant pieces of information from your conversation history.\nThis context will then overwrite the conversation history presented below. Because of this, ensure the context you extract is only the most important information to your overall goal.\n</objective_information>\n\n<instructions>\nThe conversation history below will be replaced with the context you extract in this step. Because of this, you must do your very best to extract and record all of the most important context from the conversation history.\nYou want to ensure that you don't repeat any actions you've already completed, so the context you extract from the conversation history should be focused on the most important information to your overall goal.\n</instructions>\n\nThe user will message you with the full message history you'll be extracting context from, to then replace. Carefully read over it all, and think deeply about what information is most important to your overall goal that should be saved:\n\nWith all of this in mind, please carefully read over the entire conversation history, and extract the most important and relevant context to replace it so that you can free up space in the conversation history.\nRespond ONLY with the extracted context. Do not include any additional information, or text before or after the extracted context.\n\n<messages>\nMessages to summarize:\n{messages}\n</messages>`;\n\nconst SUMMARY_PREFIX = \"## Previous conversation summary:\";\n\nconst DEFAULT_MESSAGES_TO_KEEP = 20;\nconst DEFAULT_TRIM_TOKEN_LIMIT = 4000;\nconst DEFAULT_FALLBACK_MESSAGE_COUNT = 15;\nconst SEARCH_RANGE_FOR_TOOL_PAIRS = 5;\n\ntype TokenCounter = (messages: BaseMessage[]) => number | Promise<number>;\n\nconst contextSchema = z.object({\n  model: z.custom<BaseLanguageModel>(),\n  maxTokensBeforeSummary: z.number().optional(),\n  messagesToKeep: z.number().default(DEFAULT_MESSAGES_TO_KEEP),\n  tokenCounter: z\n    .function()\n    .args(z.array(z.any()))\n    .returns(z.union([z.number(), z.promise(z.number())]))\n    .optional(),\n  summaryPrompt: z.string().default(DEFAULT_SUMMARY_PROMPT),\n  summaryPrefix: z.string().default(SUMMARY_PREFIX),\n});\n\nexport type SummarizationMiddlewareConfig = InferInteropZodInput<\n  typeof contextSchema\n>;\n\n/**\n * Summarization middleware that automatically summarizes conversation history when token limits are approached.\n *\n * This middleware monitors message token counts and automatically summarizes older\n * messages when a threshold is reached, preserving recent messages and maintaining\n * context continuity by ensuring AI/Tool message pairs remain together.\n *\n * @param options Configuration options for the summarization middleware\n * @returns A middleware instance\n *\n * @example\n * ```ts\n * import { summarizationMiddleware } from \"langchain\";\n * import { createAgent } from \"langchain\";\n *\n * const agent = createAgent({\n *   llm: model,\n *   tools: [getWeather],\n *   middleware: [\n *     summarizationMiddleware({\n *       model: new ChatOpenAI({ model: \"gpt-4o\" }),\n *       maxTokensBeforeSummary: 4000,\n *       messagesToKeep: 20,\n *     })\n *   ],\n * });\n *\n * ```\n */\nexport function summarizationMiddleware(\n  options: SummarizationMiddlewareConfig\n) {\n  return createMiddleware({\n    name: \"SummarizationMiddleware\",\n    contextSchema,\n    beforeModel: async (state, runtime) => {\n      /**\n       * Parse user options to get their explicit values\n       */\n      const userOptions = interopParse(contextSchema, options);\n\n      /**\n       * Merge context with user options, preferring user options when context has default values\n       */\n      const config = {\n        model: userOptions.model,\n        maxTokensBeforeSummary:\n          runtime.context.maxTokensBeforeSummary !== undefined\n            ? runtime.context.maxTokensBeforeSummary\n            : userOptions.maxTokensBeforeSummary,\n        messagesToKeep:\n          runtime.context.messagesToKeep === DEFAULT_MESSAGES_TO_KEEP\n            ? userOptions.messagesToKeep\n            : runtime.context.messagesToKeep ?? userOptions.messagesToKeep,\n        tokenCounter:\n          runtime.context.tokenCounter !== undefined\n            ? runtime.context.tokenCounter\n            : userOptions.tokenCounter,\n        summaryPrompt:\n          runtime.context.summaryPrompt === DEFAULT_SUMMARY_PROMPT\n            ? userOptions.summaryPrompt\n            : runtime.context.summaryPrompt ?? userOptions.summaryPrompt,\n        summaryPrefix:\n          runtime.context.summaryPrefix === SUMMARY_PREFIX\n            ? userOptions.summaryPrefix\n            : runtime.context.summaryPrefix ?? userOptions.summaryPrefix,\n      } as InferInteropZodOutput<typeof contextSchema>;\n      const { messages } = state;\n\n      // Ensure all messages have IDs\n      ensureMessageIds(messages);\n\n      const tokenCounter = config.tokenCounter || countTokensApproximately;\n      const totalTokens = await tokenCounter(messages);\n\n      if (\n        config.maxTokensBeforeSummary == null ||\n        totalTokens < config.maxTokensBeforeSummary\n      ) {\n        return;\n      }\n\n      const { systemPrompt, conversationMessages } =\n        splitSystemMessage(messages);\n      const cutoffIndex = findSafeCutoff(\n        conversationMessages,\n        config.messagesToKeep\n      );\n\n      if (cutoffIndex <= 0) {\n        return;\n      }\n\n      const { messagesToSummarize, preservedMessages } = partitionMessages(\n        systemPrompt,\n        conversationMessages,\n        cutoffIndex\n      );\n\n      const summary = await createSummary(\n        messagesToSummarize,\n        config.model,\n        config.summaryPrompt,\n        tokenCounter\n      );\n\n      const updatedSystemMessage = buildUpdatedSystemMessage(\n        systemPrompt,\n        summary,\n        config.summaryPrefix\n      );\n\n      return {\n        messages: [\n          new RemoveMessage({ id: REMOVE_ALL_MESSAGES }),\n          updatedSystemMessage,\n          ...preservedMessages,\n        ],\n      };\n    },\n  });\n}\n\n/**\n * Ensure all messages have unique IDs\n */\nfunction ensureMessageIds(messages: BaseMessage[]): void {\n  for (const msg of messages) {\n    if (!msg.id) {\n      msg.id = uuid();\n    }\n  }\n}\n\n/**\n * Separate system message from conversation messages\n */\nfunction splitSystemMessage(messages: BaseMessage[]): {\n  systemPrompt: SystemMessage | null;\n  conversationMessages: BaseMessage[];\n} {\n  if (messages.length > 0 && SystemMessage.isInstance(messages[0])) {\n    return {\n      systemPrompt: messages[0] as SystemMessage,\n      conversationMessages: messages.slice(1),\n    };\n  }\n  return {\n    systemPrompt: null,\n    conversationMessages: messages,\n  };\n}\n\n/**\n * Partition messages into those to summarize and those to preserve\n */\nfunction partitionMessages(\n  systemPrompt: SystemMessage | null,\n  conversationMessages: BaseMessage[],\n  cutoffIndex: number\n): { messagesToSummarize: BaseMessage[]; preservedMessages: BaseMessage[] } {\n  const messagesToSummarize = conversationMessages.slice(0, cutoffIndex);\n  const preservedMessages = conversationMessages.slice(cutoffIndex);\n\n  // Include system message in messages to summarize to capture previous summaries\n  if (systemPrompt) {\n    messagesToSummarize.unshift(systemPrompt);\n  }\n\n  return { messagesToSummarize, preservedMessages };\n}\n\n/**\n * Build updated system message incorporating the summary\n */\nfunction buildUpdatedSystemMessage(\n  originalSystemMessage: SystemMessage | null,\n  summary: string,\n  summaryPrefix: string\n): SystemMessage {\n  let originalContent = \"\";\n  if (originalSystemMessage) {\n    const { content } = originalSystemMessage;\n    if (typeof content === \"string\") {\n      originalContent = content.split(summaryPrefix)[0].trim();\n    }\n  }\n\n  const content = originalContent\n    ? `${originalContent}\\n${summaryPrefix}\\n${summary}`\n    : `${summaryPrefix}\\n${summary}`;\n\n  return new SystemMessage({\n    content,\n    id: originalSystemMessage?.id || uuid(),\n  });\n}\n\n/**\n * Find safe cutoff point that preserves AI/Tool message pairs\n */\nfunction findSafeCutoff(\n  messages: BaseMessage[],\n  messagesToKeep: number\n): number {\n  if (messages.length <= messagesToKeep) {\n    return 0;\n  }\n\n  const targetCutoff = messages.length - messagesToKeep;\n\n  for (let i = targetCutoff; i >= 0; i--) {\n    if (isSafeCutoffPoint(messages, i)) {\n      return i;\n    }\n  }\n\n  return 0;\n}\n\n/**\n * Check if cutting at index would separate AI/Tool message pairs\n */\nfunction isSafeCutoffPoint(\n  messages: BaseMessage[],\n  cutoffIndex: number\n): boolean {\n  if (cutoffIndex >= messages.length) {\n    return true;\n  }\n\n  const searchStart = Math.max(0, cutoffIndex - SEARCH_RANGE_FOR_TOOL_PAIRS);\n  const searchEnd = Math.min(\n    messages.length,\n    cutoffIndex + SEARCH_RANGE_FOR_TOOL_PAIRS\n  );\n\n  for (let i = searchStart; i < searchEnd; i++) {\n    if (!hasToolCalls(messages[i])) {\n      continue;\n    }\n\n    const toolCallIds = extractToolCallIds(messages[i] as AIMessage);\n    if (cutoffSeparatesToolPair(messages, i, cutoffIndex, toolCallIds)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n/**\n * Extract tool call IDs from an AI message\n */\nfunction extractToolCallIds(aiMessage: AIMessage): Set<string> {\n  const toolCallIds = new Set<string>();\n  if (aiMessage.tool_calls) {\n    for (const toolCall of aiMessage.tool_calls) {\n      const id =\n        typeof toolCall === \"object\" && \"id\" in toolCall ? toolCall.id : null;\n      if (id) {\n        toolCallIds.add(id);\n      }\n    }\n  }\n  return toolCallIds;\n}\n\n/**\n * Check if cutoff separates an AI message from its corresponding tool messages\n */\nfunction cutoffSeparatesToolPair(\n  messages: BaseMessage[],\n  aiMessageIndex: number,\n  cutoffIndex: number,\n  toolCallIds: Set<string>\n): boolean {\n  for (let j = aiMessageIndex + 1; j < messages.length; j++) {\n    const message = messages[j];\n    if (\n      ToolMessage.isInstance(message) &&\n      toolCallIds.has(message.tool_call_id)\n    ) {\n      const aiBeforeCutoff = aiMessageIndex < cutoffIndex;\n      const toolBeforeCutoff = j < cutoffIndex;\n      if (aiBeforeCutoff !== toolBeforeCutoff) {\n        return true;\n      }\n    }\n  }\n  return false;\n}\n\n/**\n * Generate summary for the given messages\n */\nasync function createSummary(\n  messagesToSummarize: BaseMessage[],\n  model: BaseLanguageModel,\n  summaryPrompt: string,\n  tokenCounter: TokenCounter\n): Promise<string> {\n  if (!messagesToSummarize.length) {\n    return \"No previous conversation history.\";\n  }\n\n  const trimmedMessages = await trimMessagesForSummary(\n    messagesToSummarize,\n    tokenCounter\n  );\n\n  if (!trimmedMessages.length) {\n    return \"Previous conversation was too long to summarize.\";\n  }\n\n  try {\n    const formattedPrompt = summaryPrompt.replace(\n      \"{messages}\",\n      JSON.stringify(trimmedMessages, null, 2)\n    );\n    const response = await model.invoke(formattedPrompt);\n    const { content } = response;\n    return typeof content === \"string\"\n      ? content.trim()\n      : \"Error generating summary: Invalid response format\";\n  } catch (e) {\n    return `Error generating summary: ${e}`;\n  }\n}\n\n/**\n * Trim messages to fit within summary generation limits\n */\nasync function trimMessagesForSummary(\n  messages: BaseMessage[],\n  tokenCounter: TokenCounter\n): Promise<BaseMessage[]> {\n  try {\n    return await trimMessages(messages, {\n      maxTokens: DEFAULT_TRIM_TOKEN_LIMIT,\n      tokenCounter: async (msgs) => Promise.resolve(tokenCounter(msgs)),\n      strategy: \"last\",\n      allowPartial: true,\n      includeSystem: true,\n    });\n  } catch {\n    // Fallback to last N messages if trimming fails\n    return messages.slice(-DEFAULT_FALLBACK_MESSAGE_COUNT);\n  }\n}\n"],"mappings":";;;;;;;;;;;AAqBA,MAAM,yBAAyB,CAAC;;;;;;;;;;;;;;;;;;;;;;;;;;WA0BrB,CAAC;AAEZ,MAAM,iBAAiB;AAEvB,MAAM,2BAA2B;AACjC,MAAM,2BAA2B;AACjC,MAAM,iCAAiC;AACvC,MAAM,8BAA8B;AAIpC,MAAM,gBAAgBA,SAAE,OAAO;CAC7B,OAAOA,SAAE,QAA2B;CACpC,wBAAwBA,SAAE,QAAQ,CAAC,UAAU;CAC7C,gBAAgBA,SAAE,QAAQ,CAAC,QAAQ,yBAAyB;CAC5D,cAAcA,SACX,UAAU,CACV,KAAKA,SAAE,MAAMA,SAAE,KAAK,CAAC,CAAC,CACtB,QAAQA,SAAE,MAAM,CAACA,SAAE,QAAQ,EAAEA,SAAE,QAAQA,SAAE,QAAQ,CAAC,AAAC,EAAC,CAAC,CACrD,UAAU;CACb,eAAeA,SAAE,QAAQ,CAAC,QAAQ,uBAAuB;CACzD,eAAeA,SAAE,QAAQ,CAAC,QAAQ,eAAe;AAClD,EAAC;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAmCF,SAAgB,wBACdC,SACA;AACA,QAAOC,oCAAiB;EACtB,MAAM;EACN;EACA,aAAa,OAAO,OAAO,YAAY;;;;GAIrC,MAAM,6DAA2B,eAAe,QAAQ;;;;GAKxD,MAAM,SAAS;IACb,OAAO,YAAY;IACnB,wBACE,QAAQ,QAAQ,2BAA2B,SACvC,QAAQ,QAAQ,yBAChB,YAAY;IAClB,gBACE,QAAQ,QAAQ,mBAAmB,2BAC/B,YAAY,iBACZ,QAAQ,QAAQ,kBAAkB,YAAY;IACpD,cACE,QAAQ,QAAQ,iBAAiB,SAC7B,QAAQ,QAAQ,eAChB,YAAY;IAClB,eACE,QAAQ,QAAQ,kBAAkB,yBAC9B,YAAY,gBACZ,QAAQ,QAAQ,iBAAiB,YAAY;IACnD,eACE,QAAQ,QAAQ,kBAAkB,iBAC9B,YAAY,gBACZ,QAAQ,QAAQ,iBAAiB,YAAY;GACpD;GACD,MAAM,EAAE,UAAU,GAAG;GAGrB,iBAAiB,SAAS;GAE1B,MAAM,eAAe,OAAO,gBAAgBC;GAC5C,MAAM,cAAc,MAAM,aAAa,SAAS;AAEhD,OACE,OAAO,0BAA0B,QACjC,cAAc,OAAO,uBAErB;GAGF,MAAM,EAAE,cAAc,sBAAsB,GAC1C,mBAAmB,SAAS;GAC9B,MAAM,cAAc,eAClB,sBACA,OAAO,eACR;AAED,OAAI,eAAe,EACjB;GAGF,MAAM,EAAE,qBAAqB,mBAAmB,GAAG,kBACjD,cACA,sBACA,YACD;GAED,MAAM,UAAU,MAAM,cACpB,qBACA,OAAO,OACP,OAAO,eACP,aACD;GAED,MAAM,uBAAuB,0BAC3B,cACA,SACA,OAAO,cACR;AAED,UAAO,EACL,UAAU;IACR,IAAIC,wCAAc,EAAE,IAAIC,0CAAqB;IAC7C;IACA,GAAG;GACJ,EACF;EACF;CACF,EAAC;AACH;;;;AAKD,SAAS,iBAAiBC,UAA+B;AACvD,MAAK,MAAM,OAAO,SAChB,KAAI,CAAC,IAAI,IACP,IAAI,mBAAW;AAGpB;;;;AAKD,SAAS,mBAAmBA,UAG1B;AACA,KAAI,SAAS,SAAS,KAAKC,wCAAc,WAAW,SAAS,GAAG,CAC9D,QAAO;EACL,cAAc,SAAS;EACvB,sBAAsB,SAAS,MAAM,EAAE;CACxC;AAEH,QAAO;EACL,cAAc;EACd,sBAAsB;CACvB;AACF;;;;AAKD,SAAS,kBACPC,cACAC,sBACAC,aAC0E;CAC1E,MAAM,sBAAsB,qBAAqB,MAAM,GAAG,YAAY;CACtE,MAAM,oBAAoB,qBAAqB,MAAM,YAAY;AAGjE,KAAI,cACF,oBAAoB,QAAQ,aAAa;AAG3C,QAAO;EAAE;EAAqB;CAAmB;AAClD;;;;AAKD,SAAS,0BACPC,uBACAC,SACAC,eACe;CACf,IAAI,kBAAkB;AACtB,KAAI,uBAAuB;EACzB,MAAM,EAAE,oBAAS,GAAG;AACpB,MAAI,OAAOC,cAAY,UACrB,kBAAkBA,UAAQ,MAAM,cAAc,CAAC,GAAG,MAAM;CAE3D;CAED,MAAM,UAAU,kBACZ,GAAG,gBAAgB,EAAE,EAAE,cAAc,EAAE,EAAE,SAAS,GAClD,GAAG,cAAc,EAAE,EAAE,SAAS;AAElC,QAAO,IAAIP,wCAAc;EACvB;EACA,IAAI,uBAAuB,oBAAY;CACxC;AACF;;;;AAKD,SAAS,eACPD,UACAS,gBACQ;AACR,KAAI,SAAS,UAAU,eACrB,QAAO;CAGT,MAAM,eAAe,SAAS,SAAS;AAEvC,MAAK,IAAI,IAAI,cAAc,KAAK,GAAG,IACjC,KAAI,kBAAkB,UAAU,EAAE,CAChC,QAAO;AAIX,QAAO;AACR;;;;AAKD,SAAS,kBACPT,UACAI,aACS;AACT,KAAI,eAAe,SAAS,OAC1B,QAAO;CAGT,MAAM,cAAc,KAAK,IAAI,GAAG,cAAc,4BAA4B;CAC1E,MAAM,YAAY,KAAK,IACrB,SAAS,QACT,cAAc,4BACf;AAED,MAAK,IAAI,IAAI,aAAa,IAAI,WAAW,KAAK;AAC5C,MAAI,CAACM,2BAAa,SAAS,GAAG,CAC5B;EAGF,MAAM,cAAc,mBAAmB,SAAS,GAAgB;AAChE,MAAI,wBAAwB,UAAU,GAAG,aAAa,YAAY,CAChE,QAAO;CAEV;AAED,QAAO;AACR;;;;AAKD,SAAS,mBAAmBC,WAAmC;CAC7D,MAAM,8BAAc,IAAI;AACxB,KAAI,UAAU,WACZ,MAAK,MAAM,YAAY,UAAU,YAAY;EAC3C,MAAM,KACJ,OAAO,aAAa,YAAY,QAAQ,WAAW,SAAS,KAAK;AACnE,MAAI,IACF,YAAY,IAAI,GAAG;CAEtB;AAEH,QAAO;AACR;;;;AAKD,SAAS,wBACPX,UACAY,gBACAR,aACAS,aACS;AACT,MAAK,IAAI,IAAI,iBAAiB,GAAG,IAAI,SAAS,QAAQ,KAAK;EACzD,MAAM,UAAU,SAAS;AACzB,MACEC,sCAAY,WAAW,QAAQ,IAC/B,YAAY,IAAI,QAAQ,aAAa,EACrC;GACA,MAAM,iBAAiB,iBAAiB;GACxC,MAAM,mBAAmB,IAAI;AAC7B,OAAI,mBAAmB,iBACrB,QAAO;EAEV;CACF;AACD,QAAO;AACR;;;;AAKD,eAAe,cACbC,qBACAC,OACAC,eACAC,cACiB;AACjB,KAAI,CAAC,oBAAoB,OACvB,QAAO;CAGT,MAAM,kBAAkB,MAAM,uBAC5B,qBACA,aACD;AAED,KAAI,CAAC,gBAAgB,OACnB,QAAO;AAGT,KAAI;EACF,MAAM,kBAAkB,cAAc,QACpC,cACA,KAAK,UAAU,iBAAiB,MAAM,EAAE,CACzC;EACD,MAAM,WAAW,MAAM,MAAM,OAAO,gBAAgB;EACpD,MAAM,EAAE,SAAS,GAAG;AACpB,SAAO,OAAO,YAAY,WACtB,QAAQ,MAAM,GACd;CACL,SAAQ,GAAG;AACV,SAAO,CAAC,0BAA0B,EAAE,GAAG;CACxC;AACF;;;;AAKD,eAAe,uBACblB,UACAkB,cACwB;AACxB,KAAI;AACF,SAAO,kDAAmB,UAAU;GAClC,WAAW;GACX,cAAc,OAAO,SAAS,QAAQ,QAAQ,aAAa,KAAK,CAAC;GACjE,UAAU;GACV,cAAc;GACd,eAAe;EAChB,EAAC;CACH,QAAO;AAEN,SAAO,SAAS,MAAM,CAAC,+BAA+B;CACvD;AACF"}
{"version":3,"file":"ioredis.d.cts","names":["Redis","BaseCache","Generation","RedisCache","Promise"],"sources":["../../src/caches/ioredis.d.ts"],"sourcesContent":["import { Redis } from \"ioredis\";\nimport { BaseCache } from \"@langchain/core/caches\";\nimport { Generation } from \"@langchain/core/outputs\";\n/**\n * Cache LLM results using Redis.\n * @example\n * ```typescript\n * const model = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   cache: new RedisCache(new Redis(), { ttl: 60 }),\n * });\n *\n * // Invoke the model with a prompt\n * const response = await model.invoke(\"Do something random!\");\n * console.log(response);\n *\n * // Remember to disconnect the Redis client when done\n * await redisClient.disconnect();\n * ```\n */\nexport declare class RedisCache extends BaseCache {\n    protected redisClient: Redis;\n    protected ttl?: number;\n    constructor(redisClient: Redis, config?: {\n        ttl?: number;\n    });\n    /**\n     * Retrieves data from the Redis server using a prompt and an LLM key. If\n     * the data is not found, it returns null.\n     * @param prompt The prompt used to find the data.\n     * @param llmKey The LLM key used to find the data.\n     * @returns The corresponding data as an array of Generation objects, or null if not found.\n     */\n    lookup(prompt: string, llmKey: string): Promise<Generation[] | null>;\n    /**\n     * Updates the data in the Redis server using a prompt and an LLM key.\n     * @param prompt The prompt used to store the data.\n     * @param llmKey The LLM key used to store the data.\n     * @param value The data to be stored, represented as an array of Generation objects.\n     */\n    update(prompt: string, llmKey: string, value: Generation[]): Promise<void>;\n}\n"],"mappings":";;;;;;;;;;;;;AAoBA;;;;;;;;;AAAiD;;;cAA5BG,UAAAA,SAAmBF,SAAAA;yBACbD;;2BAEEA;;;;;;;;;;0CAUeI,QAAQF;;;;;;;gDAOFA,eAAeE"}
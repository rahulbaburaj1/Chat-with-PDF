{"version":3,"file":"vercel_kv.d.ts","names":["VercelKV","Generation","BaseCache","VercelKVCacheProps","VercelKVCache","Promise"],"sources":["../../src/caches/vercel_kv.d.ts"],"sourcesContent":["import { type VercelKV } from \"@vercel/kv\";\nimport { Generation } from \"@langchain/core/outputs\";\nimport { BaseCache } from \"@langchain/core/caches\";\nexport type VercelKVCacheProps = {\n    /**\n     * An existing Vercel KV client\n     */\n    client?: VercelKV;\n    /**\n     * Time-to-live (TTL) for cached items in seconds\n     */\n    ttl?: number;\n};\n/**\n * A cache that uses Vercel KV as the backing store.\n * @example\n * ```typescript\n * const cache = new VercelKVCache({\n *   ttl: 3600, // Optional: Cache entries will expire after 1 hour\n * });\n *\n * // Initialize the OpenAI model with Vercel KV cache for caching responses\n * const model = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   cache,\n * });\n * await model.invoke(\"How are you today?\");\n * const cachedValues = await cache.lookup(\"How are you today?\", \"llmKey\");\n * ```\n */\nexport declare class VercelKVCache extends BaseCache {\n    private client;\n    private ttl?;\n    constructor(props: VercelKVCacheProps);\n    /**\n     * Lookup LLM generations in cache by prompt and associated LLM key.\n     */\n    lookup(prompt: string, llmKey: string): Promise<Generation[] | null>;\n    /**\n     * Update the cache with the given generations.\n     *\n     * Note this overwrites any existing generations for the given prompt and LLM key.\n     */\n    update(prompt: string, llmKey: string, value: Generation[]): Promise<void>;\n}\n"],"mappings":";;;;;;;;KAGYG,kBAAAA;;;;WAICH;;;;EAJDG,GAAAA,CAAAA,EAAAA,MAAAA;AA2BZ,CAAA;;;;;;;;AAAoD;;;;;;;;;;cAA/BC,aAAAA,SAAsBF,SAAAA;;;qBAGpBC;;;;0CAIqBE,QAAQJ;;;;;;gDAMFA,eAAeI"}
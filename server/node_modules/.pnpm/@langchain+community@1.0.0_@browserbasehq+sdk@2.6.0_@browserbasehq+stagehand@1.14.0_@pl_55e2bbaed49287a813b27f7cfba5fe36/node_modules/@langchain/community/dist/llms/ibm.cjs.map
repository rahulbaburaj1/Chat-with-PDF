{"version":3,"file":"ibm.cjs","names":["BaseLLM","fields: (WatsonxInputLLM | WatsonxDeployedInputLLM) & WatsonxAuth","authenticateAndSetInstance","options: this[\"ParsedCallOptions\"]","input: string","stream: boolean","singleGeneration: Generation[]","callback: () => T","options?: this[\"ParsedCallOptions\"]","AsyncCaller","prompts: string[]","runManager?: CallbackManagerForLLMRun","tokenUsage: TokenUsage","generations: Generation[][]","geneartionsArray: GenerationInfo[]","generationInfo: GenerationInfo","result: LLMResult","content: string","options?: TextTokenizeParameters","params: TextTokenizationParams","prompt: string","responseChunk: ResponseChunk","GenerationChunk"],"sources":["../../src/llms/ibm.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseLLM, BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { WatsonXAI } from \"@ibm-cloud/watsonx-ai\";\nimport {\n  RequestCallbacks,\n  ReturnOptionProperties,\n  TextGenLengthPenalty,\n  TextGenParameters,\n  TextTokenizationParams,\n  TextTokenizeParameters,\n} from \"@ibm-cloud/watsonx-ai/dist/watsonx-ai-ml/vml_v1.js\";\nimport {\n  Generation,\n  LLMResult,\n  GenerationChunk,\n} from \"@langchain/core/outputs\";\nimport { BaseLanguageModelCallOptions } from \"@langchain/core/language_models/base\";\nimport { AsyncCaller } from \"@langchain/core/utils/async_caller\";\nimport { authenticateAndSetInstance } from \"../utils/ibm.js\";\nimport {\n  GenerationInfo,\n  Neverify,\n  ResponseChunk,\n  TokenUsage,\n  WatsonxAuth,\n  WatsonxDeployedParams,\n  WatsonxParams,\n} from \"../types/ibm.js\";\n\n/**\n * Input to LLM class.\n */\n\nexport interface WatsonxLLMParams {\n  maxNewTokens?: number;\n  decodingMethod?: TextGenParameters.Constants.DecodingMethod | string;\n  lengthPenalty?: TextGenLengthPenalty;\n  minNewTokens?: number;\n  randomSeed?: number;\n  stopSequence?: string[];\n  temperature?: number;\n  timeLimit?: number;\n  topK?: number;\n  topP?: number;\n  repetitionPenalty?: number;\n  truncateInpuTokens?: number;\n  returnOptions?: ReturnOptionProperties;\n  includeStopSequence?: boolean;\n}\n\nexport interface WatsonxDeploymentLLMParams {\n  idOrName: string;\n}\n\nexport interface WatsonxCallOptionsLLM extends BaseLanguageModelCallOptions {\n  maxRetries?: number;\n  parameters?: Partial<WatsonxLLMParams>;\n  watsonxCallbacks?: RequestCallbacks;\n}\n\nexport interface WatsonxInputLLM\n  extends WatsonxParams,\n    BaseLLMParams,\n    WatsonxLLMParams,\n    Neverify<WatsonxDeploymentLLMParams> {}\n\nexport interface WatsonxDeployedInputLLM\n  extends WatsonxDeployedParams,\n    BaseLLMParams,\n    Neverify<WatsonxLLMParams> {\n  model?: never;\n}\n\nexport type WatsonxLLMConstructor = BaseLLMParams &\n  WatsonxLLMParams &\n  Partial<WatsonxParams> &\n  WatsonxDeployedParams;\n\n/**\n * Integration with an LLM.\n */\nexport class WatsonxLLM<\n    CallOptions extends WatsonxCallOptionsLLM = WatsonxCallOptionsLLM\n  >\n  extends BaseLLM<CallOptions>\n  implements WatsonxLLMConstructor\n{\n  // Used for tracing, replace with the same name as your class\n  static lc_name() {\n    return \"Watsonx\";\n  }\n\n  lc_serializable = true;\n\n  streaming = false;\n\n  model: string;\n\n  maxRetries = 0;\n\n  version = \"2024-05-31\";\n\n  serviceUrl: string;\n\n  maxNewTokens?: number;\n\n  spaceId?: string;\n\n  projectId?: string;\n\n  idOrName?: string;\n\n  decodingMethod?: TextGenParameters.Constants.DecodingMethod | string;\n\n  lengthPenalty?: TextGenLengthPenalty;\n\n  minNewTokens?: number;\n\n  randomSeed?: number;\n\n  stopSequence?: string[];\n\n  temperature?: number;\n\n  timeLimit?: number;\n\n  topK?: number;\n\n  topP?: number;\n\n  repetitionPenalty?: number;\n\n  truncateInpuTokens?: number;\n\n  returnOptions?: ReturnOptionProperties;\n\n  includeStopSequence?: boolean;\n\n  maxConcurrency?: number;\n\n  watsonxCallbacks?: RequestCallbacks;\n\n  private service: WatsonXAI;\n\n  constructor(\n    fields: (WatsonxInputLLM | WatsonxDeployedInputLLM) & WatsonxAuth\n  ) {\n    super(fields);\n\n    if (fields.model) {\n      this.model = fields.model ?? this.model;\n      this.version = fields.version;\n      this.maxNewTokens = fields.maxNewTokens ?? this.maxNewTokens;\n      this.serviceUrl = fields.serviceUrl;\n      this.decodingMethod = fields.decodingMethod;\n      this.lengthPenalty = fields.lengthPenalty;\n      this.minNewTokens = fields.minNewTokens;\n      this.randomSeed = fields.randomSeed;\n      this.stopSequence = fields.stopSequence;\n      this.temperature = fields.temperature;\n      this.timeLimit = fields.timeLimit;\n      this.topK = fields.topK;\n      this.topP = fields.topP;\n      this.repetitionPenalty = fields.repetitionPenalty;\n      this.truncateInpuTokens = fields.truncateInpuTokens;\n      this.returnOptions = fields.returnOptions;\n      this.includeStopSequence = fields.includeStopSequence;\n      this.projectId = fields?.projectId;\n      this.spaceId = fields?.spaceId;\n    } else {\n      this.idOrName = fields?.idOrName;\n    }\n\n    this.maxRetries = fields.maxRetries || this.maxRetries;\n    this.maxConcurrency = fields.maxConcurrency;\n    this.streaming = fields.streaming || this.streaming;\n    this.watsonxCallbacks = fields.watsonxCallbacks || this.watsonxCallbacks;\n\n    if (\n      (\"projectId\" in fields && \"spaceId\" in fields) ||\n      (\"projectId\" in fields && \"idOrName\" in fields) ||\n      (\"spaceId\" in fields && \"idOrName\" in fields)\n    )\n      throw new Error(\"Maximum 1 id type can be specified per instance\");\n\n    this.serviceUrl = fields?.serviceUrl;\n    const {\n      watsonxAIApikey,\n      watsonxAIAuthType,\n      watsonxAIBearerToken,\n      watsonxAIUsername,\n      watsonxAIPassword,\n      watsonxAIUrl,\n      disableSSL,\n      version,\n      serviceUrl,\n    } = fields;\n\n    const auth = authenticateAndSetInstance({\n      watsonxAIApikey,\n      watsonxAIAuthType,\n      watsonxAIBearerToken,\n      watsonxAIUsername,\n      watsonxAIPassword,\n      watsonxAIUrl,\n      disableSSL,\n      version,\n      serviceUrl,\n    });\n    if (auth) this.service = auth;\n    else throw new Error(\"You have not provided one type of authentication\");\n  }\n\n  get lc_secrets(): { [key: string]: string } {\n    return {\n      authenticator: \"AUTHENTICATOR\",\n      apiKey: \"WATSONX_AI_APIKEY\",\n      apikey: \"WATSONX_AI_APIKEY\",\n      watsonxAIAuthType: \"WATSONX_AI_AUTH_TYPE\",\n      watsonxAIApikey: \"WATSONX_AI_APIKEY\",\n      watsonxAIBearerToken: \"WATSONX_AI_BEARER_TOKEN\",\n      watsonxAIUsername: \"WATSONX_AI_USERNAME\",\n      watsonxAIPassword: \"WATSONX_AI_PASSWORD\",\n      watsonxAIUrl: \"WATSONX_AI_URL\",\n    };\n  }\n\n  get lc_aliases(): { [key: string]: string } {\n    return {\n      authenticator: \"authenticator\",\n      apikey: \"watsonx_ai_apikey\",\n      apiKey: \"watsonx_ai_apikey\",\n      watsonxAIAuthType: \"watsonx_ai_auth_type\",\n      watsonxAIApikey: \"watsonx_ai_apikey\",\n      watsonxAIBearerToken: \"watsonx_ai_bearer_token\",\n      watsonxAIUsername: \"watsonx_ai_username\",\n      watsonxAIPassword: \"watsonx_ai_password\",\n      watsonxAIUrl: \"watsonx_ai_url\",\n    };\n  }\n\n  invocationParams(options: this[\"ParsedCallOptions\"]) {\n    const { parameters } = options;\n    const { signal, ...rest } = options;\n    if (this.idOrName && Object.keys(rest).length > 0)\n      throw new Error(\"Options cannot be provided to a deployed model\");\n    if (this.idOrName) return undefined;\n    return {\n      max_new_tokens: parameters?.maxNewTokens ?? this.maxNewTokens,\n      decoding_method: parameters?.decodingMethod ?? this.decodingMethod,\n      length_penalty: parameters?.lengthPenalty ?? this.lengthPenalty,\n      min_new_tokens: parameters?.minNewTokens ?? this.minNewTokens,\n      random_seed: parameters?.randomSeed ?? this.randomSeed,\n      stop_sequences: options?.stop ?? this.stopSequence,\n      temperature: parameters?.temperature ?? this.temperature,\n      time_limit: parameters?.timeLimit ?? this.timeLimit,\n      top_k: parameters?.topK ?? this.topK,\n      top_p: parameters?.topP ?? this.topP,\n      repetition_penalty:\n        parameters?.repetitionPenalty ?? this.repetitionPenalty,\n      truncate_input_tokens:\n        parameters?.truncateInpuTokens ?? this.truncateInpuTokens,\n      return_options: parameters?.returnOptions ?? this.returnOptions,\n      include_stop_sequence:\n        parameters?.includeStopSequence ?? this.includeStopSequence,\n    };\n  }\n\n  invocationCallbacks(options: this[\"ParsedCallOptions\"]) {\n    return options.watsonxCallbacks ?? this.watsonxCallbacks;\n  }\n\n  scopeId() {\n    if (this.projectId)\n      return { projectId: this.projectId, modelId: this.model };\n    else if (this.spaceId)\n      return { spaceId: this.spaceId, modelId: this.model };\n    else if (this.idOrName)\n      return { idOrName: this.idOrName, modelId: this.model };\n    else return { modelId: this.model };\n  }\n\n  async listModels() {\n    const listModelParams = {\n      filters: \"function_text_generation\",\n    };\n    const listModels = await this.completionWithRetry(() =>\n      this.service.listFoundationModelSpecs(listModelParams)\n    );\n    return listModels.result.resources?.map((item) => item.model_id);\n  }\n\n  private async generateSingleMessage(\n    input: string,\n    options: this[\"ParsedCallOptions\"],\n    stream: true\n  ): Promise<\n    AsyncIterable<WatsonXAI.ObjectStreamed<WatsonXAI.TextGenResponse>>\n  >;\n\n  private async generateSingleMessage(\n    input: string,\n    options: this[\"ParsedCallOptions\"],\n    stream: false\n  ): Promise<Generation[]>;\n\n  private async generateSingleMessage(\n    input: string,\n    options: this[\"ParsedCallOptions\"],\n    stream: boolean\n  ) {\n    const {\n      signal,\n      stop,\n      maxRetries,\n      maxConcurrency,\n      timeout,\n      ...requestOptions\n    } = options;\n    const tokenUsage = { generated_token_count: 0, input_token_count: 0 };\n    const idOrName = this.idOrName;\n    const parameters = this.invocationParams(options);\n    const watsonxCallbacks = this.invocationCallbacks(options);\n    if (stream) {\n      const textStream = idOrName\n        ? this.service.deploymentGenerateTextStream({\n            idOrName,\n            ...requestOptions,\n            parameters: {\n              ...parameters,\n              prompt_variables: {\n                input,\n              },\n            },\n            returnObject: true,\n          })\n        : this.service.generateTextStream(\n            {\n              input,\n              parameters,\n              ...this.scopeId(),\n              ...requestOptions,\n              returnObject: true,\n            },\n            watsonxCallbacks\n          );\n      return (await textStream) as AsyncIterable<\n        WatsonXAI.ObjectStreamed<WatsonXAI.TextGenResponse>\n      >;\n    } else {\n      const textGenerationPromise = idOrName\n        ? this.service.deploymentGenerateText(\n            {\n              ...requestOptions,\n              idOrName,\n              parameters: {\n                ...parameters,\n                prompt_variables: {\n                  input,\n                },\n              },\n            },\n            watsonxCallbacks\n          )\n        : this.service.generateText(\n            {\n              input,\n              parameters,\n              ...this.scopeId(),\n              ...requestOptions,\n            },\n            watsonxCallbacks\n          );\n\n      const textGeneration = await textGenerationPromise;\n      const singleGeneration: Generation[] = textGeneration.result.results.map(\n        (result) => {\n          tokenUsage.generated_token_count += result.generated_token_count\n            ? result.generated_token_count\n            : 0;\n          tokenUsage.input_token_count += result.input_token_count\n            ? result.input_token_count\n            : 0;\n          return {\n            text: result.generated_text,\n            generationInfo: {\n              stop_reason: result.stop_reason,\n              input_token_count: result.input_token_count,\n              generated_token_count: result.generated_token_count,\n            },\n          };\n        }\n      );\n      return singleGeneration;\n    }\n  }\n\n  async completionWithRetry<T>(\n    callback: () => T,\n    options?: this[\"ParsedCallOptions\"]\n  ) {\n    const caller = new AsyncCaller({\n      maxConcurrency: options?.maxConcurrency || this.maxConcurrency,\n      maxRetries: this.maxRetries,\n    });\n    const result = options\n      ? caller.callWithOptions(\n          {\n            signal: options.signal,\n          },\n          async () => callback()\n        )\n      : caller.call(async () => callback());\n\n    return result;\n  }\n\n  async _generate(\n    prompts: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const tokenUsage: TokenUsage = {\n      generated_token_count: 0,\n      input_token_count: 0,\n    };\n    if (this.streaming) {\n      const generations: Generation[][] = await Promise.all(\n        prompts.map(async (prompt, promptIdx) => {\n          if (options.signal?.aborted) {\n            throw new Error(\"AbortError\");\n          }\n\n          const stream = this._streamResponseChunks(prompt, options);\n          const geneartionsArray: GenerationInfo[] = [];\n\n          for await (const chunk of stream) {\n            const completion = chunk?.generationInfo?.completion ?? 0;\n            const generationInfo: GenerationInfo = {\n              text: \"\",\n              stop_reason: \"\",\n              generated_token_count: 0,\n              input_token_count: 0,\n            };\n            geneartionsArray[completion] ??= generationInfo;\n            geneartionsArray[completion].generated_token_count =\n              chunk?.generationInfo?.usage_metadata.generated_token_count ?? 0;\n            geneartionsArray[completion].input_token_count +=\n              chunk?.generationInfo?.usage_metadata.input_token_count ?? 0;\n            geneartionsArray[completion].stop_reason =\n              chunk?.generationInfo?.stop_reason;\n            geneartionsArray[completion].text += chunk.text;\n            if (chunk.text)\n              // eslint-disable-next-line no-void\n              void runManager?.handleLLMNewToken(chunk.text, {\n                prompt: promptIdx,\n                completion: 0,\n              });\n          }\n\n          return geneartionsArray.map((item) => {\n            const { text, ...rest } = item;\n            tokenUsage.generated_token_count = rest.generated_token_count;\n            tokenUsage.input_token_count += rest.input_token_count;\n\n            return {\n              text,\n              generationInfo: rest,\n            };\n          });\n        })\n      );\n      const result: LLMResult = { generations, llmOutput: { tokenUsage } };\n      return result;\n    } else {\n      const generations: Generation[][] = await Promise.all(\n        prompts.map(async (prompt) => {\n          if (options.signal?.aborted) {\n            throw new Error(\"AbortError\");\n          }\n\n          const callback = () =>\n            this.generateSingleMessage(prompt, options, false);\n          type ReturnMessage = ReturnType<typeof callback>;\n\n          const response = await this.completionWithRetry<ReturnMessage>(\n            callback,\n            options\n          );\n          const [generated_token_count, input_token_count] = response.reduce(\n            (acc, curr) => {\n              let generated = 0;\n              let inputed = 0;\n              if (curr?.generationInfo?.generated_token_count)\n                generated = curr.generationInfo.generated_token_count + acc[0];\n              if (curr?.generationInfo?.input_token_count)\n                inputed = curr.generationInfo.input_token_count + acc[1];\n              return [generated, inputed];\n            },\n            [0, 0]\n          );\n          tokenUsage.generated_token_count += generated_token_count;\n          tokenUsage.input_token_count += input_token_count;\n          return response;\n        })\n      );\n\n      const result: LLMResult = { generations, llmOutput: { tokenUsage } };\n      return result;\n    }\n  }\n\n  async getNumTokens(\n    content: string,\n    options?: TextTokenizeParameters\n  ): Promise<number> {\n    const params: TextTokenizationParams = {\n      ...this.scopeId(),\n      input: content,\n      parameters: options,\n    };\n    const callback = () => this.service.tokenizeText(params);\n    type ReturnTokens = ReturnType<typeof callback>;\n\n    const response = await this.completionWithRetry<ReturnTokens>(callback);\n    return response.result.result.token_count;\n  }\n\n  async *_streamResponseChunks(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    const callback = () => this.generateSingleMessage(prompt, options, true);\n    type ReturnStream = ReturnType<typeof callback>;\n    const streamInferDeployedPrompt =\n      await this.completionWithRetry<ReturnStream>(callback);\n    const responseChunk: ResponseChunk = {\n      id: 0,\n      event: \"\",\n      data: {\n        results: [],\n      },\n    };\n    for await (const chunk of streamInferDeployedPrompt) {\n      if (options.signal?.aborted) {\n        throw new Error(\"AbortError\");\n      }\n\n      for (const [index, item] of chunk.data.results.entries()) {\n        yield new GenerationChunk({\n          text: item.generated_text,\n          generationInfo: {\n            stop_reason: item.stop_reason,\n            completion: index,\n            usage_metadata: {\n              generated_token_count: item.generated_token_count,\n              input_token_count: item.input_token_count,\n              stop_reason: item.stop_reason,\n            },\n          },\n        });\n        if (!this.streaming)\n          // eslint-disable-next-line no-void\n          void runManager?.handleLLMNewToken(item.generated_text);\n      }\n      Object.assign(responseChunk, { id: 0, event: \"\", data: {} });\n    }\n  }\n\n  _llmType() {\n    return \"watsonx\";\n  }\n}\n"],"mappings":";;;;;;;;;;;;AAiFA,IAAa,aAAb,cAGUA,8CAEV;CAEE,OAAO,UAAU;AACf,SAAO;CACR;CAED,kBAAkB;CAElB,YAAY;CAEZ;CAEA,aAAa;CAEb,UAAU;CAEV;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA,AAAQ;CAER,YACEC,QACA;EACA,MAAM,OAAO;AAEb,MAAI,OAAO,OAAO;GAChB,KAAK,QAAQ,OAAO,SAAS,KAAK;GAClC,KAAK,UAAU,OAAO;GACtB,KAAK,eAAe,OAAO,gBAAgB,KAAK;GAChD,KAAK,aAAa,OAAO;GACzB,KAAK,iBAAiB,OAAO;GAC7B,KAAK,gBAAgB,OAAO;GAC5B,KAAK,eAAe,OAAO;GAC3B,KAAK,aAAa,OAAO;GACzB,KAAK,eAAe,OAAO;GAC3B,KAAK,cAAc,OAAO;GAC1B,KAAK,YAAY,OAAO;GACxB,KAAK,OAAO,OAAO;GACnB,KAAK,OAAO,OAAO;GACnB,KAAK,oBAAoB,OAAO;GAChC,KAAK,qBAAqB,OAAO;GACjC,KAAK,gBAAgB,OAAO;GAC5B,KAAK,sBAAsB,OAAO;GAClC,KAAK,YAAY,QAAQ;GACzB,KAAK,UAAU,QAAQ;EACxB,OACC,KAAK,WAAW,QAAQ;EAG1B,KAAK,aAAa,OAAO,cAAc,KAAK;EAC5C,KAAK,iBAAiB,OAAO;EAC7B,KAAK,YAAY,OAAO,aAAa,KAAK;EAC1C,KAAK,mBAAmB,OAAO,oBAAoB,KAAK;AAExD,MACG,eAAe,UAAU,aAAa,UACtC,eAAe,UAAU,cAAc,UACvC,aAAa,UAAU,cAAc,OAEtC,OAAM,IAAI,MAAM;EAElB,KAAK,aAAa,QAAQ;EAC1B,MAAM,EACJ,iBACA,mBACA,sBACA,mBACA,mBACA,cACA,YACA,SACA,YACD,GAAG;EAEJ,MAAM,OAAOC,uCAA2B;GACtC;GACA;GACA;GACA;GACA;GACA;GACA;GACA;GACA;EACD,EAAC;AACF,MAAI,MAAM,KAAK,UAAU;MACpB,OAAM,IAAI,MAAM;CACtB;CAED,IAAI,aAAwC;AAC1C,SAAO;GACL,eAAe;GACf,QAAQ;GACR,QAAQ;GACR,mBAAmB;GACnB,iBAAiB;GACjB,sBAAsB;GACtB,mBAAmB;GACnB,mBAAmB;GACnB,cAAc;EACf;CACF;CAED,IAAI,aAAwC;AAC1C,SAAO;GACL,eAAe;GACf,QAAQ;GACR,QAAQ;GACR,mBAAmB;GACnB,iBAAiB;GACjB,sBAAsB;GACtB,mBAAmB;GACnB,mBAAmB;GACnB,cAAc;EACf;CACF;CAED,iBAAiBC,SAAoC;EACnD,MAAM,EAAE,YAAY,GAAG;EACvB,MAAM,EAAE,OAAQ,GAAG,MAAM,GAAG;AAC5B,MAAI,KAAK,YAAY,OAAO,KAAK,KAAK,CAAC,SAAS,EAC9C,OAAM,IAAI,MAAM;AAClB,MAAI,KAAK,SAAU,QAAO;AAC1B,SAAO;GACL,gBAAgB,YAAY,gBAAgB,KAAK;GACjD,iBAAiB,YAAY,kBAAkB,KAAK;GACpD,gBAAgB,YAAY,iBAAiB,KAAK;GAClD,gBAAgB,YAAY,gBAAgB,KAAK;GACjD,aAAa,YAAY,cAAc,KAAK;GAC5C,gBAAgB,SAAS,QAAQ,KAAK;GACtC,aAAa,YAAY,eAAe,KAAK;GAC7C,YAAY,YAAY,aAAa,KAAK;GAC1C,OAAO,YAAY,QAAQ,KAAK;GAChC,OAAO,YAAY,QAAQ,KAAK;GAChC,oBACE,YAAY,qBAAqB,KAAK;GACxC,uBACE,YAAY,sBAAsB,KAAK;GACzC,gBAAgB,YAAY,iBAAiB,KAAK;GAClD,uBACE,YAAY,uBAAuB,KAAK;EAC3C;CACF;CAED,oBAAoBA,SAAoC;AACtD,SAAO,QAAQ,oBAAoB,KAAK;CACzC;CAED,UAAU;AACR,MAAI,KAAK,UACP,QAAO;GAAE,WAAW,KAAK;GAAW,SAAS,KAAK;EAAO;WAClD,KAAK,QACZ,QAAO;GAAE,SAAS,KAAK;GAAS,SAAS,KAAK;EAAO;WAC9C,KAAK,SACZ,QAAO;GAAE,UAAU,KAAK;GAAU,SAAS,KAAK;EAAO;MACpD,QAAO,EAAE,SAAS,KAAK,MAAO;CACpC;CAED,MAAM,aAAa;EACjB,MAAM,kBAAkB,EACtB,SAAS,2BACV;EACD,MAAM,aAAa,MAAM,KAAK,oBAAoB,MAChD,KAAK,QAAQ,yBAAyB,gBAAgB,CACvD;AACD,SAAO,WAAW,OAAO,WAAW,IAAI,CAAC,SAAS,KAAK,SAAS;CACjE;CAgBD,MAAc,sBACZC,OACAD,SACAE,QACA;EACA,MAAM,EACJ,QACA,MACA,YACA,gBACA,QACA,GAAG,gBACJ,GAAG;EACJ,MAAM,aAAa;GAAE,uBAAuB;GAAG,mBAAmB;EAAG;EACrE,MAAM,WAAW,KAAK;EACtB,MAAM,aAAa,KAAK,iBAAiB,QAAQ;EACjD,MAAM,mBAAmB,KAAK,oBAAoB,QAAQ;AAC1D,MAAI,QAAQ;GACV,MAAM,aAAa,WACf,KAAK,QAAQ,6BAA6B;IACxC;IACA,GAAG;IACH,YAAY;KACV,GAAG;KACH,kBAAkB,EAChB,MACD;IACF;IACD,cAAc;GACf,EAAC,GACF,KAAK,QAAQ,mBACX;IACE;IACA;IACA,GAAG,KAAK,SAAS;IACjB,GAAG;IACH,cAAc;GACf,GACD,iBACD;AACL,UAAQ,MAAM;EAGf,OAAM;GACL,MAAM,wBAAwB,WAC1B,KAAK,QAAQ,uBACX;IACE,GAAG;IACH;IACA,YAAY;KACV,GAAG;KACH,kBAAkB,EAChB,MACD;IACF;GACF,GACD,iBACD,GACD,KAAK,QAAQ,aACX;IACE;IACA;IACA,GAAG,KAAK,SAAS;IACjB,GAAG;GACJ,GACD,iBACD;GAEL,MAAM,iBAAiB,MAAM;GAC7B,MAAMC,mBAAiC,eAAe,OAAO,QAAQ,IACnE,CAAC,WAAW;IACV,WAAW,yBAAyB,OAAO,wBACvC,OAAO,wBACP;IACJ,WAAW,qBAAqB,OAAO,oBACnC,OAAO,oBACP;AACJ,WAAO;KACL,MAAM,OAAO;KACb,gBAAgB;MACd,aAAa,OAAO;MACpB,mBAAmB,OAAO;MAC1B,uBAAuB,OAAO;KAC/B;IACF;GACF,EACF;AACD,UAAO;EACR;CACF;CAED,MAAM,oBACJC,UACAC,SACA;EACA,MAAM,SAAS,IAAIC,gDAAY;GAC7B,gBAAgB,SAAS,kBAAkB,KAAK;GAChD,YAAY,KAAK;EAClB;EACD,MAAM,SAAS,UACX,OAAO,gBACL,EACE,QAAQ,QAAQ,OACjB,GACD,YAAY,UAAU,CACvB,GACD,OAAO,KAAK,YAAY,UAAU,CAAC;AAEvC,SAAO;CACR;CAED,MAAM,UACJC,SACAP,SACAQ,YACoB;EACpB,MAAMC,aAAyB;GAC7B,uBAAuB;GACvB,mBAAmB;EACpB;AACD,MAAI,KAAK,WAAW;GAClB,MAAMC,cAA8B,MAAM,QAAQ,IAChD,QAAQ,IAAI,OAAO,QAAQ,cAAc;AACvC,QAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM;IAGlB,MAAM,SAAS,KAAK,sBAAsB,QAAQ,QAAQ;IAC1D,MAAMC,mBAAqC,CAAE;AAE7C,eAAW,MAAM,SAAS,QAAQ;KAChC,MAAM,aAAa,OAAO,gBAAgB,cAAc;KACxD,MAAMC,iBAAiC;MACrC,MAAM;MACN,aAAa;MACb,uBAAuB;MACvB,mBAAmB;KACpB;KACD,iBAAiB,gBAAgB;KACjC,iBAAiB,YAAY,wBAC3B,OAAO,gBAAgB,eAAe,yBAAyB;KACjE,iBAAiB,YAAY,qBAC3B,OAAO,gBAAgB,eAAe,qBAAqB;KAC7D,iBAAiB,YAAY,cAC3B,OAAO,gBAAgB;KACzB,iBAAiB,YAAY,QAAQ,MAAM;AAC3C,SAAI,MAAM,MAEH,YAAY,kBAAkB,MAAM,MAAM;MAC7C,QAAQ;MACR,YAAY;KACb,EAAC;IACL;AAED,WAAO,iBAAiB,IAAI,CAAC,SAAS;KACpC,MAAM,EAAE,KAAM,GAAG,MAAM,GAAG;KAC1B,WAAW,wBAAwB,KAAK;KACxC,WAAW,qBAAqB,KAAK;AAErC,YAAO;MACL;MACA,gBAAgB;KACjB;IACF,EAAC;GACH,EAAC,CACH;GACD,MAAMC,SAAoB;IAAE;IAAa,WAAW,EAAE,WAAY;GAAE;AACpE,UAAO;EACR,OAAM;GACL,MAAMH,cAA8B,MAAM,QAAQ,IAChD,QAAQ,IAAI,OAAO,WAAW;AAC5B,QAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM;IAGlB,MAAM,WAAW,MACf,KAAK,sBAAsB,QAAQ,SAAS,MAAM;IAGpD,MAAM,WAAW,MAAM,KAAK,oBAC1B,UACA,QACD;IACD,MAAM,CAAC,uBAAuB,kBAAkB,GAAG,SAAS,OAC1D,CAAC,KAAK,SAAS;KACb,IAAI,YAAY;KAChB,IAAI,UAAU;AACd,SAAI,MAAM,gBAAgB,uBACxB,YAAY,KAAK,eAAe,wBAAwB,IAAI;AAC9D,SAAI,MAAM,gBAAgB,mBACxB,UAAU,KAAK,eAAe,oBAAoB,IAAI;AACxD,YAAO,CAAC,WAAW,OAAQ;IAC5B,GACD,CAAC,GAAG,CAAE,EACP;IACD,WAAW,yBAAyB;IACpC,WAAW,qBAAqB;AAChC,WAAO;GACR,EAAC,CACH;GAED,MAAMG,SAAoB;IAAE;IAAa,WAAW,EAAE,WAAY;GAAE;AACpE,UAAO;EACR;CACF;CAED,MAAM,aACJC,SACAC,SACiB;EACjB,MAAMC,SAAiC;GACrC,GAAG,KAAK,SAAS;GACjB,OAAO;GACP,YAAY;EACb;EACD,MAAM,WAAW,MAAM,KAAK,QAAQ,aAAa,OAAO;EAGxD,MAAM,WAAW,MAAM,KAAK,oBAAkC,SAAS;AACvE,SAAO,SAAS,OAAO,OAAO;CAC/B;CAED,OAAO,sBACLC,QACAjB,SACAQ,YACiC;EACjC,MAAM,WAAW,MAAM,KAAK,sBAAsB,QAAQ,SAAS,KAAK;EAExE,MAAM,4BACJ,MAAM,KAAK,oBAAkC,SAAS;EACxD,MAAMU,gBAA+B;GACnC,IAAI;GACJ,OAAO;GACP,MAAM,EACJ,SAAS,CAAE,EACZ;EACF;AACD,aAAW,MAAM,SAAS,2BAA2B;AACnD,OAAI,QAAQ,QAAQ,QAClB,OAAM,IAAI,MAAM;AAGlB,QAAK,MAAM,CAAC,OAAO,KAAK,IAAI,MAAM,KAAK,QAAQ,SAAS,EAAE;IACxD,MAAM,IAAIC,yCAAgB;KACxB,MAAM,KAAK;KACX,gBAAgB;MACd,aAAa,KAAK;MAClB,YAAY;MACZ,gBAAgB;OACd,uBAAuB,KAAK;OAC5B,mBAAmB,KAAK;OACxB,aAAa,KAAK;MACnB;KACF;IACF;AACD,QAAI,CAAC,KAAK,WAEH,YAAY,kBAAkB,KAAK,eAAe;GAC1D;GACD,OAAO,OAAO,eAAe;IAAE,IAAI;IAAG,OAAO;IAAI,MAAM,CAAE;GAAE,EAAC;EAC7D;CACF;CAED,WAAW;AACT,SAAO;CACR;AACF"}
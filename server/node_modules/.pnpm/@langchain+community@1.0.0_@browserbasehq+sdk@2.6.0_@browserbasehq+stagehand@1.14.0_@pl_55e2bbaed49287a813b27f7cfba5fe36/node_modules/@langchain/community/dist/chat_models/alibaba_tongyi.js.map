{"version":3,"file":"alibaba_tongyi.js","names":["message: ChatMessage","message: BaseMessage","fields: Partial<AlibabaTongyiChatInput> & BaseChatModelParams","parameters: ChatCompletionRequest[\"parameters\"]","messages: BaseMessage[]","options?: this[\"ParsedCallOptions\"]","runManager?: CallbackManagerForLLMRun","messagesMapped: TongyiMessage[]","response: ChatCompletionResponse","data: ChatCompletionResponse","data","text","request: ChatCompletionRequest","stream: boolean","signal?: AbortSignal","onmessage?: (event: MessageEvent) => void"],"sources":["../../src/chat_models/alibaba_tongyi.ts"],"sourcesContent":["import { type CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport {\n  BaseChatModel,\n  type BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport {\n  AIMessage,\n  type BaseMessage,\n  ChatMessage,\n  AIMessageChunk,\n} from \"@langchain/core/messages\";\nimport { type ChatResult } from \"@langchain/core/outputs\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\nimport { IterableReadableStream } from \"@langchain/core/utils/stream\";\n\n/**\n * Type representing the role of a message in the Tongyi chat model.\n */\nexport type TongyiMessageRole = \"system\" | \"assistant\" | \"user\";\n\n/**\n * Interface representing a message in the Tongyi chat model.\n */\ninterface TongyiMessage {\n  role: TongyiMessageRole;\n  content: string;\n}\n\n/**\n * Interface representing a request for a chat completion.\n *\n * See https://help.aliyun.com/zh/dashscope/developer-reference/model-square/\n */\ninterface ChatCompletionRequest {\n  model:\n    | (string & NonNullable<unknown>)\n    | \"qwen-turbo\"\n    | \"qwen-plus\"\n    | \"qwen-max\"\n    | \"qwen-max-1201\"\n    | \"qwen-max-longcontext\"\n    // 通义千问开源系列\n    | \"qwen-7b-chat\"\n    | \"qwen-14b-chat\"\n    | \"qwen-72b-chat\"\n    // LLAMA2\n    | \"llama2-7b-chat-v2\"\n    | \"llama2-13b-chat-v2\"\n    // 百川\n    | \"baichuan-7b-v1\"\n    | \"baichuan2-13b-chat-v1\"\n    | \"baichuan2-7b-chat-v1\"\n    // ChatGLM\n    | \"chatglm3-6b\"\n    | \"chatglm-6b-v2\";\n  input: {\n    messages: TongyiMessage[];\n  };\n  parameters: {\n    stream?: boolean;\n    result_format?: \"text\" | \"message\";\n    seed?: number | null;\n    max_tokens?: number | null;\n    top_p?: number | null;\n    top_k?: number | null;\n    repetition_penalty?: number | null;\n    temperature?: number | null;\n    enable_search?: boolean | null;\n    incremental_output?: boolean | null;\n  };\n}\n\n/**\n * Interface representing a response from a chat completion.\n */\ninterface ChatCompletionResponse {\n  code?: string;\n  message?: string;\n  request_id: string;\n  usage: {\n    output_tokens: number;\n    input_tokens: number;\n    total_tokens: number;\n  };\n  output: {\n    text: string;\n    finish_reason: \"stop\" | \"length\" | \"null\" | null;\n  };\n}\n\n/**\n * Interface defining the input to the ChatAlibabaTongyi class.\n */\ninterface AlibabaTongyiChatInput {\n  /**\n   * Model name to use. Available options are: qwen-turbo, qwen-plus, qwen-max, or Other compatible models.\n   * Alias for `model`\n   * @default \"qwen-turbo\"\n   */\n  modelName: string;\n\n  /** Model name to use. Available options are: qwen-turbo, qwen-plus, qwen-max, or Other compatible models.\n   * @default \"qwen-turbo\"\n   */\n  model: string;\n\n  /** Whether to stream the results or not. Defaults to false. */\n  streaming?: boolean;\n\n  /** Messages to pass as a prefix to the prompt */\n  prefixMessages?: TongyiMessage[];\n\n  /**\n   * API key to use when making requests. Defaults to the value of\n   * `ALIBABA_API_KEY` environment variable.\n   */\n  alibabaApiKey?: string;\n\n  /** Amount of randomness injected into the response. Ranges\n   * from 0 to 1 (0 is not included). Use temp closer to 0 for analytical /\n   * multiple choice, and temp closer to 1 for creative\n   * and generative tasks. Defaults to 0.95.\n   */\n  temperature?: number;\n\n  /** Total probability mass of tokens to consider at each step. Range\n   * from 0 to 1.0. Defaults to 0.8.\n   */\n  topP?: number;\n\n  topK?: number;\n\n  enableSearch?: boolean;\n\n  maxTokens?: number;\n\n  seed?: number;\n\n  /** Penalizes repeated tokens according to frequency. Range\n   * from 1.0 to 2.0. Defaults to 1.0.\n   */\n  repetitionPenalty?: number;\n}\n\n/**\n * Function that extracts the custom role of a generic chat message.\n * @param message Chat message from which to extract the custom role.\n * @returns The custom role of the chat message.\n */\nfunction extractGenericMessageCustomRole(message: ChatMessage) {\n  if ([\"system\", \"assistant\", \"user\"].includes(message.role) === false) {\n    console.warn(`Unknown message role: ${message.role}`);\n  }\n\n  return message.role as TongyiMessageRole;\n}\n\n/**\n * Function that converts a base message to a Tongyi message role.\n * @param message Base message to convert.\n * @returns The Tongyi message role.\n */\nfunction messageToTongyiRole(message: BaseMessage): TongyiMessageRole {\n  const type = message._getType();\n  switch (type) {\n    case \"ai\":\n      return \"assistant\";\n    case \"human\":\n      return \"user\";\n    case \"system\":\n      return \"system\";\n    case \"function\":\n      throw new Error(\"Function messages not supported\");\n    case \"generic\": {\n      if (!ChatMessage.isInstance(message))\n        throw new Error(\"Invalid generic chat message\");\n      return extractGenericMessageCustomRole(message);\n    }\n    default:\n      throw new Error(`Unknown message type: ${type}`);\n  }\n}\n\n/**\n * Wrapper around Ali Tongyi large language models that use the Chat endpoint.\n *\n * To use you should have the `ALIBABA_API_KEY`\n * environment variable set.\n *\n * @augments BaseLLM\n * @augments AlibabaTongyiInput\n * @example\n * ```typescript\n * const qwen = new ChatAlibabaTongyi({\n *   alibabaApiKey: \"YOUR-API-KEY\",\n * });\n *\n * const qwen = new ChatAlibabaTongyi({\n *   model: \"qwen-turbo\",\n *   temperature: 1,\n *   alibabaApiKey: \"YOUR-API-KEY\",\n * });\n *\n * const messages = [new HumanMessage(\"Hello\")];\n *\n * await qwen.call(messages);\n * ```\n */\nexport class ChatAlibabaTongyi\n  extends BaseChatModel\n  implements AlibabaTongyiChatInput\n{\n  static lc_name() {\n    return \"ChatAlibabaTongyi\";\n  }\n\n  get callKeys() {\n    return [\"stop\", \"signal\", \"options\"];\n  }\n\n  get lc_secrets() {\n    return {\n      alibabaApiKey: \"ALIBABA_API_KEY\",\n    };\n  }\n\n  get lc_aliases() {\n    return undefined;\n  }\n\n  lc_serializable: boolean;\n\n  alibabaApiKey?: string;\n\n  streaming: boolean;\n\n  prefixMessages?: TongyiMessage[];\n\n  modelName: ChatCompletionRequest[\"model\"];\n\n  model: ChatCompletionRequest[\"model\"];\n\n  apiUrl: string;\n\n  maxTokens?: number | undefined;\n\n  temperature?: number | undefined;\n\n  topP?: number | undefined;\n\n  topK?: number | undefined;\n\n  repetitionPenalty?: number | undefined;\n\n  seed?: number | undefined;\n\n  enableSearch?: boolean | undefined;\n\n  constructor(\n    fields: Partial<AlibabaTongyiChatInput> & BaseChatModelParams = {}\n  ) {\n    super(fields);\n\n    this.alibabaApiKey =\n      fields?.alibabaApiKey ?? getEnvironmentVariable(\"ALIBABA_API_KEY\");\n    if (!this.alibabaApiKey) {\n      throw new Error(\"Ali API key not found\");\n    }\n\n    this.apiUrl =\n      \"https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation\";\n    this.lc_serializable = true;\n    this.streaming = fields.streaming ?? false;\n    this.prefixMessages = fields.prefixMessages ?? [];\n    this.temperature = fields.temperature;\n    this.topP = fields.topP;\n    this.topK = fields.topK;\n    this.seed = fields.seed;\n    this.maxTokens = fields.maxTokens;\n    this.repetitionPenalty = fields.repetitionPenalty;\n    this.enableSearch = fields.enableSearch;\n    this.modelName = fields?.model ?? fields.modelName ?? \"qwen-turbo\";\n    this.model = this.modelName;\n  }\n\n  /**\n   * Get the parameters used to invoke the model\n   */\n  invocationParams(): ChatCompletionRequest[\"parameters\"] {\n    const parameters: ChatCompletionRequest[\"parameters\"] = {\n      stream: this.streaming,\n      temperature: this.temperature,\n      top_p: this.topP,\n      top_k: this.topK,\n      seed: this.seed,\n      max_tokens: this.maxTokens,\n      result_format: \"text\",\n      enable_search: this.enableSearch,\n    };\n\n    if (this.streaming) {\n      parameters.incremental_output = true;\n    } else {\n      parameters.repetition_penalty = this.repetitionPenalty;\n    }\n\n    return parameters;\n  }\n\n  /**\n   * Get the identifying parameters for the model\n   */\n  identifyingParams(): ChatCompletionRequest[\"parameters\"] &\n    Pick<ChatCompletionRequest, \"model\"> {\n    return {\n      model: this.model,\n      ...this.invocationParams(),\n    };\n  }\n\n  /** @ignore */\n  async _generate(\n    messages: BaseMessage[],\n    options?: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    const parameters = this.invocationParams();\n\n    const messagesMapped: TongyiMessage[] = messages.map((message) => ({\n      role: messageToTongyiRole(message),\n      content: message.content as string,\n    }));\n\n    const data = parameters.stream\n      ? await new Promise<ChatCompletionResponse>((resolve, reject) => {\n          let response: ChatCompletionResponse;\n          let rejected = false;\n          let resolved = false;\n          this.completionWithRetry(\n            {\n              model: this.model,\n              parameters,\n              input: {\n                messages: messagesMapped,\n              },\n            },\n            true,\n            options?.signal,\n            (event) => {\n              const data: ChatCompletionResponse = JSON.parse(event.data);\n              if (data?.code) {\n                if (rejected) {\n                  return;\n                }\n                rejected = true;\n                reject(new Error(data?.message));\n                return;\n              }\n\n              const { text, finish_reason } = data.output;\n\n              if (!response) {\n                response = data;\n              } else {\n                response.output.text += text;\n                response.output.finish_reason = finish_reason;\n                response.usage = data.usage;\n              }\n\n              // eslint-disable-next-line no-void\n              void runManager?.handleLLMNewToken(text ?? \"\");\n              if (finish_reason && finish_reason !== \"null\") {\n                if (resolved || rejected) {\n                  return;\n                }\n                resolved = true;\n                resolve(response);\n              }\n            }\n          ).catch((error) => {\n            if (!rejected) {\n              rejected = true;\n              reject(error);\n            }\n          });\n        })\n      : await this.completionWithRetry(\n          {\n            model: this.model,\n            parameters,\n            input: {\n              messages: messagesMapped,\n            },\n          },\n          false,\n          options?.signal\n        ).then<ChatCompletionResponse>((data) => {\n          if (data?.code) {\n            throw new Error(data?.message);\n          }\n\n          return data;\n        });\n\n    const {\n      input_tokens = 0,\n      output_tokens = 0,\n      total_tokens = 0,\n    } = data.usage;\n\n    const { text } = data.output;\n\n    return {\n      generations: [\n        {\n          text,\n          message: new AIMessage(text),\n        },\n      ],\n      llmOutput: {\n        tokenUsage: {\n          promptTokens: input_tokens,\n          completionTokens: output_tokens,\n          totalTokens: total_tokens,\n        },\n      },\n    };\n  }\n\n  /** @ignore */\n  async completionWithRetry(\n    request: ChatCompletionRequest,\n    stream: boolean,\n    signal?: AbortSignal,\n    onmessage?: (event: MessageEvent) => void\n  ) {\n    const makeCompletionRequest = async () => {\n      const response = await fetch(this.apiUrl, {\n        method: \"POST\",\n        headers: {\n          ...(stream ? { Accept: \"text/event-stream\" } : {}),\n          Authorization: `Bearer ${this.alibabaApiKey}`,\n          \"Content-Type\": \"application/json\",\n        },\n        body: JSON.stringify(request),\n        signal,\n      });\n\n      if (!stream) {\n        return response.json();\n      }\n\n      if (response.body) {\n        // response will not be a stream if an error occurred\n        if (\n          !response.headers.get(\"content-type\")?.startsWith(\"text/event-stream\")\n        ) {\n          onmessage?.(\n            new MessageEvent(\"message\", {\n              data: await response.text(),\n            })\n          );\n          return;\n        }\n        const reader = response.body.getReader();\n        const decoder = new TextDecoder(\"utf-8\");\n        let data = \"\";\n        let continueReading = true;\n        while (continueReading) {\n          const { done, value } = await reader.read();\n          if (done) {\n            continueReading = false;\n            break;\n          }\n          data += decoder.decode(value);\n          let continueProcessing = true;\n          while (continueProcessing) {\n            const newlineIndex = data.indexOf(\"\\n\");\n            if (newlineIndex === -1) {\n              continueProcessing = false;\n              break;\n            }\n            const line = data.slice(0, newlineIndex);\n            data = data.slice(newlineIndex + 1);\n            if (line.startsWith(\"data:\")) {\n              const event = new MessageEvent(\"message\", {\n                data: line.slice(\"data:\".length).trim(),\n              });\n              onmessage?.(event);\n            }\n          }\n        }\n      }\n    };\n\n    return this.caller.call(makeCompletionRequest);\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options?: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    const parameters = {\n      ...this.invocationParams(),\n      stream: true,\n      incremental_output: true,\n    };\n\n    const messagesMapped: TongyiMessage[] = messages.map((message) => ({\n      role: messageToTongyiRole(message),\n      content: message.content as string,\n    }));\n\n    const stream = await this.caller.call(async () =>\n      this.createTongyiStream(\n        {\n          model: this.model,\n          parameters,\n          input: {\n            messages: messagesMapped,\n          },\n        },\n        options?.signal\n      )\n    );\n\n    for await (const chunk of stream) {\n      /* if some error occurs:\n         {\n          \"code\": \"DataInspectionFailed\",\n          \"message\": \"Output data may contain inappropriate content.\",\n          \"request_id\": \"43d18007-5aa5-9d18-b3b3-a55aba9ce8cb\"\n        }\n      */\n      if (!chunk.output && chunk.code) {\n        throw new Error(JSON.stringify(chunk));\n      }\n      const { text, finish_reason } = chunk.output;\n      yield new ChatGenerationChunk({\n        text,\n        message: new AIMessageChunk({ content: text }),\n        generationInfo:\n          finish_reason === \"stop\"\n            ? {\n                finish_reason,\n                request_id: chunk.request_id,\n                usage: chunk.usage,\n              }\n            : undefined,\n      });\n      await runManager?.handleLLMNewToken(text);\n    }\n  }\n\n  private async *createTongyiStream(\n    request: ChatCompletionRequest,\n    signal?: AbortSignal\n  ) {\n    const response = await fetch(this.apiUrl, {\n      method: \"POST\",\n      headers: {\n        Authorization: `Bearer ${this.alibabaApiKey}`,\n        Accept: \"text/event-stream\",\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify(request),\n      signal,\n    });\n\n    if (!response.ok) {\n      let error;\n      const responseText = await response.text();\n      try {\n        const json = JSON.parse(responseText);\n        error = new Error(\n          `Tongyi call failed with status code ${response.status}: ${json.error}`\n        );\n      } catch {\n        error = new Error(\n          `Tongyi call failed with status code ${response.status}: ${responseText}`\n        );\n      }\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      (error as any).response = response;\n      throw error;\n    }\n    if (!response.body) {\n      throw new Error(\n        \"Could not begin Tongyi stream. Please check the given URL and try again.\"\n      );\n    }\n    const stream = IterableReadableStream.fromReadableStream(response.body);\n    const decoder = new TextDecoder();\n    let extra = \"\";\n    for await (const chunk of stream) {\n      const decoded = extra + decoder.decode(chunk);\n      const lines = decoded.split(\"\\n\");\n      extra = lines.pop() || \"\";\n      for (const line of lines) {\n        if (!line.startsWith(\"data:\")) {\n          continue;\n        }\n        try {\n          yield JSON.parse(line.slice(\"data:\".length).trim());\n        } catch {\n          console.warn(`Received a non-JSON parseable chunk: ${line}`);\n        }\n      }\n    }\n  }\n\n  _llmType(): string {\n    return \"alibaba_tongyi\";\n  }\n\n  /** @ignore */\n  _combineLLMOutput() {\n    return [];\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;AAsJA,SAAS,gCAAgCA,SAAsB;AAC7D,KAAI;EAAC;EAAU;EAAa;CAAO,EAAC,SAAS,QAAQ,KAAK,KAAK,OAC7D,QAAQ,KAAK,CAAC,sBAAsB,EAAE,QAAQ,MAAM,CAAC;AAGvD,QAAO,QAAQ;AAChB;;;;;;AAOD,SAAS,oBAAoBC,SAAyC;CACpE,MAAM,OAAO,QAAQ,UAAU;AAC/B,SAAQ,MAAR;EACE,KAAK,KACH,QAAO;EACT,KAAK,QACH,QAAO;EACT,KAAK,SACH,QAAO;EACT,KAAK,WACH,OAAM,IAAI,MAAM;EAClB,KAAK;AACH,OAAI,CAAC,YAAY,WAAW,QAAQ,CAClC,OAAM,IAAI,MAAM;AAClB,UAAO,gCAAgC,QAAQ;EAEjD,QACE,OAAM,IAAI,MAAM,CAAC,sBAAsB,EAAE,MAAM;CAClD;AACF;;;;;;;;;;;;;;;;;;;;;;;;;;AA2BD,IAAa,oBAAb,cACU,cAEV;CACE,OAAO,UAAU;AACf,SAAO;CACR;CAED,IAAI,WAAW;AACb,SAAO;GAAC;GAAQ;GAAU;EAAU;CACrC;CAED,IAAI,aAAa;AACf,SAAO,EACL,eAAe,kBAChB;CACF;CAED,IAAI,aAAa;AACf,SAAO;CACR;CAED;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA;CAEA,YACEC,SAAgE,CAAE,GAClE;EACA,MAAM,OAAO;EAEb,KAAK,gBACH,QAAQ,iBAAiB,uBAAuB,kBAAkB;AACpE,MAAI,CAAC,KAAK,cACR,OAAM,IAAI,MAAM;EAGlB,KAAK,SACH;EACF,KAAK,kBAAkB;EACvB,KAAK,YAAY,OAAO,aAAa;EACrC,KAAK,iBAAiB,OAAO,kBAAkB,CAAE;EACjD,KAAK,cAAc,OAAO;EAC1B,KAAK,OAAO,OAAO;EACnB,KAAK,OAAO,OAAO;EACnB,KAAK,OAAO,OAAO;EACnB,KAAK,YAAY,OAAO;EACxB,KAAK,oBAAoB,OAAO;EAChC,KAAK,eAAe,OAAO;EAC3B,KAAK,YAAY,QAAQ,SAAS,OAAO,aAAa;EACtD,KAAK,QAAQ,KAAK;CACnB;;;;CAKD,mBAAwD;EACtD,MAAMC,aAAkD;GACtD,QAAQ,KAAK;GACb,aAAa,KAAK;GAClB,OAAO,KAAK;GACZ,OAAO,KAAK;GACZ,MAAM,KAAK;GACX,YAAY,KAAK;GACjB,eAAe;GACf,eAAe,KAAK;EACrB;AAED,MAAI,KAAK,WACP,WAAW,qBAAqB;OAEhC,WAAW,qBAAqB,KAAK;AAGvC,SAAO;CACR;;;;CAKD,oBACuC;AACrC,SAAO;GACL,OAAO,KAAK;GACZ,GAAG,KAAK,kBAAkB;EAC3B;CACF;;CAGD,MAAM,UACJC,UACAC,SACAC,YACqB;EACrB,MAAM,aAAa,KAAK,kBAAkB;EAE1C,MAAMC,iBAAkC,SAAS,IAAI,CAAC,aAAa;GACjE,MAAM,oBAAoB,QAAQ;GAClC,SAAS,QAAQ;EAClB,GAAE;EAEH,MAAM,OAAO,WAAW,SACpB,MAAM,IAAI,QAAgC,CAAC,SAAS,WAAW;GAC7D,IAAIC;GACJ,IAAI,WAAW;GACf,IAAI,WAAW;GACf,KAAK,oBACH;IACE,OAAO,KAAK;IACZ;IACA,OAAO,EACL,UAAU,eACX;GACF,GACD,MACA,SAAS,QACT,CAAC,UAAU;IACT,MAAMC,SAA+B,KAAK,MAAM,MAAM,KAAK;AAC3D,QAAIC,QAAM,MAAM;AACd,SAAI,SACF;KAEF,WAAW;KACX,OAAO,IAAI,MAAMA,QAAM,SAAS;AAChC;IACD;IAED,MAAM,EAAE,cAAM,eAAe,GAAGA,OAAK;AAErC,QAAI,CAAC,UACH,WAAWA;SACN;KACL,SAAS,OAAO,QAAQC;KACxB,SAAS,OAAO,gBAAgB;KAChC,SAAS,QAAQD,OAAK;IACvB;IAGI,YAAY,kBAAkBC,UAAQ,GAAG;AAC9C,QAAI,iBAAiB,kBAAkB,QAAQ;AAC7C,SAAI,YAAY,SACd;KAEF,WAAW;KACX,QAAQ,SAAS;IAClB;GACF,EACF,CAAC,MAAM,CAAC,UAAU;AACjB,QAAI,CAAC,UAAU;KACb,WAAW;KACX,OAAO,MAAM;IACd;GACF,EAAC;EACH,KACD,MAAM,KAAK,oBACT;GACE,OAAO,KAAK;GACZ;GACA,OAAO,EACL,UAAU,eACX;EACF,GACD,OACA,SAAS,OACV,CAAC,KAA6B,CAACD,WAAS;AACvC,OAAIA,QAAM,KACR,OAAM,IAAI,MAAMA,QAAM;AAGxB,UAAOA;EACR,EAAC;EAEN,MAAM,EACJ,eAAe,GACf,gBAAgB,GAChB,eAAe,GAChB,GAAG,KAAK;EAET,MAAM,EAAE,MAAM,GAAG,KAAK;AAEtB,SAAO;GACL,aAAa,CACX;IACE;IACA,SAAS,IAAI,UAAU;GACxB,CACF;GACD,WAAW,EACT,YAAY;IACV,cAAc;IACd,kBAAkB;IAClB,aAAa;GACd,EACF;EACF;CACF;;CAGD,MAAM,oBACJE,SACAC,QACAC,QACAC,WACA;EACA,MAAM,wBAAwB,YAAY;GACxC,MAAM,WAAW,MAAM,MAAM,KAAK,QAAQ;IACxC,QAAQ;IACR,SAAS;KACP,GAAI,SAAS,EAAE,QAAQ,oBAAqB,IAAG,CAAE;KACjD,eAAe,CAAC,OAAO,EAAE,KAAK,eAAe;KAC7C,gBAAgB;IACjB;IACD,MAAM,KAAK,UAAU,QAAQ;IAC7B;GACD,EAAC;AAEF,OAAI,CAAC,OACH,QAAO,SAAS,MAAM;AAGxB,OAAI,SAAS,MAAM;AAEjB,QACE,CAAC,SAAS,QAAQ,IAAI,eAAe,EAAE,WAAW,oBAAoB,EACtE;KACA,YACE,IAAI,aAAa,WAAW,EAC1B,MAAM,MAAM,SAAS,MAAM,CAC5B,GACF;AACD;IACD;IACD,MAAM,SAAS,SAAS,KAAK,WAAW;IACxC,MAAM,UAAU,IAAI,YAAY;IAChC,IAAI,OAAO;IACX,IAAI,kBAAkB;AACtB,WAAO,iBAAiB;KACtB,MAAM,EAAE,MAAM,OAAO,GAAG,MAAM,OAAO,MAAM;AAC3C,SAAI,MAAM;MACR,kBAAkB;AAClB;KACD;KACD,QAAQ,QAAQ,OAAO,MAAM;KAC7B,IAAI,qBAAqB;AACzB,YAAO,oBAAoB;MACzB,MAAM,eAAe,KAAK,QAAQ,KAAK;AACvC,UAAI,iBAAiB,IAAI;OACvB,qBAAqB;AACrB;MACD;MACD,MAAM,OAAO,KAAK,MAAM,GAAG,aAAa;MACxC,OAAO,KAAK,MAAM,eAAe,EAAE;AACnC,UAAI,KAAK,WAAW,QAAQ,EAAE;OAC5B,MAAM,QAAQ,IAAI,aAAa,WAAW,EACxC,MAAM,KAAK,MAAM,EAAe,CAAC,MAAM,CACxC;OACD,YAAY,MAAM;MACnB;KACF;IACF;GACF;EACF;AAED,SAAO,KAAK,OAAO,KAAK,sBAAsB;CAC/C;CAED,OAAO,sBACLX,UACAC,SACAC,YACqC;EACrC,MAAM,aAAa;GACjB,GAAG,KAAK,kBAAkB;GAC1B,QAAQ;GACR,oBAAoB;EACrB;EAED,MAAMC,iBAAkC,SAAS,IAAI,CAAC,aAAa;GACjE,MAAM,oBAAoB,QAAQ;GAClC,SAAS,QAAQ;EAClB,GAAE;EAEH,MAAM,SAAS,MAAM,KAAK,OAAO,KAAK,YACpC,KAAK,mBACH;GACE,OAAO,KAAK;GACZ;GACA,OAAO,EACL,UAAU,eACX;EACF,GACD,SAAS,OACV,CACF;AAED,aAAW,MAAM,SAAS,QAAQ;AAQhC,OAAI,CAAC,MAAM,UAAU,MAAM,KACzB,OAAM,IAAI,MAAM,KAAK,UAAU,MAAM;GAEvC,MAAM,EAAE,MAAM,eAAe,GAAG,MAAM;GACtC,MAAM,IAAI,oBAAoB;IAC5B;IACA,SAAS,IAAI,eAAe,EAAE,SAAS,KAAM;IAC7C,gBACE,kBAAkB,SACd;KACE;KACA,YAAY,MAAM;KAClB,OAAO,MAAM;IACd,IACD;GACP;GACD,MAAM,YAAY,kBAAkB,KAAK;EAC1C;CACF;CAED,OAAe,mBACbK,SACAE,QACA;EACA,MAAM,WAAW,MAAM,MAAM,KAAK,QAAQ;GACxC,QAAQ;GACR,SAAS;IACP,eAAe,CAAC,OAAO,EAAE,KAAK,eAAe;IAC7C,QAAQ;IACR,gBAAgB;GACjB;GACD,MAAM,KAAK,UAAU,QAAQ;GAC7B;EACD,EAAC;AAEF,MAAI,CAAC,SAAS,IAAI;GAChB,IAAI;GACJ,MAAM,eAAe,MAAM,SAAS,MAAM;AAC1C,OAAI;IACF,MAAM,OAAO,KAAK,MAAM,aAAa;IACrC,wBAAQ,IAAI,MACV,CAAC,oCAAoC,EAAE,SAAS,OAAO,EAAE,EAAE,KAAK,OAAO;GAE1E,QAAO;IACN,wBAAQ,IAAI,MACV,CAAC,oCAAoC,EAAE,SAAS,OAAO,EAAE,EAAE,cAAc;GAE5E;GAEA,MAAc,WAAW;AAC1B,SAAM;EACP;AACD,MAAI,CAAC,SAAS,KACZ,OAAM,IAAI,MACR;EAGJ,MAAM,SAAS,uBAAuB,mBAAmB,SAAS,KAAK;EACvE,MAAM,UAAU,IAAI;EACpB,IAAI,QAAQ;AACZ,aAAW,MAAM,SAAS,QAAQ;GAChC,MAAM,UAAU,QAAQ,QAAQ,OAAO,MAAM;GAC7C,MAAM,QAAQ,QAAQ,MAAM,KAAK;GACjC,QAAQ,MAAM,KAAK,IAAI;AACvB,QAAK,MAAM,QAAQ,OAAO;AACxB,QAAI,CAAC,KAAK,WAAW,QAAQ,CAC3B;AAEF,QAAI;KACF,MAAM,KAAK,MAAM,KAAK,MAAM,EAAe,CAAC,MAAM,CAAC;IACpD,QAAO;KACN,QAAQ,KAAK,CAAC,qCAAqC,EAAE,MAAM,CAAC;IAC7D;GACF;EACF;CACF;CAED,WAAmB;AACjB,SAAO;CACR;;CAGD,oBAAoB;AAClB,SAAO,CAAE;CACV;AACF"}
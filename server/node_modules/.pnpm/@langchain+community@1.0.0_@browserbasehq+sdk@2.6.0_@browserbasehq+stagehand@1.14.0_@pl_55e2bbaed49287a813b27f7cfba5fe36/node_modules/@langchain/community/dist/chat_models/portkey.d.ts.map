{"version":3,"file":"portkey.d.ts","names":["LLMOptions","CallbackManagerForLLMRun","BaseMessage","ChatResult","ChatGenerationChunk","BaseChatModel","PortkeySession","PortkeyChat","Partial","Promise","AsyncGenerator"],"sources":["../../src/chat_models/portkey.d.ts"],"sourcesContent":["import { LLMOptions } from \"portkey-ai\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChatResult, ChatGenerationChunk } from \"@langchain/core/outputs\";\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport { PortkeySession } from \"../llms/portkey.js\";\nexport declare class PortkeyChat extends BaseChatModel {\n    apiKey?: string;\n    baseURL?: string;\n    mode?: string;\n    llms?: [LLMOptions] | null;\n    session: PortkeySession;\n    constructor(init?: Partial<PortkeyChat>);\n    _llmType(): string;\n    _generate(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], _?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    _combineLLMOutput(): {};\n}\n"],"mappings":";;;;;;;;;;;cAMqBO,WAAAA,SAAoBF,aAAAA;;;;UAI7BL;EAJSO,OAAAA,EAKRD,cALmB;EAAA,WAAA,CAAA,IAAA,CAAA,EAMTE,OANS,CAMDD,WANC,CAAA;EAAA,QAIpBP,CAAAA,CAAAA,EAAAA,MAAAA;EAAU,SACTM,CAAAA,QAAAA,EAGWJ,WAHXI,EAAAA,EAAAA,OAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA,EAAAA,CAAAA,CAAAA,EAGkEL,wBAHlEK,CAAAA,EAG6FG,OAH7FH,CAGqGH,UAHrGG,CAAAA;EAAc,qBACIC,CAAAA,QAAAA,EAGKL,WAHLK,EAAAA,EAAAA,OAAAA,EAAAA,IAAAA,CAAAA,mBAAAA,CAAAA,EAAAA,UAAAA,CAAAA,EAGqEN,wBAHrEM,CAAAA,EAGgGG,cAHhGH,CAG+GH,mBAH/GG,CAAAA;EAAW,iBAAnBC,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA"}
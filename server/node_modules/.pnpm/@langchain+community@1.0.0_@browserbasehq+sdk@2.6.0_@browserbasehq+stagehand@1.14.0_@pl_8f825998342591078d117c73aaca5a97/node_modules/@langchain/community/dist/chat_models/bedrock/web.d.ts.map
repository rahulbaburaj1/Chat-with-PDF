{"version":3,"file":"web.d.ts","names":["EventStreamCodec","CallbackManagerForLLMRun","BaseChatModelParams","BaseChatModel","LangSmithParams","BaseChatModelCallOptions","BindToolsInput","BaseLanguageModelInput","Runnable","AIMessageChunk","BaseMessage","BaseMessageChunk","ChatGenerationChunk","ChatResult","SerializedFields","BaseBedrockInput","CredentialType","AnthropicTool","Record","BedrockChatToolType","convertMessagesToPromptAnthropic","convertMessagesToPrompt","BedrockChatCallOptions","BedrockChatFields","Partial","BedrockChat","fetch","Promise","Response","AsyncGenerator","ArrayBuffer","Uint8Array"],"sources":["../../../src/chat_models/bedrock/web.d.ts"],"sourcesContent":["import { EventStreamCodec } from \"@smithy/eventstream-codec\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { type BaseChatModelParams, BaseChatModel, LangSmithParams, BaseChatModelCallOptions, BindToolsInput } from \"@langchain/core/language_models/chat_models\";\nimport { BaseLanguageModelInput } from \"@langchain/core/language_models/base\";\nimport { Runnable } from \"@langchain/core/runnables\";\nimport { AIMessageChunk, BaseMessage, BaseMessageChunk } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\nimport type { SerializedFields } from \"../../load/map_keys.js\";\nimport { BaseBedrockInput, type CredentialType } from \"../../utils/bedrock/index.js\";\ntype AnthropicTool = Record<string, unknown>;\ntype BedrockChatToolType = BindToolsInput | AnthropicTool;\nexport declare function convertMessagesToPromptAnthropic(messages: BaseMessage[], humanPrompt?: string, aiPrompt?: string): string;\n/**\n * Function that converts an array of messages into a single string prompt\n * that can be used as input for a chat model. It delegates the conversion\n * logic to the appropriate provider-specific function.\n * @param messages Array of messages to be converted.\n * @param options Options to be used during the conversion.\n * @returns A string prompt that can be used as input for a chat model.\n */\nexport declare function convertMessagesToPrompt(messages: BaseMessage[], provider: string): string;\nexport interface BedrockChatCallOptions extends BaseChatModelCallOptions {\n    tools?: BedrockChatToolType[];\n}\nexport interface BedrockChatFields extends Partial<BaseBedrockInput>, BaseChatModelParams {\n}\n/**\n * AWS Bedrock chat model integration.\n *\n * Setup:\n * Install `@langchain/community` and set the following environment variables:\n *\n * ```bash\n * npm install @langchain/openai\n * export AWS_REGION=\"your-aws-region\"\n * export AWS_SECRET_ACCESS_KEY=\"your-aws-secret-access-key\"\n * export AWS_ACCESS_KEY_ID=\"your-aws-access-key-id\"\n * ```\n *\n * ## [Constructor args](/classes/langchain_community_chat_models_bedrock.BedrockChat.html#constructor)\n *\n * ## [Runtime args](/interfaces/langchain_community_chat_models_bedrock_web.BedrockChatCallOptions.html)\n *\n * Runtime args can be passed as the second argument to any of the base runnable methods `.invoke`. `.stream`, `.batch`, etc.\n * They can also be passed via `.withConfig`, or the second arg in `.bindTools`, like shown in the examples below:\n *\n * ```typescript\n * // When calling `.withConfig`, call options should be passed via the first argument\n * const llmWithArgsBound = llm.withConfig({\n *   stop: [\"\\n\"],\n *   tools: [...],\n * });\n *\n * // When calling `.bindTools`, call options should be passed via the second argument\n * const llmWithTools = llm.bindTools(\n *   [...],\n *   {\n *     stop: [\"stop on this token!\"],\n *   }\n * );\n * ```\n *\n * ## Examples\n *\n * <details open>\n * <summary><strong>Instantiate</strong></summary>\n *\n * ```typescript\n * import { BedrockChat } from '@langchain/community/chat_models/bedrock/web';\n *\n * const llm = new BedrockChat({\n *   region: process.env.AWS_REGION,\n *   maxRetries: 0,\n *   model: \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n *   temperature: 0,\n *   maxTokens: undefined,\n *   // other params...\n * });\n *\n * // You can also pass credentials in explicitly:\n * const llmWithCredentials = new BedrockChat({\n *   region: process.env.BEDROCK_AWS_REGION,\n *   model: \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n *   credentials: {\n *     secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n *     accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n *   },\n * });\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Invoking</strong></summary>\n *\n * ```typescript\n * const input = `Translate \"I love programming\" into French.`;\n *\n * // Models also accept a list of chat messages or a formatted prompt\n * const result = await llm.invoke(input);\n * console.log(result);\n * ```\n *\n * ```txt\n * AIMessage {\n *   \"content\": \"Here's the translation to French:\\n\\nJ'adore la programmation.\",\n *   \"additional_kwargs\": {\n *     \"id\": \"msg_bdrk_01HCZHa2mKbMZeTeHjLDd286\"\n *   },\n *   \"response_metadata\": {\n *     \"type\": \"message\",\n *     \"role\": \"assistant\",\n *     \"model\": \"claude-3-5-sonnet-20240620\",\n *     \"stop_reason\": \"end_turn\",\n *     \"stop_sequence\": null,\n *     \"usage\": {\n *       \"input_tokens\": 25,\n *       \"output_tokens\": 19\n *     }\n *   },\n *   \"tool_calls\": [],\n *   \"invalid_tool_calls\": []\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Streaming Chunks</strong></summary>\n *\n * ```typescript\n * for await (const chunk of await llm.stream(input)) {\n *   console.log(chunk);\n * }\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {\n *     \"id\": \"msg_bdrk_01RhFuGR9uJ2bj5GbdAma4y6\"\n *   },\n *   \"response_metadata\": {\n *     \"type\": \"message\",\n *     \"role\": \"assistant\",\n *     \"model\": \"claude-3-5-sonnet-20240620\",\n *     \"stop_reason\": null,\n *     \"stop_sequence\": null\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"J\",\n * }\n * AIMessageChunk {\n *   \"content\": \"'adore la\",\n * }\n * AIMessageChunk {\n *   \"content\": \" programmation.\",\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"additional_kwargs\": {\n *     \"stop_reason\": \"end_turn\",\n *     \"stop_sequence\": null\n *   },\n * }\n * AIMessageChunk {\n *   \"content\": \"\",\n *   \"response_metadata\": {\n *     \"amazon-bedrock-invocationMetrics\": {\n *       \"inputTokenCount\": 25,\n *       \"outputTokenCount\": 11,\n *       \"invocationLatency\": 659,\n *       \"firstByteLatency\": 506\n *     }\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 25,\n *     \"output_tokens\": 11,\n *     \"total_tokens\": 36\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Aggregate Streamed Chunks</strong></summary>\n *\n * ```typescript\n * import { AIMessageChunk } from '@langchain/core/messages';\n * import { concat } from '@langchain/core/utils/stream';\n *\n * const stream = await llm.stream(input);\n * let full: AIMessageChunk | undefined;\n * for await (const chunk of stream) {\n *   full = !full ? chunk : concat(full, chunk);\n * }\n * console.log(full);\n * ```\n *\n * ```txt\n * AIMessageChunk {\n *   \"content\": \"J'adore la programmation.\",\n *   \"additional_kwargs\": {\n *     \"id\": \"msg_bdrk_017b6PuBybA51P5LZ9K6gZHm\",\n *     \"stop_reason\": \"end_turn\",\n *     \"stop_sequence\": null\n *   },\n *   \"response_metadata\": {\n *     \"type\": \"message\",\n *     \"role\": \"assistant\",\n *     \"model\": \"claude-3-5-sonnet-20240620\",\n *     \"stop_reason\": null,\n *     \"stop_sequence\": null,\n *     \"amazon-bedrock-invocationMetrics\": {\n *       \"inputTokenCount\": 25,\n *       \"outputTokenCount\": 11,\n *       \"invocationLatency\": 1181,\n *       \"firstByteLatency\": 1177\n *     }\n *   },\n *   \"usage_metadata\": {\n *     \"input_tokens\": 25,\n *     \"output_tokens\": 11,\n *     \"total_tokens\": 36\n *   }\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Bind tools</strong></summary>\n *\n * ```typescript\n * import { z } from 'zod';\n * import { AIMessage } from '@langchain/core/messages';\n *\n * const GetWeather = {\n *   name: \"GetWeather\",\n *   description: \"Get the current weather in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const GetPopulation = {\n *   name: \"GetPopulation\",\n *   description: \"Get the current population in a given location\",\n *   schema: z.object({\n *     location: z.string().describe(\"The city and state, e.g. San Francisco, CA\")\n *   }),\n * }\n *\n * const llmWithTools = llm.bindTools([GetWeather, GetPopulation]);\n * const aiMsg: AIMessage = await llmWithTools.invoke(\n *   \"Which city is hotter today and which is bigger: LA or NY?\"\n * );\n * console.log(aiMsg.tool_calls);\n * ```\n *\n * ```txt\n * [\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'Los Angeles, CA' },\n *     id: 'toolu_bdrk_01R2daqwHR931r4baVNzbe38',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetWeather',\n *     args: { location: 'New York, NY' },\n *     id: 'toolu_bdrk_01WDadwNc7PGqVZvCN7Dr7eD',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'Los Angeles, CA' },\n *     id: 'toolu_bdrk_014b8zLkpAgpxrPfewKinJFc',\n *     type: 'tool_call'\n *   },\n *   {\n *     name: 'GetPopulation',\n *     args: { location: 'New York, NY' },\n *     id: 'toolu_bdrk_01Tt8K2MUP15kNuMDFCLEFKN',\n *     type: 'tool_call'\n *   }\n * ]\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Structured Output</strong></summary>\n *\n * ```typescript\n * const Joke = z.object({\n *   setup: z.string().describe(\"The setup of the joke\"),\n *   punchline: z.string().describe(\"The punchline to the joke\"),\n *   rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\")\n * }).describe('Joke to tell user.');\n *\n * const structuredLlm = llm.withStructuredOutput(Joke);\n * const jokeResult = await structuredLlm.invoke(\"Tell me a joke about cats\");\n * console.log(jokeResult);\n * ```\n *\n * ```txt\n * {\n *   setup: \"Why don't cats play poker in the jungle?\",\n *   punchline: 'Too many cheetahs!'\n * }\n * ```\n * </details>\n *\n * <br />\n *\n * <details>\n * <summary><strong>Response Metadata</strong></summary>\n *\n * ```typescript\n * const aiMsgForResponseMetadata = await llm.invoke(input);\n * console.log(aiMsgForResponseMetadata.response_metadata);\n * ```\n *\n * ```txt\n * \"response_metadata\": {\n *   \"type\": \"message\",\n *   \"role\": \"assistant\",\n *   \"model\": \"claude-3-5-sonnet-20240620\",\n *   \"stop_reason\": \"end_turn\",\n *   \"stop_sequence\": null,\n *   \"usage\": {\n *     \"input_tokens\": 25,\n *     \"output_tokens\": 19\n *   }\n * }\n * ```\n * </details>\n */\nexport declare class BedrockChat extends BaseChatModel<BedrockChatCallOptions, AIMessageChunk> implements BaseBedrockInput {\n    model: string;\n    modelProvider: string;\n    region: string;\n    credentials: CredentialType;\n    temperature?: number | undefined;\n    maxTokens?: number | undefined;\n    fetchFn: typeof fetch;\n    endpointHost?: string;\n    modelKwargs?: Record<string, unknown>;\n    codec: EventStreamCodec;\n    streaming: boolean;\n    usesMessagesApi: boolean;\n    lc_serializable: boolean;\n    trace?: \"ENABLED\" | \"DISABLED\";\n    guardrailIdentifier: string;\n    guardrailVersion: string;\n    guardrailConfig?: {\n        tagSuffix: string;\n        streamProcessingMode: \"SYNCHRONOUS\" | \"ASYNCHRONOUS\";\n    };\n    get lc_aliases(): Record<string, string>;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    get lc_attributes(): SerializedFields | undefined;\n    _identifyingParams(): Record<string, string>;\n    _llmType(): string;\n    static lc_name(): string;\n    constructor(fields?: BedrockChatFields);\n    invocationParams(options?: this[\"ParsedCallOptions\"]): {\n        tools: AnthropicTool[] | undefined;\n        temperature: number | undefined;\n        max_tokens: number | undefined;\n        stop: string[] | undefined;\n        modelKwargs: Record<string, unknown> | undefined;\n        guardrailConfig: {\n            tagSuffix: string;\n            streamProcessingMode: \"ASYNCHRONOUS\" | \"SYNCHRONOUS\";\n        } | undefined;\n    };\n    getLsParams(options: this[\"ParsedCallOptions\"]): LangSmithParams;\n    _generate(messages: BaseMessage[], options: Partial<this[\"ParsedCallOptions\"]>, runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _generateNonStreaming(messages: BaseMessage[], options: Partial<this[\"ParsedCallOptions\"]>, _runManager?: CallbackManagerForLLMRun): Promise<ChatResult>;\n    _signedFetch(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], fields: {\n        bedrockMethod: \"invoke\" | \"invoke-with-response-stream\";\n        endpointHost: string;\n        provider: string;\n    }): Promise<Response>;\n    _streamResponseChunks(messages: BaseMessage[], options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    _readChunks(reader: any): {\n        [Symbol.asyncIterator](): AsyncGenerator<Uint8Array<ArrayBuffer>, void, unknown>;\n    };\n    _combineLLMOutput(): {};\n    bindTools(tools: BedrockChatToolType[], _kwargs?: Partial<this[\"ParsedCallOptions\"]>): Runnable<BaseLanguageModelInput, BaseMessageChunk, this[\"ParsedCallOptions\"]>;\n}\nexport {};\n"],"mappings":";;;;;;;;;;;;;;KASKiB,aAAAA,GAAgBC;KAChBC,mBAAAA,GAAsBb,iBAAiBW;iBACpBG,gCAAAA,WAA2CV;;;;;;;;;AAF9DO,iBAWmBI,uBAAAA,CAXG,QAAA,EAW+BX,WAX/B,EAAA,EAAA,QAAA,EAAA,MAAA,CAAA,EAAA,MAAA;AACtBS,UAWYG,sBAAAA,SAA+BjB,wBAXxB,CAAA;EAAA,KAAA,CAAA,EAYZc,mBAZY,EAAA;;AAAoBF,UAc3BM,iBAAAA,SAA0BC,OAdCP,CAcOF,gBAdPE,CAAAA,EAc0Bf,mBAd1Be,CAAAA,CAAa;AACzD;AASA;AACA;;;;AAAwE;AAGxE;;;;;AAAyF;AAkUzF;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAA0H;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;cAArGQ,WAAAA,SAAoBtB,cAAcmB,wBAAwBb,2BAA2BM;;;;eAIzFC;;;kBAGGU;;gBAEFR;SACPlB;;;;;;;;;;;oBAWWkB;;;;uBAIGJ;wBACCI;;;uBAGDK;;WAEVN;;;;iBAIMC;;;;;;mDAMgCd;sBAC7BM,wBAAwBc,iDAAiDvB,2BAA2B0B,QAAQd;kCAChGH,wBAAwBc,kDAAkDvB,2BAA2B0B,QAAQd;yBACtHH;;;;MAInBiB,QAAQC;kCACoBlB,gEAAgET,2BAA2B4B,eAAejB;;;8BAG5GiB,eAAeE,WAAWD;;;mBAGvCX,iCAAiCK,qCAAqChB,SAASD,wBAAwBI"}
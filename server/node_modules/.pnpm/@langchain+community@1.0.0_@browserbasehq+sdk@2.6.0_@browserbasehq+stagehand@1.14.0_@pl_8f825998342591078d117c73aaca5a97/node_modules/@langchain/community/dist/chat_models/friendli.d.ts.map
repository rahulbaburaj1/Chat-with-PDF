{"version":3,"file":"friendli.d.ts","names":["CallbackManagerForLLMRun","BaseChatModel","BaseChatModelCallOptions","BaseChatModelParams","BaseMessage","ChatGenerationChunk","ChatResult","FriendliMessageRole","ChatFriendliParams","Record","ChatFriendli","Promise","AsyncGenerator"],"sources":["../../src/chat_models/friendli.d.ts"],"sourcesContent":["import { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseChatModel, BaseChatModelCallOptions, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk, ChatResult } from \"@langchain/core/outputs\";\n/**\n * Type representing the role of a message in the Friendli chat model.\n */\nexport type FriendliMessageRole = \"system\" | \"assistant\" | \"user\";\n/**\n * The ChatFriendliParams interface defines the input parameters for\n * the ChatFriendli class.\n */\nexport interface ChatFriendliParams extends BaseChatModelParams {\n    /**\n     * Model name to use.\n     */\n    model?: string;\n    /**\n     * Base endpoint url.\n     */\n    baseUrl?: string;\n    /**\n     * Friendli personal access token to run as.\n     */\n    friendliToken?: string;\n    /**\n     * Friendli team ID to run as.\n     */\n    friendliTeam?: string;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalizes tokens that have been\n     * sampled, taking into account their frequency in the preceding text. This\n     * penalization diminishes the model's tendency to reproduce identical lines\n     * verbatim.\n     */\n    frequencyPenalty?: number;\n    /**\n     * Number between -2.0 and 2.0. Positive values penalizes tokens that have been\n     * sampled at least once in the existing text.\n     * presence_penalty: Optional[float] = None\n     * The maximum number of tokens to generate. The length of your input tokens plus\n     * `max_tokens` should not exceed the model's maximum length (e.g., 2048 for OpenAI\n     * GPT-3)\n     */\n    maxTokens?: number;\n    /**\n     * When one of the stop phrases appears in the generation result, the API will stop\n     * generation. The phrase is included in the generated result. If you are using\n     * beam search, all of the active beams should contain the stop phrase to terminate\n     * generation. Before checking whether a stop phrase is included in the result, the\n     * phrase is converted into tokens.\n     */\n    stop?: string[];\n    /**\n     * Sampling temperature. Smaller temperature makes the generation result closer to\n     * greedy, argmax (i.e., `top_k = 1`) sampling. If it is `None`, then 1.0 is used.\n     */\n    temperature?: number;\n    /**\n     * Tokens comprising the top `top_p` probability mass are kept for sampling. Numbers\n     * between 0.0 (exclusive) and 1.0 (inclusive) are allowed. If it is `None`, then 1.0\n     * is used by default.\n     */\n    topP?: number;\n    /**\n     * Additional kwargs to pass to the model.\n     */\n    modelKwargs?: Record<string, unknown>;\n}\n/**\n * The ChatFriendli class is used to interact with Friendli inference Endpoint models.\n * This requires your Friendli Token and Friendli Team which is autoloaded if not specified.\n */\nexport declare class ChatFriendli extends BaseChatModel<BaseChatModelCallOptions> {\n    lc_serializable: boolean;\n    static lc_name(): string;\n    get lc_secrets(): {\n        [key: string]: string;\n    } | undefined;\n    model: string;\n    baseUrl: string;\n    friendliToken?: string;\n    friendliTeam?: string;\n    frequencyPenalty?: number;\n    maxTokens?: number;\n    stop?: string[];\n    temperature?: number;\n    topP?: number;\n    modelKwargs?: Record<string, unknown>;\n    constructor(fields: ChatFriendliParams);\n    _llmType(): string;\n    private constructHeaders;\n    private constructBody;\n    /**\n     * Calls the Friendli endpoint and retrieves the result.\n     * @param {BaseMessage[]} messages The input messages.\n     * @returns {Promise<ChatResult>} A promise that resolves to the generated chat result.\n     */\n    /** @ignore */\n    _generate(messages: BaseMessage[], _options: this[\"ParsedCallOptions\"]): Promise<ChatResult>;\n    _streamResponseChunks(messages: BaseMessage[], _options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;\n}\n"],"mappings":";;;;;;;;;;;;KAOYO,mBAAAA;;;;;UAKKC,kBAAAA,SAA2BL;EALhCI;AAKZ;;EAAmC,KAuDjBE,CAAAA,EAAAA,MAAAA;EAAM;AAvDuC;AA6D/D;EAAiC,OAAA,CAAA,EAAA,MAAA;EAAA;;;EAgBS,aAUlBL,CAAAA,EAAAA,MAAAA;EAAW;;;EACY,YAAsDJ,CAAAA,EAAAA,MAAAA;EAAwB;;;AA3BtE;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;gBANrCS;;;;;;cAMGC,YAAAA,SAAqBT,cAAcC;;;;;;;;;;;;;;;gBAetCO;sBACMD;;;;;;;;;;sBAUAJ,qDAAqDO,QAAQL;kCACjDF,iEAAiEJ,2BAA2BY,eAAeP"}
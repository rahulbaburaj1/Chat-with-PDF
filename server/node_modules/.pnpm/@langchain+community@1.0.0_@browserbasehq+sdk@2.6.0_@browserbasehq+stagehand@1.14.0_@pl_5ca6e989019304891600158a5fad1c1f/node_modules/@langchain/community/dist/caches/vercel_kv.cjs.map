{"version":3,"file":"vercel_kv.cjs","names":["BaseCache","props: VercelKVCacheProps","kv","prompt: string","llmKey: string","generations: Generation[]","value: Generation[]"],"sources":["../../src/caches/vercel_kv.ts"],"sourcesContent":["import { kv, type VercelKV } from \"@vercel/kv\";\n\nimport { Generation } from \"@langchain/core/outputs\";\nimport {\n  BaseCache,\n  deserializeStoredGeneration,\n  serializeGeneration,\n} from \"@langchain/core/caches\";\nimport { StoredGeneration } from \"@langchain/core/messages\";\n\nexport type VercelKVCacheProps = {\n  /**\n   * An existing Vercel KV client\n   */\n  client?: VercelKV;\n  /**\n   * Time-to-live (TTL) for cached items in seconds\n   */\n  ttl?: number;\n};\n\n/**\n * A cache that uses Vercel KV as the backing store.\n * @example\n * ```typescript\n * const cache = new VercelKVCache({\n *   ttl: 3600, // Optional: Cache entries will expire after 1 hour\n * });\n *\n * // Initialize the OpenAI model with Vercel KV cache for caching responses\n * const model = new ChatOpenAI({\n *   model: \"gpt-4o-mini\",\n *   cache,\n * });\n * await model.invoke(\"How are you today?\");\n * const cachedValues = await cache.lookup(\"How are you today?\", \"llmKey\");\n * ```\n */\nexport class VercelKVCache extends BaseCache {\n  private client: VercelKV;\n\n  private ttl?: number;\n\n  constructor(props: VercelKVCacheProps) {\n    super();\n    const { client, ttl } = props;\n    this.client = client ?? kv;\n    this.ttl = ttl;\n  }\n\n  /**\n   * Lookup LLM generations in cache by prompt and associated LLM key.\n   */\n  public async lookup(prompt: string, llmKey: string) {\n    let idx = 0;\n    let key = this.keyEncoder(prompt, llmKey, String(idx));\n    let value = await this.client.get<StoredGeneration | null>(key);\n    const generations: Generation[] = [];\n\n    while (value) {\n      generations.push(deserializeStoredGeneration(value));\n      idx += 1;\n      key = this.keyEncoder(prompt, llmKey, String(idx));\n      value = await this.client.get<StoredGeneration | null>(key);\n    }\n\n    return generations.length > 0 ? generations : null;\n  }\n\n  /**\n   * Update the cache with the given generations.\n   *\n   * Note this overwrites any existing generations for the given prompt and LLM key.\n   */\n  public async update(prompt: string, llmKey: string, value: Generation[]) {\n    for (let i = 0; i < value.length; i += 1) {\n      const key = this.keyEncoder(prompt, llmKey, String(i));\n      const serializedValue = JSON.stringify(serializeGeneration(value[i]));\n\n      if (this.ttl) {\n        await this.client.set(key, serializedValue, { ex: this.ttl });\n      } else {\n        await this.client.set(key, serializedValue);\n      }\n    }\n  }\n}\n"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;AAsCA,IAAa,gBAAb,cAAmCA,kCAAU;CAC3C,AAAQ;CAER,AAAQ;CAER,YAAYC,OAA2B;EACrC,OAAO;EACP,MAAM,EAAE,QAAQ,KAAK,GAAG;EACxB,KAAK,SAAS,UAAUC;EACxB,KAAK,MAAM;CACZ;;;;CAKD,MAAa,OAAOC,QAAgBC,QAAgB;EAClD,IAAI,MAAM;EACV,IAAI,MAAM,KAAK,WAAW,QAAQ,QAAQ,OAAO,IAAI,CAAC;EACtD,IAAI,QAAQ,MAAM,KAAK,OAAO,IAA6B,IAAI;EAC/D,MAAMC,cAA4B,CAAE;AAEpC,SAAO,OAAO;GACZ,YAAY,8DAAiC,MAAM,CAAC;GACpD,OAAO;GACP,MAAM,KAAK,WAAW,QAAQ,QAAQ,OAAO,IAAI,CAAC;GAClD,QAAQ,MAAM,KAAK,OAAO,IAA6B,IAAI;EAC5D;AAED,SAAO,YAAY,SAAS,IAAI,cAAc;CAC/C;;;;;;CAOD,MAAa,OAAOF,QAAgBC,QAAgBE,OAAqB;AACvE,OAAK,IAAI,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK,GAAG;GACxC,MAAM,MAAM,KAAK,WAAW,QAAQ,QAAQ,OAAO,EAAE,CAAC;GACtD,MAAM,kBAAkB,KAAK,2DAA8B,MAAM,GAAG,CAAC;AAErE,OAAI,KAAK,KACP,MAAM,KAAK,OAAO,IAAI,KAAK,iBAAiB,EAAE,IAAI,KAAK,IAAK,EAAC;QAE7D,MAAM,KAAK,OAAO,IAAI,KAAK,gBAAgB;EAE9C;CACF;AACF"}
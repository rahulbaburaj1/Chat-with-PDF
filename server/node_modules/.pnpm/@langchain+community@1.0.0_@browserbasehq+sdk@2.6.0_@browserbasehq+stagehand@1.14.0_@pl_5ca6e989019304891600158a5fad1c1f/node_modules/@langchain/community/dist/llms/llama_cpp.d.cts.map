{"version":3,"file":"llama_cpp.d.cts","names":["LlamaModel","LlamaContext","LlamaChatSession","LlamaJsonSchemaGrammar","LlamaGrammar","GbnfJsonSchema","LLM","BaseLLMCallOptions","BaseLLMParams","CallbackManagerForLLMRun","GenerationChunk","LlamaBaseCppInputs","LlamaCppInputs","LlamaCppCallOptions","LlamaCpp","Promise","AsyncGenerator"],"sources":["../../src/llms/llama_cpp.d.ts"],"sourcesContent":["import { LlamaModel, LlamaContext, LlamaChatSession, LlamaJsonSchemaGrammar, LlamaGrammar, GbnfJsonSchema } from \"node-llama-cpp\";\nimport { LLM, type BaseLLMCallOptions, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\nimport { LlamaBaseCppInputs } from \"../utils/llama_cpp.js\";\n/**\n * Note that the modelPath is the only required parameter. For testing you\n * can set this in the environment variable `LLAMA_PATH`.\n */\nexport interface LlamaCppInputs extends LlamaBaseCppInputs, BaseLLMParams {\n}\nexport interface LlamaCppCallOptions extends BaseLLMCallOptions {\n    /** The maximum number of tokens the response should contain. */\n    maxTokens?: number;\n    /** A function called when matching the provided token array */\n    onToken?: (tokens: number[]) => void;\n}\n/**\n *  To use this model you need to have the `node-llama-cpp` module installed.\n *  This can be installed using `npm install -S node-llama-cpp` and the minimum\n *  version supported in version 2.0.0.\n *  This also requires that have a locally built version of Llama3 installed.\n */\nexport declare class LlamaCpp extends LLM<LlamaCppCallOptions> {\n    lc_serializable: boolean;\n    static inputs: LlamaCppInputs;\n    maxTokens?: number;\n    temperature?: number;\n    topK?: number;\n    topP?: number;\n    trimWhitespaceSuffix?: boolean;\n    _model: LlamaModel;\n    _context: LlamaContext;\n    _session: LlamaChatSession;\n    _jsonSchema: LlamaJsonSchemaGrammar<GbnfJsonSchema> | undefined;\n    _gbnf: LlamaGrammar | undefined;\n    static lc_name(): string;\n    constructor(inputs: LlamaCppInputs);\n    /**\n     * Initializes the llama_cpp model for usage.\n     * @param inputs - the inputs passed onto the model.\n     * @returns A Promise that resolves to the LlamaCpp type class.\n     */\n    static initialize(inputs: LlamaCppInputs): Promise<LlamaCpp>;\n    _llmType(): string;\n    /** @ignore */\n    _call(prompt: string, options?: this[\"ParsedCallOptions\"]): Promise<string>;\n    _streamResponseChunks(prompt: string, _options: this[\"ParsedCallOptions\"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<GenerationChunk>;\n}\n"],"mappings":";;;;;;;;;;;;;;UASiBY,cAAAA,SAAuBD,oBAAoBH;UAE3CK,mBAAAA,SAA4BN;;;EAF5BK;EAAc,OAAA,CAAA,EAAA,CAAA,MAAA,EAAA,MAAA,EAAA,EAAA,GAAA,IAAA;;;AAA0C;AAEzE;AAYA;;;AAEmBA,cAFEE,QAAAA,SAAiBR,GAEnBM,CAFuBC,mBAEvBD,CAAAA,CAAAA;EAAc,eAMrBZ,EAAAA,OAAAA;EAAU,OACRC,MAAAA,EAPKW,cAOLX;EAAY,SACZC,CAAAA,EAAAA,MAAAA;EAAgB,WACUG,CAAAA,EAAAA,MAAAA;EAAc,IAArCF,CAAAA,EAAAA,MAAAA;EAAsB,IAC5BC,CAAAA,EAAAA,MAAAA;EAAY,oBAECQ,CAAAA,EAAAA,OAAAA;EAAc,MAMRA,EAZlBZ,UAYkBY;EAAc,QAAWE,EAXzCb,YAWyCa;EAAQ,QAAhBC,EAVjCb,gBAUiCa;EAAO,WAGUA,EAZ/CZ,sBAY+CY,CAZxBV,cAYwBU,CAAAA,GAAAA,SAAAA;EAAO,KACqBN,EAZjFL,YAYiFK,GAAAA,SAAAA;EAAwB,OAAkBC,OAAAA,CAAAA,CAAAA,EAAAA,MAAAA;EAAe,WAA9BM,CAAAA,MAAAA,EAV/FJ,cAU+FI;EAAc;AAxB5F;;;;4BAoBXJ,iBAAiBG,QAAQD;;;8DAGSC;0FAC4BN,2BAA2BO,eAAeN"}